/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
!function(t,e){"object"==typeof exports&&"undefined"!=typeof module?e(exports,require("@tensorflow/tfjs-core")):"function"==typeof define&&define.amd?define(["exports","@tensorflow/tfjs-core"],e):e((t=t||self).tf=t.tf||{},t.tf)}(this,(function(t,e){"use strict";function n(t){throw new Error(`'${t}' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen`)}function s(t,e){if(!t)throw new Error("string"==typeof e?e:e())}function i(t,e=[],n=!1){if(null==e&&(e=[]),Array.isArray(t)||h(t)&&!n)for(let s=0;s<t.length;++s)i(t[s],e,n);else e.push(t);return e}function r(t){if(0===t.length)return 1;let e=t[0];for(let n=1;n<t.length;n++)e*=t[n];return e}function a(t,e){if(t===e)return!0;if(null==t||null==e)return!1;if(t.length!==e.length)return!1;for(let n=0;n<t.length;n++)if(t[n]!==e[n])return!1;return!0}function o(t){return t%1==0}function l(t,e){return e<=t.length?t:t+" ".repeat(e-t.length)}function u(t,e){const n=e.length;return s((t=null==t?e.map(((t,e)=>e)):[].concat(t)).every((t=>t>=-n&&t<n)),(()=>`All values in axis param must be in range [-${n}, ${n}) but got axis ${t}`)),s(t.every((t=>o(t))),(()=>`All values in axis param must be integers but got axis ${t}`)),t.map((t=>t<0?n+t:t))}function h(t){return t instanceof Float32Array||t instanceof Int32Array||t instanceof Uint8Array}function c(t){if("float32"===t||"int32"===t)return 4;if("complex64"===t)return 8;if("bool"===t)return 1;throw new Error(`Unknown dtype ${t}`)}function p(t){return"string"==typeof t||t instanceof String}function d(t){return Array.isArray(t)?d(t[0]):t instanceof Float32Array?"float32":t instanceof Int32Array||t instanceof Uint8Array?"int32":"number"==typeof t?"float32":p(t)?"string":function(t){return"boolean"==typeof t}(t)?"bool":"float32"}function f(t){return!!(t&&t.constructor&&t.call&&t.apply)}function g(t){const e=t.length;if(e<2)return[];const n=new Array(e-1);n[e-2]=t[e-1];for(let s=e-3;s>=0;--s)n[s]=n[s+1]*t[s+1];return n}function m(t,e,n,s=!1){const i=new Array;if(1===e.length){const r=e[0]*(s?2:1);for(let e=0;e<r;e++)i[e]=n[t+e]}else{const r=e[0],a=e.slice(1),o=a.reduce(((t,e)=>t*e))*(s?2:1);for(let e=0;e<r;e++)i[e]=m(t+e*o,a,n,s)}return i}function y(t,e,n=!1){if(0===t.length)return e[0];const s=t.reduce(((t,e)=>t*e))*(n?2:1);if(0===s)return[];if(s!==e.length)throw new Error(`[${t}] does not match the input size ${e.length}${n?" for a complex tensor":""}.`);return m(0,t,e,n)}function b(t,e){const n=w(t,e);for(let t=0;t<n.length;t++)n[t]=1;return n}function w(t,e){if(null==e||"float32"===e||"complex64"===e)return new Float32Array(t);if("int32"===e)return new Int32Array(t);if("bool"===e)return new Uint8Array(t);throw new Error(`Unknown data type ${e}`)}function k(t){return t&&t.then&&"function"==typeof t.then}const x="tfjsflags";class v{constructor(t){this.global=t,this.flags={},this.flagRegistry={},this.urlFlags={},this.getQueryParams=S,this.populateURLFlags()}setPlatform(t,e){null!=this.platform&&console.warn(`Platform ${this.platformName} has already been set. Overwriting the platform with ${e}.`),this.platformName=t,this.platform=e}registerFlag(t,e,n){if(this.flagRegistry[t]={evaluationFn:e,setHook:n},null!=this.urlFlags[t]){const e=this.urlFlags[t];console.warn(`Setting feature override from URL ${t}: ${e}.`),this.set(t,e)}}async getAsync(t){return t in this.flags||(this.flags[t]=await this.evaluateFlag(t)),this.flags[t]}get(t){if(t in this.flags)return this.flags[t];const e=this.evaluateFlag(t);if(k(e))throw new Error(`Flag ${t} cannot be synchronously evaluated. Please use getAsync() instead.`);return this.flags[t]=e,this.flags[t]}getNumber(t){return this.get(t)}getBool(t){return this.get(t)}getFlags(){return this.flags}get features(){return this.flags}set(t,e){if(null==this.flagRegistry[t])throw new Error(`Cannot set flag ${t} as it has not been registered.`);this.flags[t]=e,null!=this.flagRegistry[t].setHook&&this.flagRegistry[t].setHook(e)}evaluateFlag(t){if(null==this.flagRegistry[t])throw new Error(`Cannot evaluate flag '${t}': no evaluation function found.`);return this.flagRegistry[t].evaluationFn()}setFlags(t){this.flags=Object.assign({},t)}reset(){this.flags={},this.urlFlags={},this.populateURLFlags()}populateURLFlags(){if(void 0===this.global||void 0===this.global.location||void 0===this.global.location.search)return;const t=this.getQueryParams(this.global.location.search);if(x in t){t.tfjsflags.split(",").forEach((t=>{const[e,n]=t.split(":");this.urlFlags[e]=function(t,e){if("true"===(e=e.toLowerCase())||"false"===e)return"true"===e;if(""+ +e===e)return+e;throw new Error(`Could not parse value flag value ${e} for flag ${t}.`)}(e,n)}))}}}function S(t){const e={};return t.replace(/[?&]([^=?&]+)(?:=([^&]*))?/g,((t,...n)=>(function(t,e,n){t[decodeURIComponent(e)]=decodeURIComponent(n||"")}(e,n[0],n[1]),n.join("=")))),e}function I(){return z}let N,z=null;function A(){if(null==N){let t;if("undefined"!=typeof window)t=window;else if("undefined"!=typeof global)t=global;else if("undefined"!=typeof process)t=process;else{if("undefined"==typeof self)throw new Error("Could not find a global object");t=self}N=t}return N}function C(t,e){const n=function(){const t=A();return null==t._tfGlobals&&(t._tfGlobals=new Map),t._tfGlobals}();if(n.has(t))return n.get(t);{const s=e();return n.set(t,s),n.get(t)}}const $="Acos",D="Acosh",T="Add",E="ArgMax",F="ArgMin",L="Asin",_="Asinh",R="Atan",M="Atanh",O="Atan2",B="AvgPool",W="BatchMatMul",P="BatchToSpaceND",U="Cast",K="Ceil",V="ClipByValue",j="ComplexAbs",q="Concat",G="Conv2D",H="Conv2DBackpropInput",J="Cosh",Z="Cumsum",X="DepthwiseConv2dNative",Y="Dilation2D",Q="RealDiv",tt="ExpandDims",et="Expm1",nt="Floor",st="FloorDiv",it="FusedBatchNorm",rt="GatherV2",at="GreaterEqual",ot="Identity",lt="IsFinite",ut="IsInf",ht="IsNan",ct="LeakyRelu",pt="Log1p",dt="Maximum",ft="MaxPool",gt="Mean",mt="Minimum",yt="MirrorPad",bt="Multiply",wt="OnesLike",kt="OneHot",xt="Pack",vt="PadV2",St="Prelu",It="Reciprocal",Nt="Relu",zt="Reshape",At="ResizeNearestNeighbor",Ct="ResizeBilinear",$t="Relu6",Dt="Reverse",Tt="Round",Et="Rsqrt",Ft="Select",Lt="Selu",_t="Slice",Rt="Sinh",Mt="Sign",Ot="Sigmoid",Bt="Softplus",Wt="Sqrt",Pt="SpaceToBatchND",Ut="SplitV",Kt="Softmax",Vt="SquaredDifference",jt="Tanh",qt="Tile",Gt="Transpose",Ht="Unpack",Jt="UnsortedSegmentSum",Zt="ZerosLike",Xt="Step",Yt=C("kernelRegistry",(()=>new Map)),Qt=C("gradRegistry",(()=>new Map));function te(t,e){const n=function(t,e){return`${e}_${t}`}(t,e);return Yt.get(n)}function ee(t){return Qt.get(t)}function ne(t){const e=Yt.entries(),n=[];for(;;){const{done:s,value:i}=e.next();if(s)break;const[r,a]=i,[o]=r.split("_");o===t&&n.push(a)}return n}function se(t){const{kernelName:e}=t;Qt.has(e)&&I().getBool("DEBUG")&&console.warn(`Overriding the gradient for '${e}'`),Qt.set(e,t)}function ie(t,e){if("string"===e)throw new Error("Cannot convert a string[] to a TypedArray");if(Array.isArray(t)&&(t=i(t)),I().getBool("DEBUG")&&function(t,e){for(let n=0;n<t.length;n++){const s=t[n];if(isNaN(s)||!isFinite(s))throw Error(`A tensor of type ${e} being uploaded contains ${s}.`)}}(t,e),function(t,e){return t instanceof Float32Array&&"float32"===e||t instanceof Int32Array&&"int32"===e||t instanceof Uint8Array&&"bool"===e}(t,e))return t;if(null==e||"float32"===e||"complex64"===e)return new Float32Array(t);if("int32"===e)return new Int32Array(t);if("bool"===e){const e=new Uint8Array(t.length);for(let n=0;n<e.length;++n)0!==Math.round(t[n])&&(e[n]=1);return e}throw new Error(`Unknown data type ${e}`)}function re(){return I().platform.now()}function ae(t,e="utf-8"){return e=e||"utf-8",I().platform.decode(t,e)}class oe{constructor(t,e){this.backendTimer=t,this.logger=e,null==e&&(this.logger=new ue)}profileKernel(t,e,n){let s;const i=()=>{s=n()};let r;const a=re();if(this.backendTimer.timerAvailable())r=this.backendTimer.time(i);else{i();for(const t of s)t.dataSync();r=Promise.resolve({kernelMs:re()-a})}if(I().getBool("CHECK_COMPUTATION_FOR_ERRORS"))for(let e=0;e<s.length;e++){const n=s[e];n.data().then((e=>{le(e,n.dtype,t)}))}return{kernelName:t,outputs:s,inputs:e,timeMs:r.then((t=>t.kernelMs)),extraInfo:r.then((t=>null!=t.getExtraProfileInfo?t.getExtraProfileInfo():""))}}logKernelProfile(t){const{kernelName:e,outputs:n,timeMs:s,inputs:i,extraInfo:r}=t;n.forEach((t=>{Promise.all([t.data(),s,r]).then((n=>{this.logger.logKernelProfile(e,t,n[0],n[1],i,n[2])}))}))}}function le(t,e,n){if("float32"!==e)return!1;for(let e=0;e<t.length;e++){const s=t[e];if(isNaN(s)||!isFinite(s))return console.warn(`Found ${s} in the result of '${n}'`),!0}return!1}class ue{logKernelProfile(t,e,n,s,i,r){const a="number"==typeof s?l(`${s}ms`,9):s.error,o=l(t,25),u=e.rank,h=e.size,c=l(e.shape.toString(),14);let p="";for(const t in i){const n=i[t];if(null!=n){const s=n.shape||e.shape,i=s.length;p+=`${t}: ${i}D ${i>0?s:""} `}}console.log(`%c${o}\t%c${a}\t%c${u}D ${c}\t%c${h}\t%c${p}\t%c${r}`,"font-weight:bold","color:red","color:blue","color: orange","color: green","color: steelblue")}}function he(t,e,n,s){const i=g(e),a=function(t,e,n,s){const i=r(e),a=s[s.length-1],o=new Array(a).fill(0),l=e.length,u="complex64"===n?fe(t):t;if(l>1)for(let t=0;t<i/a;t++){const e=t*a;for(let t=0;t<a;t++)o[t]=Math.max(o[t],ce(u[e+t],0,n).length)}return o}(t,e,n,i),o=e.length,l=de(t,e,n,i,a),u=["Tensor"];return s&&(u.push(`  dtype: ${n}`),u.push(`  rank: ${o}`),u.push(`  shape: [${e}]`),u.push("  values:")),u.push(l.map((t=>"    "+t)).join("\n")),u.join("\n")}function ce(t,e,n){let s;return s=Array.isArray(t)?`${parseFloat(t[0].toFixed(7))} + ${parseFloat(t[1].toFixed(7))}j`:p(t)?`'${t}'`:"bool"===n?pe(t):parseFloat(t.toFixed(7)).toString(),l(s,e)}function pe(t){return 0===t?"false":"true"}function de(t,e,n,s,i,r=!0){const a="complex64"===n?2:1,o=e[0],l=e.length;if(0===l){if("complex64"===n){return[ce(fe(t)[0],0,n)]}return"bool"===n?[pe(t[0])]:[t[0].toString()]}if(1===l){if(o>20){const e=3*a;let s=Array.from(t.slice(0,e)),r=Array.from(t.slice((o-3)*a,o*a));return"complex64"===n&&(s=fe(s),r=fe(r)),["["+s.map(((t,e)=>ce(t,i[e],n))).join(", ")+", ..., "+r.map(((t,e)=>ce(t,i[o-3+e],n))).join(", ")+"]"]}return["["+("complex64"===n?fe(t):Array.from(t)).map(((t,e)=>ce(t,i[e],n))).join(", ")+"]"]}const u=e.slice(1),h=s.slice(1),c=s[0]*a,p=[];if(o>20){for(let e=0;e<3;e++){const s=e*c,r=s+c;p.push(...de(t.slice(s,r),u,n,h,i,!1))}p.push("...");for(let e=o-3;e<o;e++){const s=e*c,r=s+c;p.push(...de(t.slice(s,r),u,n,h,i,e===o-1))}}else for(let e=0;e<o;e++){const s=e*c,r=s+c;p.push(...de(t.slice(s,r),u,n,h,i,e===o-1))}const d=2===l?",":"";p[0]="["+p[0]+d;for(let t=1;t<p.length-1;t++)p[t]=" "+p[t]+d;let f=",\n";for(let t=2;t<l;t++)f+="\n";return p[p.length-1]=" "+p[p.length-1]+"]"+(r?"":f),p}function fe(t){const e=[];for(let n=0;n<t.length;n+=2)e.push([t[n],t[n+1]]);return e}let ge=null,me=null;class ye{constructor(t,e,n,s){this.kept=!1,this.isDisposedInternal=!1,this.shape=t.slice(),this.dtype=e||"float32",this.size=r(t),this.strides=g(t),this.dataId=n,this.id=s,this.rankType=this.rank<5?this.rank.toString():"higher"}get rank(){return this.shape.length}async buffer(){const t=await this.data();return me.buffer(this.shape,this.dtype,t)}bufferSync(){return me.buffer(this.shape,this.dtype,this.dataSync())}async array(){const t=await this.data();return y(this.shape,t,"complex64"===this.dtype)}arraySync(){return y(this.shape,this.dataSync(),"complex64"===this.dtype)}async data(){this.throwIfDisposed();const t=ge().read(this.dataId);if("string"===this.dtype){const e=await t;try{return e.map((t=>ae(t)))}catch(t){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}}return t}dataSync(){this.throwIfDisposed();const t=ge().readSync(this.dataId);if("string"===this.dtype)try{return t.map((t=>ae(t)))}catch(t){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}return t}async bytes(){this.throwIfDisposed();const t=await ge().read(this.dataId);return"string"===this.dtype?t:new Uint8Array(t.buffer)}dispose(){this.isDisposed||(ge().disposeTensor(this),this.isDisposedInternal=!0)}get isDisposed(){return this.isDisposedInternal}throwIfDisposed(){if(this.isDisposed)throw new Error("Tensor is disposed.")}print(t=!1){return me.print(this,t)}clone(){return this.throwIfDisposed(),me.clone(this)}toString(t=!1){return he(this.dataSync(),this.shape,this.dtype,t)}cast(t){return this.throwIfDisposed(),me.cast(this,t)}variable(t=!0,e,n){return this.throwIfDisposed(),ge().makeVariable(this,t,e,n)}}function be(){return C("Tensor",(()=>ye))}Object.defineProperty(ye,Symbol.hasInstance,{value:t=>!!t&&null!=t.data&&null!=t.dataSync&&null!=t.throwIfDisposed}),be();class we extends ye{constructor(t,e,n,s){super(t.shape,t.dtype,t.dataId,s),this.trainable=e,this.name=n}assign(t){if(t.dtype!==this.dtype)throw new Error(`dtype of the new value (${t.dtype}) and previous value (${this.dtype}) must match`);if(!a(t.shape,this.shape))throw new Error(`shape of the new value (${t.shape}) and previous value (${this.shape}) must match`);ge().disposeTensor(this),this.dataId=t.dataId,ge().incRef(this,null)}dispose(){ge().disposeVariable(this),this.isDisposedInternal=!0}}var ke,xe,ve,Se,Ie;Object.defineProperty(we,Symbol.hasInstance,{value:t=>t instanceof ye&&null!=t.assign&&t.assign instanceof Function}),function(t){t.R0="R0",t.R1="R1",t.R2="R2",t.R3="R3",t.R4="R4",t.R5="R5",t.R6="R6"}(ke||(ke={})),function(t){t.float32="float32",t.int32="int32",t.bool="int32",t.complex64="complex64"}(xe||(xe={})),function(t){t.float32="float32",t.int32="int32",t.bool="bool",t.complex64="complex64"}(ve||(ve={})),function(t){t.float32="float32",t.int32="float32",t.bool="float32",t.complex64="complex64"}(Se||(Se={})),function(t){t.float32="complex64",t.int32="complex64",t.bool="complex64",t.complex64="complex64"}(Ie||(Ie={}));const Ne={float32:Se,int32:xe,bool:ve,complex64:Ie};function ze(t,e){if(t.dtype===e.dtype)return[t,e];const n=function(t,e){if("string"===t||"string"===e){if("string"===t&&"string"===e)return"string";throw new Error(`Can not upcast ${t} with ${e}`)}return Ne[t][e]}(t.dtype,e.dtype);return[t.cast(n),e.cast(n)]}function Ae(t){const e=[];return Ce(t,e,new Set),e}function Ce(t,e,n){if(null==t)return;if(t instanceof ye)return void e.push(t);if(s=t,!Array.isArray(s)&&"object"!=typeof s)return;var s;const i=t;for(const t in i){const s=i[t];n.has(s)||(n.add(s),Ce(s,e,n))}}function $e(t){return null!=t.kernelName}class De{constructor(){this.registeredVariables={},this.nextTapeNodeId=0,this.numBytes=0,this.numTensors=0,this.numStringTensors=0,this.numDataBuffers=0,this.gradientDepth=0,this.kernelDepth=0,this.scopeStack=[],this.numDataMovesStack=[],this.nextScopeId=0,this.tensorInfo=new WeakMap,this.profiling=!1,this.activeProfile={newBytes:0,newTensors:0,peakBytes:0,kernels:[],result:null,get kernelNames(){return Array.from(new Set(this.kernels.map((t=>t.name))))}}}dispose(){for(const t in this.registeredVariables)this.registeredVariables[t].dispose()}}class Te{constructor(t){this.ENV=t,this.registry={},this.registryFactory={},this.pendingBackendInitId=0,this.state=new De}async ready(){if(null!=this.pendingBackendInit)return this.pendingBackendInit.then((()=>{}));if(null!=this.backendInstance)return;const t=this.getSortedBackends();for(let e=0;e<t.length;e++){const n=t[e];if(await this.initializeBackend(n).success)return void await this.setBackend(n)}throw new Error("Could not initialize any backends, all backend initializations failed.")}get backend(){if(null!=this.pendingBackendInit)throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);if(null==this.backendInstance){const{name:t,asyncInit:e}=this.initializeBackendsAndReturnBest();if(e)throw new Error(`The highest priority backend '${t}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);this.setBackend(t)}return this.backendInstance}backendNames(){return Object.keys(this.registryFactory)}findBackend(t){if(!(t in this.registry)){if(!(t in this.registryFactory))return null;{const{asyncInit:e}=this.initializeBackend(t);if(e)return null}}return this.registry[t]}findBackendFactory(t){return t in this.registryFactory?this.registryFactory[t].factory:null}registerBackend(t,e,n=1){return t in this.registryFactory?(console.warn(`${t} backend was already registered. Reusing existing backend factory.`),!1):(this.registryFactory[t]={factory:e,priority:n},!0)}async setBackend(t){if(null==this.registryFactory[t])throw new Error(`Backend name '${t}' not found in registry`);if(this.backendName=t,null==this.registry[t]){this.backendInstance=null;const{success:e,asyncInit:n}=this.initializeBackend(t);if(!(n?await e:e))return!1}return this.backendInstance=this.registry[t],this.setupRegisteredKernels(),this.profiler=new oe(this.backendInstance),!0}setupRegisteredKernels(){ne(this.backendName).forEach((t=>{null!=t.setupFunc&&t.setupFunc(this.backendInstance)}))}disposeRegisteredKernels(t){ne(t).forEach((e=>{null!=e.disposeFunc&&e.disposeFunc(this.registry[t])}))}initializeBackend(t){const e=this.registryFactory[t];if(null==e)throw new Error(`Cannot initialize backend ${t}, no registration found.`);try{const s=e.factory();if(!s||s instanceof class{refCount(t){return n("refCount")}incRef(t){return n("incRef")}timerAvailable(){return!0}time(t){return n("time")}read(t){return n("read")}readSync(t){return n("readSync")}numDataIds(){return n("numDataIds")}disposeData(t,e){return n("disposeData")}write(t,e,s){return n("write")}move(t,e,s,i,r){return n("move")}memory(){return n("memory")}floatPrecision(){return n("floatPrecision")}epsilon(){return 32===this.floatPrecision()?1e-7:1e-4}dispose(){return n("dispose")}}||"function"!=typeof s.then)return this.registry[t]=s,{success:!0,asyncInit:!1};{const e=++this.pendingBackendInitId,n=s.then((n=>!(e<this.pendingBackendInitId)&&(this.registry[t]=n,this.pendingBackendInit=null,!0))).catch((n=>(e<this.pendingBackendInitId||(this.pendingBackendInit=null,console.warn(`Initialization of backend ${t} failed`),console.warn(n.stack||n.message)),!1)));return this.pendingBackendInit=n,{success:n,asyncInit:!0}}}catch(e){return console.warn(`Initialization of backend ${t} failed`),console.warn(e.stack||e.message),{success:!1,asyncInit:!1}}}removeBackend(t){if(!(t in this.registryFactory))throw new Error(`${t} backend not found in registry`);this.backendName===t&&null!=this.pendingBackendInit&&this.pendingBackendInitId++,t in this.registry&&(this.disposeRegisteredKernels(t),this.registry[t].dispose(),delete this.registry[t]),delete this.registryFactory[t],this.backendName===t&&(this.pendingBackendInit=null,this.backendName=null,this.backendInstance=null)}getSortedBackends(){if(0===Object.keys(this.registryFactory).length)throw new Error("No backend found in registry.");return Object.keys(this.registryFactory).sort(((t,e)=>this.registryFactory[e].priority-this.registryFactory[t].priority))}initializeBackendsAndReturnBest(){const t=this.getSortedBackends();for(let e=0;e<t.length;e++){const n=t[e],{success:s,asyncInit:i}=this.initializeBackend(n);if(i||s)return{name:n,asyncInit:i}}throw new Error("Could not initialize any backends, all backend initializations failed.")}moveData(t,e){const n=this.state.tensorInfo.get(e),s=n.backend,i=this.readSync(e),r=s.refCount(e);s.disposeData(e,!0),n.backend=t,t.move(e,i,n.shape,n.dtype,r),this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack[this.state.numDataMovesStack.length-1]++}tidy(t,e){let n,s=null;if(null==e){if("function"!=typeof t)throw new Error("Please provide a function to tidy()");e=t}else{if("string"!=typeof t&&!(t instanceof String))throw new Error("When calling with two arguments, the first argument to tidy() must be a string");if("function"!=typeof e)throw new Error("When calling with two arguments, the 2nd argument to tidy() must be a function");s=t}return this.scopedRun((()=>this.startScope(s)),(()=>this.endScope(n)),(()=>(n=e(),n instanceof Promise&&console.error("Cannot return a Promise inside of tidy."),n)))}scopedRun(t,e,n){t();try{const t=n();return e(),t}catch(t){throw e(),t}}nextTensorId(){return Te.nextTensorId++}nextVariableId(){return Te.nextVariableId++}clone(t){const e=Ee.runKernel(ot,{x:t}),n={x:t};return this.addTapeNode(this.state.activeScope.name,n,[e],(t=>({x:()=>{const e={x:t},n={dtype:"float32"};return Ee.runKernel(U,e,n)}})),[],{}),e}runKernel(t,e,n){if(!(null!=te(t,this.backendName)))throw new Error(`Kernel '${t}' not registered for backend '${this.backendName}'`);return this.runKernelFunc({kernelName:t,inputs:e,attrs:n})}shouldCheckForMemLeaks(){return this.ENV.getBool("IS_TEST")}checkKernelForMemLeak(t,e,n){const s=this.backend.numDataIds();let i=0;n.forEach((t=>{i+="complex64"===t.dtype?3:1}));const r=this.state.numDataMovesStack[this.state.numDataMovesStack.length-1],a=s-e-i-r;if(a>0)throw new Error(`Backend '${this.backendName}' has an internal memory leak (${a} data ids) after running '${t}'`)}runKernelFunc(t){let e,n=[];const i=this.isTapeOn(),r=this.state.numBytes,a=this.state.numTensors;let o,l;this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack.push(0),null==this.backendName&&this.backend;const u=$e(t)?t.kernelName:null!=this.state.activeScope?this.state.activeScope.name:"";if($e(t)){const{kernelName:e,inputs:r,attrs:a}=t;null==this.backendName&&this.backend;const u=te(e,this.backendName);s(null!=u,(()=>`Cannot find registered kernel '${e}' for backend '${this.backendName}'`)),o=()=>{const t=this.backend.numDataIds();l=u.kernelFunc({inputs:r,attrs:a,backend:this.backend});const s=Array.isArray(l)?l:[l];this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(e,t,s);const o=s.map((t=>{if(null!=t.rank)return t;const{dataId:e,shape:n,dtype:s}=t;return this.makeTensorFromDataId(e,n,s)}));if(i){const t=this.getTensorsForGradient(e,r,o);n=this.saveTensorsForBackwardMode(t)}return o}}else{const{forwardFunc:e}=t,s=t=>{i&&(n=t.map((t=>this.keep(this.clone(t)))))};o=()=>{const t=this.backend.numDataIds();l=this.tidy((()=>e(this.backend,s)));const n=Array.isArray(l)?l:[l];return this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(u,t,n),n}}const{inputs:h,attrs:c}=t,p=$e(t)?null:t.backwardsFunc;let d;return this.scopedRun((()=>this.state.kernelDepth++),(()=>this.state.kernelDepth--),(()=>{this.ENV.getBool("DEBUG")||this.state.profiling?(d=this.profiler.profileKernel(u,h,(()=>o())),this.ENV.getBool("DEBUG")&&this.profiler.logKernelProfile(d),e=d.outputs):e=o()})),i&&this.addTapeNode(u,h,e,p,n,c),this.state.profiling&&this.state.activeProfile.kernels.push({name:u,bytesAdded:this.state.numBytes-r,totalBytesSnapshot:this.state.numBytes,tensorsAdded:this.state.numTensors-a,totalTensorsSnapshot:this.state.numTensors,inputShapes:Object.keys(h).map((t=>null!=h[t]?h[t].shape:null)),outputShapes:e.map((t=>t.shape)),kernelTimeMs:d.timeMs,extraInfo:d.extraInfo}),Array.isArray(l)?e:e[0]}saveTensorsForBackwardMode(t){return t.map((t=>this.keep(this.clone(t))))}getTensorsForGradient(t,e,n){const i=ee(t);if(null!=i){const t=i.inputsToSave||[],r=i.outputsToSave||[];let a;i.saveAllInputs?(s(Array.isArray(e),(()=>"saveAllInputs is true, expected inputs to be an array.")),a=Object.keys(e).map((t=>e[t]))):a=t.map((t=>e[t]));const o=n.filter(((t,e)=>r[e]));return a.concat(o)}return[]}makeTensor(t,e,n,s){if(null==t)throw new Error("Values passed to engine.makeTensor() are null");n=n||"float32",s=s||this.backend;let i=t;"string"===n&&p(t[0])&&(i=t.map((t=>function(t,e="utf-8"){return e=e||"utf-8",I().platform.encode(t,e)}(t))));const r=s.write(i,e,n),a=new ye(e,n,r,this.nextTensorId());if(this.trackTensor(a,s),"string"===n){const t=this.state.tensorInfo.get(r),e=function(t){if(null==t)return 0;let e=0;return t.forEach((t=>e+=t.length)),e}(i);this.state.numBytes+=e-t.bytes,t.bytes=e}return a}makeTensorFromDataId(t,e,n,s){const i=new ye(e,n=n||"float32",t,this.nextTensorId());return this.trackTensor(i,s),i}makeVariable(t,e=!0,n,s){n=n||this.nextVariableId().toString(),null!=s&&s!==t.dtype&&(t=t.cast(s));const i=new we(t,e,n,this.nextTensorId());if(null!=this.state.registeredVariables[i.name])throw new Error(`Variable with name ${i.name} was already registered`);return this.state.registeredVariables[i.name]=i,this.incRef(i,this.backend),i}trackTensor(t,e){this.state.numTensors++,"string"===t.dtype&&this.state.numStringTensors++;let n=0;"complex64"!==t.dtype&&"string"!==t.dtype&&(n=t.size*c(t.dtype)),this.state.numBytes+=n,this.state.tensorInfo.has(t.dataId)||(this.state.numDataBuffers++,this.state.tensorInfo.set(t.dataId,{backend:e||this.backend,dtype:t.dtype,shape:t.shape,bytes:n})),t instanceof we||this.track(t)}incRef(t,e){this.trackTensor(t,e),this.backend.incRef(t.dataId)}removeDataId(t,e){this.state.tensorInfo.has(t)&&this.state.tensorInfo.get(t).backend===e&&(this.state.tensorInfo.delete(t),this.state.numDataBuffers--)}disposeTensor(t){if(!this.state.tensorInfo.has(t.dataId))return;const e=this.state.tensorInfo.get(t.dataId);if(this.state.numTensors--,"string"===t.dtype&&(this.state.numStringTensors--,this.state.numBytes-=e.bytes),"complex64"!==t.dtype&&"string"!==t.dtype){const e=t.size*c(t.dtype);this.state.numBytes-=e}e.backend.disposeData(t.dataId)&&this.removeDataId(t.dataId,e.backend)}disposeVariables(){for(const t in this.state.registeredVariables){const e=this.state.registeredVariables[t];this.disposeVariable(e)}}disposeVariable(t){this.disposeTensor(t),null!=this.state.registeredVariables[t.name]&&delete this.state.registeredVariables[t.name]}memory(){const t=this.backend.memory();return t.numTensors=this.state.numTensors,t.numDataBuffers=this.state.numDataBuffers,t.numBytes=this.state.numBytes,this.state.numStringTensors>0&&(t.unreliable=!0,null==t.reasons&&(t.reasons=[]),t.reasons.push("Memory usage by string tensors is approximate (2 bytes per character)")),t}async profile(t){this.state.profiling=!0;const e=this.state.numBytes,n=this.state.numTensors;this.state.activeProfile.kernels=[],this.state.activeProfile.result=await t(),this.state.profiling=!1,this.state.activeProfile.peakBytes=Math.max(...this.state.activeProfile.kernels.map((t=>t.totalBytesSnapshot))),this.state.activeProfile.newBytes=this.state.numBytes-e,this.state.activeProfile.newTensors=this.state.numTensors-n;for(const t of this.state.activeProfile.kernels)t.kernelTimeMs=await t.kernelTimeMs,t.extraInfo=await t.extraInfo;return this.state.activeProfile}isTapeOn(){return this.state.gradientDepth>0&&0===this.state.kernelDepth}addTapeNode(t,e,n,s,i,r){const a={id:this.state.nextTapeNodeId++,kernelName:t,inputs:e,outputs:n,saved:i},o=ee(t);null!=o&&(s=o.gradFunc),null!=s&&(a.gradient=t=>(t=t.map(((t,e)=>{if(null==t){const t=n[e],s=w(t.size,t.dtype);return this.makeTensor(s,t.shape,t.dtype)}return t})),s(t.length>1?t:t[0],i,r))),this.state.activeTape.push(a)}keep(t){return t.kept=!0,t}startTape(){0===this.state.gradientDepth&&(this.state.activeTape=[]),this.state.gradientDepth++}endTape(){this.state.gradientDepth--}startScope(t){const e={track:[],name:"unnamed scope",id:this.state.nextScopeId++};t&&(e.name=t),this.state.scopeStack.push(e),this.state.activeScope=e}endScope(t){const e=Ae(t),n=new Set(e.map((t=>t.id)));for(let t=0;t<this.state.activeScope.track.length;t++){const e=this.state.activeScope.track[t];e.kept||n.has(e.id)||e.dispose()}const s=this.state.scopeStack.pop();this.state.activeScope=0===this.state.scopeStack.length?null:this.state.scopeStack[this.state.scopeStack.length-1],e.forEach((t=>{t.kept||t.scopeId!==s.id||this.track(t)}))}gradients(t,e,n,i=!1){if(s(e.length>0,(()=>"gradients() received an empty list of xs.")),null!=n&&"float32"!==n.dtype)throw new Error(`dy must have 'float32' dtype, but has '${n.dtype}'`);const o=this.scopedRun((()=>this.startTape()),(()=>this.endTape()),(()=>this.tidy("forward",t)));s(o instanceof ye,(()=>"The result y returned by f() must be a tensor."));const l=function(t,e,n){const s={},i={};for(let t=0;t<e.length;t++)s[e[t].id]=!0;for(let n=0;n<t.length;n++){const r=t[n],a=r.inputs;for(const t in a){const n=a[t];let o=!1;for(let t=0;t<e.length;t++)if(s[n.id]){r.outputs.forEach((t=>s[t.id]=!0)),o=!0,i[r.id]=!0;break}if(o)break}}const r={};r[n.id]=!0;const a={};for(let e=t.length-1;e>=0;e--){const n=t[e],s=n.inputs;for(let t=0;t<n.outputs.length;t++)if(r[n.outputs[t].id]){for(const t in s)r[s[t].id]=!0,a[n.id]=!0;break}}const o=[];for(let e=0;e<t.length;e++){const n=t[e];if(i[n.id]&&a[n.id]){const t={};for(const e in n.inputs){const i=n.inputs[e];s[i.id]&&(t[e]=i)}const e=Object.assign({},n);e.inputs=t,e.outputs=n.outputs,o.push(e)}}return o}(this.state.activeTape,e,o);if(!i&&0===l.length&&e.length>0)throw new Error("Cannot compute gradient of y=f(x) with respect to x. Make sure that the f you passed encloses all operations that lead from x to y.");return this.tidy("backward",(()=>{const t={};t[o.id]=null==n?function(t){const e=b(r(t),"float32");return Ee.makeTensor(e,t,"float32")}(o.shape):n,function(t,e,n,s){for(let i=e.length-1;i>=0;i--){const r=e[i],o=[];if(r.outputs.forEach((e=>{const n=t[e.id];null!=n?o.push(n):o.push(null)})),null==r.gradient)throw new Error(`Cannot compute gradient: gradient function not found for ${r.kernelName}.`);const l=r.gradient(o);for(const e in r.inputs){if(!(e in l))throw new Error(`Cannot backprop through input ${e}. Available gradients found: ${Object.keys(l)}.`);const i=n((()=>l[e]()));if("float32"!==i.dtype)throw new Error(`Error in gradient for op ${r.kernelName}. The gradient of input ${e} must have 'float32' dtype, but has '${i.dtype}'`);const o=r.inputs[e];if(!a(i.shape,o.shape))throw new Error(`Error in gradient for op ${r.kernelName}. The gradient of input '${e}' has shape '${i.shape}', which does not match the shape of the input '${o.shape}'`);if(null==t[o.id])t[o.id]=i;else{const e=t[o.id];t[o.id]=s(e,i),e.dispose()}}}}(t,l,(t=>this.tidy(t)),Fe);const s=e.map((e=>t[e.id]));return 0===this.state.gradientDepth&&(this.state.activeTape.forEach((t=>{for(const e of t.saved)e.dispose()})),this.state.activeTape=null),{value:o,grads:s}}))}customGrad(t){return s(f(t),(()=>"The f passed in customGrad(f) must be a function.")),(...e)=>{let n;s(e.every((t=>t instanceof ye)),(()=>"The args passed in customGrad(f)(x1, x2,...) must all be tensors"));const i={};e.forEach(((t,e)=>{i[e]=t}));return this.runKernelFunc({forwardFunc:(i,r)=>(n=t(...e,r),s(n.value instanceof ye,(()=>"The function f passed in customGrad(f) must return an object where `obj.value` is a tensor")),s(f(n.gradFunc),(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function.")),n.value),backwardsFunc:(t,i)=>{const r=n.gradFunc(t,i),a=Array.isArray(r)?r:[r];s(a.length===e.length,(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns the same number of tensors as inputs passed to f(...).")),s(a.every((t=>t instanceof ye)),(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns a list of only tensors."));const o={};return a.forEach(((t,e)=>{o[e]=()=>t})),o},inputs:i})}}readSync(t){return this.state.tensorInfo.get(t).backend.readSync(t)}read(t){return this.state.tensorInfo.get(t).backend.read(t)}async time(t){const e=re(),n=await this.backend.time(t);return n.wallMs=re()-e,n}track(t){return null!=this.state.activeScope&&(t.scopeId=this.state.activeScope.id,this.state.activeScope.track.push(t)),t}get registeredVariables(){return this.state.registeredVariables}reset(){this.pendingBackendInitId++,this.state.dispose(),this.ENV.reset(),this.state=new De;for(const t in this.registry)this.disposeRegisteredKernels(t),this.registry[t].dispose(),delete this.registry[t];this.backendName=null,this.backendInstance=null,this.pendingBackendInit=null}}Te.nextTensorId=0,Te.nextVariableId=0;const Ee=function(){const t=A();if(null==t._tfengine){const e=new v(t);t._tfengine=new Te(e)}var e;return e=t._tfengine.ENV,z=e,ge=()=>t._tfengine,t._tfengine}();function Fe(t,e){const n={a:t,b:e};return Ee.runKernel(T,n)}function Le(t,e){let n=t;if(h(t))return"string"===e?[]:[t.length];if(!Array.isArray(t))return[];const s=[];for(;Array.isArray(n)||h(n)&&"string"!==e;)s.push(n.length),n=n[0];return Array.isArray(t)&&I().getBool("TENSORLIKE_CHECK_SHAPE_CONSISTENCY")&&_e(t,s,[]),s}function _e(t,e,n){if(n=n||[],!Array.isArray(t)&&!h(t))return void s(0===e.length,(()=>`Element arr[${n.join("][")}] is a primitive, but should be an array/TypedArray of ${e[0]} elements`));s(e.length>0,(()=>`Element arr[${n.join("][")}] should be a primitive, but is an array of ${t.length} elements`)),s(t.length===e[0],(()=>`Element arr[${n.join("][")}] should have ${e[0]} elements, but has ${t.length} elements`));const i=e.slice(1);for(let e=0;e<t.length;++e)_e(t[e],i,n.concat(e))}function Re(t,e,n,s){if("string_or_numeric"!==t){if(null==t)throw new Error("Expected dtype cannot be null.");if("numeric"!==t&&t!==e||"numeric"===t&&"string"===e)throw new Error(`Argument '${n}' passed to '${s}' must be ${t} tensor, but got ${e} tensor`)}}function Me(t,e,n,s="numeric"){if(t instanceof ye)return Re(s,t.dtype,e,n),t;let r=d(t);if("string"!==r&&["bool","int32","float32"].indexOf(s)>=0&&(r=s),Re(s,r,e,n),null==t||!h(t)&&!Array.isArray(t)&&"number"!=typeof t&&"boolean"!=typeof t&&"string"!=typeof t){const s=null==t?"null":t.constructor.name;throw new Error(`Argument '${e}' passed to '${n}' must be a Tensor or TensorLike, but got '${s}'`)}const a=Le(t,r);h(t)||Array.isArray(t)||(t=[t]);const o="string"!==r?ie(t,r):i(t,[],!0);return Ee.makeTensor(o,a,r)}function Oe(t,e,n,s="numeric"){if(!Array.isArray(t))throw new Error(`Argument ${e} passed to ${n} must be a \`Tensor[]\` or \`TensorLike[]\``);return t.map(((t,i)=>Me(t,`${e}[${i}]`,n,s)))}function Be(t){const e=Object.keys(t);if(1!==e.length)throw new Error(`Please provide an object with a single key (operation name) mapping to a function. Got an object with ${e.length} keys.`);let n=e[0];const s=t[n];n.endsWith("_")&&(n=n.substring(0,n.length-1)),n+="__op";const i=(...t)=>{Ee.startScope(n);try{const e=s(...t);return k(e)&&console.error("Cannot return a Promise inside of tidy."),Ee.endScope(e),e}catch(t){throw Ee.endScope(null),t}};return Object.defineProperty(i,"name",{value:n,configurable:!0}),i}const We=Be({abs_:function(t){const e=Me(t,"x","abs");if("complex64"===e.dtype){const t={x:e};return Ee.runKernel(j,t)}{const t={x:e};return Ee.runKernel("Abs",t)}}});const Pe=Be({acos_:function(t){const e={x:Me(t,"x","acos")};return Ee.runKernel($,e)}});const Ue=Be({acosh_:function(t){const e={x:Me(t,"x","acosh")};return Ee.runKernel(D,e)}});const Ke=Be({add_:function(t,e){let n=Me(t,"a","add"),s=Me(e,"b","add");[n,s]=ze(n,s);const i={a:n,b:s};return Ee.runKernel(T,i)}});const Ve=Be({all_:function(t,e=null,n=!1){const s={x:Me(t,"x","all","bool")},i={axis:e,keepDims:n};return Ee.runKernel("All",s,i)}});const je=Be({any_:function(t,e=null,n=!1){const s={x:Me(t,"x","any","bool")},i={axis:e,keepDims:n};return Ee.runKernel("Any",s,i)}});const qe=Be({argMax_:function(t,e=0){const n={x:Me(t,"x","argMax")},s={axis:e};return Ee.runKernel(E,n,s)}});const Ge=Be({argMin_:function(t,e=0){const n={x:Me(t,"x","argMin")},s={axis:e};return Ee.runKernel(F,n,s)}});const He=Be({asin_:function(t){const e={x:Me(t,"x","asin")};return Ee.runKernel(L,e)}});const Je=Be({asinh_:function(t){const e={x:Me(t,"x","asinh")};return Ee.runKernel(_,e)}});const Ze=Be({atan_:function(t){const e={x:Me(t,"x","atan")};return Ee.runKernel(R,e)}});const Xe=Be({atan2_:function(t,e){let n=Me(t,"a","atan2"),s=Me(e,"b","atan2");[n,s]=ze(n,s);const i={a:n,b:s};return Ee.runKernel(O,i)}});const Ye=Be({atanh_:function(t){const e={x:Me(t,"x","atanh")};return Ee.runKernel(M,e)}});const Qe=Be({cast_:function(t,e){const n=Me(t,"x","cast");if(!function(t){return"bool"===t||"complex64"===t||"float32"===t||"int32"===t||"string"===t}(e))throw new Error(`Failed to cast to unknown dtype ${e}`);if("string"===e&&"string"!==n.dtype||"string"!==e&&"string"===n.dtype)throw new Error("Only strings can be casted to strings");const s={x:n},i={dtype:e};return Ee.runKernel(U,s,i)}});function tn(t,e,n,s,i,r,a="channelsLast"){const[o,l]=en(e);let u;if("channelsLast"===a)u=[o,l,t[3],t[3]];else{if("channelsFirst"!==a)throw new Error(`Unknown dataFormat ${a}`);u=[o,l,t[1],t[1]]}return function(t,e,n,s,i,r,a=!1,o="channelsLast"){let[l,u,h,c]=[-1,-1,-1,-1];if("channelsLast"===o)[l,u,h,c]=t;else{if("channelsFirst"!==o)throw new Error(`Unknown dataFormat ${o}`);[l,c,u,h]=t}const[p,d,,f]=e,[g,m]=en(n),[y,b]=en(s),w=nn(p,y),k=nn(d,b),{padInfo:x,outHeight:v,outWidth:S}=function(t,e,n,s,i,r,a,o,l){let u,h,c;if("number"==typeof t){u={top:t,bottom:t,left:t,right:t,type:0===t?"VALID":"NUMBER"};const i=function(t,e,n,s,i){null==s&&(s=function(t,e,n,s=1){const i=nn(e,s);return Math.floor((t[0]*(n-1)-n+i)/2)}(t,e,n));const r=t[0],a=t[1],o=sn((r-e+2*s)/n+1,i),l=sn((a-e+2*s)/n+1,i);return[o,l]}([e,n],r,s,t,o);h=i[0],c=i[1]}else if("same"===t){h=Math.ceil(e/s),c=Math.ceil(n/i);const t=Math.max(0,(h-1)*s+r-e),o=Math.max(0,(c-1)*i+a-n),l=Math.floor(t/2),p=t-l,d=Math.floor(o/2);u={top:l,bottom:p,left:d,right:o-d,type:"SAME"}}else if("valid"===t)u={top:0,bottom:0,left:0,right:0,type:"VALID"},h=Math.ceil((e-r+1)/s),c=Math.ceil((n-a+1)/i);else{if("object"!=typeof t)throw Error(`Unknown padding parameter: ${t}`);{const p="channelsLast"===l?t[1][0]:t[2][0],d="channelsLast"===l?t[1][1]:t[2][1],f="channelsLast"===l?t[2][0]:t[3][0],g="channelsLast"===l?t[2][1]:t[3][1];u={top:p,bottom:d,left:f,right:g,type:0===p&&0===d&&0===f&&0===g?"VALID":"EXPLICIT"},h=sn((e-r+p+d)/s+1,o),c=sn((n-a+f+g)/i+1,o)}}return{padInfo:u,outHeight:h,outWidth:c}}(i,u,h,g,m,w,k,r,o),I=a?f*c:f;let N;"channelsFirst"===o?N=[l,I,v,S]:"channelsLast"===o&&(N=[l,v,S,I]);return{batchSize:l,dataFormat:o,inHeight:u,inWidth:h,inChannels:c,outHeight:v,outWidth:S,outChannels:I,padInfo:x,strideHeight:g,strideWidth:m,filterHeight:p,filterWidth:d,effectiveFilterHeight:w,effectiveFilterWidth:k,dilationHeight:y,dilationWidth:b,inShape:t,outShape:N,filterShape:e}}(t,u,n,s,i,r,!1,a)}function en(t){return"number"==typeof t?[t,t,t]:2===t.length?[t[0],t[1],1]:t}function nn(t,e){return e<=1?t:t+(t-1)*(e-1)}function sn(t,e){if(!e)return Math.trunc(t);switch(e){case"round":return Math.round(t);case"ceil":return Math.ceil(t);case"floor":return Math.floor(t);default:throw new Error(`Unknown roundingMode ${e}`)}}function rn(t){const[e,n,s]=en(t);return 1===e&&1===n&&1===s}function an(t,e){return rn(t)||rn(e)}const on=Be({reshape_:function(t,e){const n={x:Me(t,"x","reshape","string_or_numeric")},s={shape:e};return Ee.runKernel(zt,n,s)}});const ln=Be({avgPool_:function(t,e,n,i,r){const a=Me(t,"x","avgPool","float32");s(an(n,1),(()=>`Error in avgPool: Either strides or dilations must be 1. Got strides ${n} and dilations '1'`));let l=a,u=!1;3===a.rank&&(u=!0,l=on(a,[1,a.shape[0],a.shape[1],a.shape[2]])),s(4===l.rank,(()=>`Error in avgPool: x must be rank 4 but got rank ${l.rank}.`)),null!=r&&s(o(i),(()=>`Error in avgPool: pad must be an integer when using, dimRoundingMode ${r} but got pad ${i}.`));const h={x:l},c={filterSize:e,strides:n,pad:i,dimRoundingMode:r};let p=Ee.runKernel(B,h,c);return p=Qe(p,a.dtype),u?on(p,[p.shape[1],p.shape[2],p.shape[3]]):p}});const un=Be({clone_:function(t){const e={x:Me(t,"x","clone","string_or_numeric")};return Ee.runKernel(ot,e)}});const hn=Be({concat_:function(t,e=0){s(t.length>=1,(()=>"Pass at least one tensor to concat"));const n=Oe(t,"tensors","concat","string_or_numeric");if("complex64"===n[0].dtype&&n.forEach((t=>{if("complex64"!==t.dtype)throw new Error(`Cannot concatenate complex64 tensors with a tensor\n          with dtype ${t.dtype}. `)})),1===n.length)return un(n[0]);const i=n,r={axis:e};return Ee.runKernel(q,i,r)}});const cn=Be({matMul_:function(t,e,n=!1,s=!1){let i=Me(t,"a","matMul"),r=Me(e,"b","matMul");[i,r]=ze(i,r);const a={a:i,b:r},o={transposeA:n,transposeB:s};return Ee.runKernel(W,a,o)}});const pn=Be({mul_:function(t,e){let n=Me(t,"a","mul"),s=Me(e,"b","mul");[n,s]=ze(n,s);const i={a:n,b:s};return Ee.runKernel(bt,i)}});const dn=Be({sigmoid_:function(t){const e={x:Me(t,"x","sigmoid")};return Ee.runKernel(Ot,e)}});const fn=Be({slice_:function(t,e,n){const s=Me(t,"x","slice","string_or_numeric");if(0===s.rank)throw new Error("Slicing scalar is not possible");const i={x:s},r={begin:e,size:n};return Ee.runKernel(_t,i,r)}});const gn=Be({tanh_:function(t){const e={x:Me(t,"x","tanh")};return Ee.runKernel(jt,e)}});const mn=Be({batchToSpaceND_:function(t,e,n){const i=Me(t,"x","batchToSpaceND"),r=e.reduce(((t,e)=>t*e));s(i.rank>=1+e.length,(()=>`input rank is ${i.rank} but should be > than blockShape.length ${e.length}`)),s(n.length===e.length,(()=>`crops.length is ${n.length} but should be equal to blockShape.length  ${e.length}`)),s(i.shape[0]%r==0,(()=>`input tensor batch is ${i.shape[0]} but is not divisible by the product of the elements of blockShape ${e.join(" * ")} === ${r}`));const a={x:i},o={blockShape:e,crops:n};return Ee.runKernel(P,a,o)}});const yn=Be({batchNorm_:function(t,e,n,i,r,a){null==a&&(a=.001);const o=Me(t,"x","batchNorm"),l=Me(e,"mean","batchNorm"),u=Me(n,"variance","batchNorm");let h,c;null!=r&&(h=Me(r,"scale","batchNorm")),null!=i&&(c=Me(i,"offset","batchNorm")),s(l.rank===u.rank,(()=>"Batch normalization gradient requires mean and variance to have equal ranks.")),s(null==c||l.rank===c.rank,(()=>"Batch normalization gradient requires mean and offset to have equal ranks.")),s(null==h||l.rank===h.rank,(()=>"Batch normalization gradient requires mean and scale to have equal ranks."));const p={x:function(t){let e;return e=0===t.rank||1===t.rank?on(t,[1,1,1,t.size]):2===t.rank?on(t,[1,1,t.shape[0],t.shape[1]]):3===t.rank?on(t,[1,t.shape[0],t.shape[1],t.shape[2]]):t,e}(o),scale:h,offset:c,mean:l,variance:u},d={varianceEpsilon:a},f=Ee.runKernel(it,p,d);return on(f,o.shape)}});const bn=Be({broadcastTo_:function(t,e){let n=Me(t,"broadcastTo","x");const s=n.shape;if(e.some((t=>!(t>0)||t%1!=0)))throw new Error(`broadcastTo(): Invalid broadcast shape [${e}].`);if(e.length<n.rank)throw new Error(`broadcastTo(): shape.length=${e.length} < input.rank=${n.rank}.`);if(e.length>n.rank){const t=n.shape.slice();for(;t.length<e.length;)t.unshift(1);n=on(n,t)}const i=n.shape,r=Array.from(e);for(let t=e.length-1;t>=0;t--)if(i[t]===e[t])r[t]=1;else if(1!==n.shape[t])throw new Error(`broadcastTo(): [${s}] cannot be broadcast to [${e}].`);if(0===r.map(((t,e)=>t>1?e:-1)).filter((t=>t>=0)).length)return un(n);const a={x:n},o={reps:r};return Ee.runKernel(qt,a,o)}});const wn=Be({ceil_:function(t){const e={x:Me(t,"x","ceil")};return Ee.runKernel(K,e)}});const kn=Be({clipByValue_:function(t,e,n){const i=Me(t,"x","clipByValue");s(e<=n,(()=>`Error in clip: min (${e}) must be less than or equal to max (${n}).`));const r={x:i},a={clipValueMin:e,clipValueMax:n};return Ee.runKernel(V,r,a)}});const xn=Be({complex_:function(t,e){const n=Me(t,"real","complex"),i=Me(e,"imag","complex");!function(t,e,n=""){s(a(t,e),(()=>n+` Shapes ${t} and ${e} must match`))}(n.shape,i.shape,`real and imag shapes, ${n.shape} and ${i.shape}, must match in call to tf.complex().`);const r={real:n,imag:i};return Ee.runKernel("Complex",r)}});const vn=Be({conv2d_:function(t,e,n,i,r="NHWC",a=[1,1],l){const u=Me(t,"x","conv2d"),h=Me(e,"filter","conv2d");let c=u,p=!1;3===u.rank&&(p=!0,c=on(u,[1,u.shape[0],u.shape[1],u.shape[2]])),s(4===c.rank,(()=>`Error in conv2d: input must be rank 4, but got rank ${c.rank}.`)),s(4===h.rank,(()=>`Error in conv2d: filter must be rank 4, but got rank ${h.rank}.`)),null!=l&&s(o(i),(()=>`Error in conv2d: pad must be an integer when using, dimRoundingMode ${l} but got pad ${i}.`));const d="NHWC"===r?c.shape[3]:c.shape[1];s(d===h.shape[2],(()=>`Error in conv2d: depth of input (${d}) must match input depth for filter ${h.shape[2]}.`)),s(an(n,a),(()=>`Error in conv2D: Either strides or dilations must be 1. Got strides ${n} and dilations '${a}'`));const f={x:c,filter:h},g={strides:n,pad:i,dataFormat:r,dilations:a,dimRoundingMode:l},m=Ee.runKernel(G,f,g);return p?on(m,[m.shape[1],m.shape[2],m.shape[3]]):m}});const Sn=Be({conv1d_:function(t,e,n,i,r="NWC",a=1,l){const u=Me(t,"x","conv1d"),h=Me(e,"filter","conv1d");let c=u,p=!1;2===u.rank&&(p=!0,c=on(u,[1,u.shape[0],u.shape[1]])),s(3===c.rank,(()=>`Error in conv1d: input must be rank 3, but got rank ${c.rank}.`)),s(3===h.rank,(()=>`Error in conv1d: filter must be rank 3, but got rank ${h.rank}.`)),null!=l&&s(o(i),(()=>`Error in conv1d: pad must be an integer when using, dimRoundingMode ${l} but got pad ${i}.`)),s(c.shape[2]===h.shape[1],(()=>`Error in conv1d: depth of input (${c.shape[2]}) must match input depth for filter ${h.shape[1]}.`)),s(an(n,a),(()=>`Error in conv1D: Either stride or dilation must be 1. Got stride ${n} and dilation '${a}'`)),s("NWC"===r,(()=>`Error in conv1d: got dataFormat of ${r} but only NWC is currently supported.`));const d=on(h,[1,h.shape[0],h.shape[1],h.shape[2]]),f=on(c,[c.shape[0],1,c.shape[1],c.shape[2]]),g=vn(f,d,[1,n],i,"NHWC",[1,a],l);return on(g,p?[g.shape[2],g.shape[3]]:[g.shape[0],g.shape[2],g.shape[3]])}});const In=Be({conv2DBackpropInput_:function(t,e,n,i,r,a="NHWC",l){s(t.length===e.rank,(()=>`Length of inShape (${t.length}) and rank of dy (${e.rank}) must match`));let u=t,h=e,c=!1;3===e.rank&&(c=!0,h=on(e,[1,e.shape[0],e.shape[1],e.shape[2]]),u=[1,t[0],t[1],t[2]]),s(4===u.length,(()=>`Error in conv2dDerInput: inShape must be length 4, but got length ${u.length}.`)),s(4===h.rank,(()=>`Error in conv2dDerInput: dy must be rank 4, but got rank ${h.rank}`)),s(4===n.rank,(()=>`Error in conv2dDerInput: filter must be rank 4, but got rank ${n.rank}`));const p="NHWC"===a?u[3]:u[1],d="NHWC"===a?h.shape[3]:h.shape[1];s(p===n.shape[2],(()=>`Error in conv2dDerInput: depth of input (${p}) must match input depth for filter ${n.shape[2]}.`)),s(d===n.shape[3],(()=>`Error in conv2dDerInput: depth of output (${d}) must match output depth for filter ${n.shape[3]}.`)),null!=l&&s(o(r),(()=>`Error in conv2dDerInput: pad must be an integer when using, dimRoundingMode ${l} but got pad ${r}.`));const f={dy:h,filter:n},g={strides:i,pad:r,dataFormat:a,dimRoundingMode:l,inputShape:u},m=Ee.runKernel(H,f,g);return c?on(m,[m.shape[1],m.shape[2],m.shape[3]]):m}});const Nn=Be({conv2dTranspose_:function(t,e,n,s,i,r){const a=Me(t,"x","conv2dTranspose"),o=Me(e,"filter","conv2dTranspose");return In(n,a,o,s,i,"NHWC",r)}});const zn=Be({conv3DBackpropInput_:function(t,e,n,i,r){s(t.length===e.rank,(()=>`Length of inShape (${t.length}) and rank of dy (${e.rank}) must match`));let a=t,o=e,l=!1;4===e.rank&&(l=!0,o=on(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]]),a=[1,t[0],t[1],t[2],t[3]]);const u=a[4],h=o.shape[4];s(5===a.length,(()=>`Error in conv3dDerInput: inShape must be length 5, but got length ${a.length}.`)),s(5===o.rank,(()=>`Error in conv3dDerInput: dy must be rank 5, but got rank ${o.rank}`)),s(5===n.rank,(()=>`Error in conv3dDerInput: filter must be rank 5, but got rank ${n.rank}`)),s(u===n.shape[3],(()=>`Error in conv3dDerInput: depth of input (${u}) must match input depth for filter ${n.shape[3]}.`)),s(h===n.shape[4],(()=>`Error in conv3dDerInput: depth of output (${h}) must match output depth for filter ${n.shape[4]}.`));const c={dy:o,filter:n},p={pad:r,strides:i,inputShape:a},d=Ee.runKernel("Conv3DBackpropInputV2",c,p);return l?on(d,[d.shape[1],d.shape[2],d.shape[3],d.shape[4]]):d}});const An=Be({cos_:function(t){const e={x:Me(t,"x","cos")};return Ee.runKernel("Cos",e)}});const Cn=Be({cosh_:function(t){const e={x:Me(t,"x","cosh")};return Ee.runKernel(J,e)}});const $n=Be({cumsum_:function(t,e=0,n=!1,s=!1){const i={x:Me(t,"x","cumsum")},r={axis:e,exclusive:n,reverse:s};return Ee.runKernel(Z,i,r)}});const Dn=Be({depthToSpace_:function(t,e,n="NHWC"){const i=Me(t,"x","depthToSpace"),r="NHWC"===n?i.shape[1]:i.shape[2],a="NHWC"===n?i.shape[2]:i.shape[3],o="NHWC"===n?i.shape[3]:i.shape[1];s(r*e>=0,(()=>`Negative dimension size caused by overflow when multiplying\n    ${r} and ${e}  for depthToSpace with input shape\n    ${i.shape}`)),s(a*e>=0,(()=>`Negative dimension size caused by overflow when multiplying\n    ${a} and ${e} for depthToSpace with input shape\n        ${i.shape}`)),s(o%(e*e)==0,(()=>`Dimension size must be evenly divisible by ${e*e} but is ${o} for depthToSpace with input shape ${i.shape}`));const l={x:i},u={blockSize:e,dataFormat:n};return Ee.runKernel("DepthToSpace",l,u)}});const Tn=Be({depthwiseConv2d_:function(t,e,n,i,r="NHWC",a=[1,1],l){const u=Me(t,"x","depthwiseConv2d"),h=Me(e,"filter","depthwiseConv2d");let c=u,p=!1;3===u.rank&&(p=!0,c=on(u,[1,u.shape[0],u.shape[1],u.shape[2]])),s(4===c.rank,(()=>`Error in depthwiseConv2d: input must be rank 4, but got rank ${c.rank}.`)),s(4===h.rank,(()=>`Error in depthwiseConv2d: filter must be rank 4, but got rank ${h.rank}.`)),s(c.shape[3]===h.shape[2],(()=>`Error in depthwiseConv2d: number of input channels (${c.shape[3]}) must match the inChannels dimension in filter ${h.shape[2]}.`)),null!=l&&s(o(i),(()=>`Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode ${l} but got pad ${i}.`));const d={x:c,filter:h},f={strides:n,pad:i,dataFormat:r,dilations:a,dimRoundingMode:l},g=Ee.runKernel(X,d,f);return p?on(g,[g.shape[1],g.shape[2],g.shape[3]]):g}});const En=Be({dilation2d_:function(t,e,n,i,r=[1,1],a="NHWC"){const o=Me(t,"x","dilation2d"),l=Me(e,"filter","dilation2d");s(3===o.rank||4===o.rank,(()=>`Error in dilation2d: input must be rank 3 or 4, but got rank ${o.rank}.`)),s(3===l.rank,(()=>`Error in dilation2d: filter must be rank 3, but got rank ${l.rank}.`)),s("NHWC"===a,(()=>`Error in dilation2d: Only NHWC is currently supported, but got dataFormat of ${a}`));let u=o,h=!1;3===o.rank&&(u=on(o,[1,o.shape[0],o.shape[1],o.shape[2]]),h=!0);const c={x:u,filter:l},p={strides:n,pad:i,dilations:r},d=Ee.runKernel(Y,c,p);return h?on(d,[d.shape[1],d.shape[2],d.shape[3]]):d}});const Fn=Be({floorDiv_:function(t,e){let n=Me(t,"a","floorDiv"),s=Me(e,"b","floorDiv");[n,s]=ze(n,s);const i={a:n,b:s};return Ee.runKernel(st,i)}});const Ln=Be({div_:function(t,e){let n=Me(t,"a","div"),s=Me(e,"b","div");if([n,s]=ze(n,s),"int32"===n.dtype&&"int32"===s.dtype)return Fn(n,s);const i={a:n,b:s};return Ee.runKernel(Q,i,{})}});function _n(t,e){const n=[];for(let s=0;s<e.length;s++){const i=t[t.length-s-1],r=e.length-s-1,a=e[r];(null==i||1===i&&a>1)&&n.unshift(r)}return n}function Rn(t,e){const n=[],s=Math.max(t.length,e.length);for(let i=0;i<s;i++){let s=t[t.length-i-1];null==s&&(s=1);let r=e[e.length-i-1];if(null==r&&(r=1),1===s)n.unshift(r);else if(1===r)n.unshift(s);else{if(s!==r){throw Error(`Operands could not be broadcast together with shapes ${t} and ${e}.`)}n.unshift(s)}}return n}const Mn=Be({equal_:function(t,e){let n=Me(t,"a","equal"),s=Me(e,"b","equal");[n,s]=ze(n,s),Rn(n.shape,s.shape);const i={a:n,b:s};return Ee.runKernel("Equal",i)}});const On=Be({where_:function(t,e,n){const s=Me(e,"a","where"),i=Me(n,"b","where"),r=Me(t,"condition","where","bool"),a=Rn(Rn(r.shape,s.shape),i.shape),o={condition:bn(r,a),t:bn(s,a),e:bn(i,a)};return Ee.runKernel(Ft,o)}});const Bn=Be({zerosLike_:function(t){const e={x:Me(t,"x","zerosLike")};return Ee.runKernel(Zt,e)}});const Wn=Be({divNoNan_:function(t,e){let n=Me(t,"a","div"),s=Me(e,"b","div");[n,s]=ze(n,s);const i=Ln(n,s),r=Bn(i),a=Mn(s,r);return On(a,r,i)}});const Pn=Be({dot_:function(t,e){const n=Me(t,"t1","dot"),i=Me(e,"t2","dot");s(!(1!==n.rank&&2!==n.rank||1!==i.rank&&2!==i.rank),(()=>`Error in dot: inputs must all be rank 1 or 2, but got ranks ${n.rank} and ${i.rank}.`));const r=1===n.rank?n.size:n.shape[1],a=1===i.rank?i.size:i.shape[0];if(s(r===a,(()=>`Error in dot: inner dimensions of inputs must match, but got ${r} and ${a}.`)),1===n.rank&&1===i.rank){const t=on(n,[1,-1]),e=on(i,[-1,1]),s=cn(t,e);return on(s,[])}if(1===n.rank&&2===i.rank){const t=on(n,[1,-1]),e=on(i,[i.shape[0],i.shape[1]]),s=cn(t,e);return on(s,[s.size])}if(2===n.rank&&1===i.rank){const t=on(i,[-1,1]),e=cn(n,t);return on(e,[e.size])}{const t=on(i,[i.shape[0],i.shape[1]]);return cn(n,t)}}});const Un=Be({elu_:function(t){const e={x:Me(t,"x","elu")};return Ee.runKernel("Elu",e)}});const Kn=Be({erf_:function(t){let e=Me(t,"x","erf");s("int32"===e.dtype||"float32"===e.dtype,(()=>"Input dtype must be `int32` or `float32`.")),"int32"===e.dtype&&(e=Qe(e,"float32"));const n={x:e};return Ee.runKernel("Erf",n)}});const Vn=Be({exp_:function(t){const e={x:Me(t,"x","exp")};return Ee.runKernel("Exp",e)}});const jn=Be({expandDims_:function(t,e=0){const n=Me(t,"x","expandDims","string_or_numeric");s(e<=n.rank,(()=>"Axis must be <= rank of the tensor"));const i={input:n},r={dim:e};return Ee.runKernel(tt,i,r)}});const qn=Be({expm1_:function(t){const e={x:Me(t,"x","expm1")};return Ee.runKernel(et,e)}});const Gn=Be({tile_:function(t,e){const n=Me(t,"x","tile","string_or_numeric");s(n.rank===e.length,(()=>`Error in transpose: rank of input ${n.rank} must match length of reps ${e}.`));const i={x:n},r={reps:e};return Ee.runKernel(qt,i,r)}});const Hn=Be({floor_:function(t){const e={x:Me(t,"x","floor")};return Ee.runKernel(nt,e)}});const Jn=Be({gather_:function(t,e,n=0,s=0){const i={x:Me(t,"x","gather"),indices:Me(e,"indices","gather","int32")},r={axis:n,batchDims:s};return Ee.runKernel(rt,i,r)}});const Zn=Be({greater_:function(t,e){let n=Me(t,"a","greater"),s=Me(e,"b","greater");[n,s]=ze(n,s),Rn(n.shape,s.shape);const i={a:n,b:s};return Ee.runKernel("Greater",i)}});const Xn=Be({greaterEqual_:function(t,e){let n=Me(t,"a","greaterEqual"),s=Me(e,"b","greaterEqual");[n,s]=ze(n,s),Rn(n.shape,s.shape);const i={a:n,b:s};return Ee.runKernel(at,i)}});const Yn=Be({imag_:function(t){const e={input:Me(t,"input","imag")};return Ee.runKernel("Imag",e)}});const Qn=Be({isFinite_:function(t){const e={x:Me(t,"x","isFinite")};return Ee.runKernel(lt,e)}});const ts=Be({isInf_:function(t){const e={x:Me(t,"x","isInf")};return Ee.runKernel(ut,e)}});const es=Be({isNaN_:function(t){const e={x:Me(t,"x","isNaN")};return Ee.runKernel(ht,e)}});const ns=Be({leakyRelu_:function(t,e=.2){const n={x:Me(t,"x","leakyRelu")},s={alpha:e};return Ee.runKernel(ct,n,s)}});const ss=Be({less_:function(t,e){let n=Me(t,"a","less"),s=Me(e,"b","less");[n,s]=ze(n,s),Rn(n.shape,s.shape);const i={a:n,b:s};return Ee.runKernel("Less",i)}});const is=Be({lessEqual_:function(t,e){let n=Me(t,"a","lessEqual"),s=Me(e,"b","lessEqual");[n,s]=ze(n,s),Rn(n.shape,s.shape);const i={a:n,b:s};return Ee.runKernel("LessEqual",i)}});const rs=Be({localResponseNormalization_:function(t,e=5,n=1,i=1,r=.5){const a=Me(t,"x","localResponseNormalization");s(4===a.rank||3===a.rank,(()=>`Error in localResponseNormalization: x must be rank 3 or 4 but got\n               rank ${a.rank}.`)),s(o(e),(()=>`Error in localResponseNormalization: depthRadius must be an integer but got depthRadius ${e}.`));let l=a,u=!1;3===a.rank&&(u=!0,l=on(a,[1,a.shape[0],a.shape[1],a.shape[2]]));const h={x:l},c={depthRadius:e,bias:n,alpha:i,beta:r},p=Ee.runKernel("LRN",h,c);return u?on(p,[p.shape[1],p.shape[2],p.shape[3]]):p}});const as=Be({log_:function(t){const e={x:Me(t,"x","log")};return Ee.runKernel("Log",e)}});const os=Be({log1p_:function(t){const e={x:Me(t,"x","log1p")};return Ee.runKernel(pt,e)}});function ls(t){return Ee.customGrad(t)}const us=Be({neg_:function(t){const e={x:Me(t,"x","neg")};return Ee.runKernel("Neg",e)}});const hs=Be({softplus_:function(t){const e={x:Me(t,"x","softplus")};return Ee.runKernel(Bt,e)}});const cs=Be({logSigmoid_:function(t){const e=Me(t,"x","logSigmoid");return ls((t=>({value:us(hs(us(t))),gradFunc:e=>pn(e,dn(us(t)))})))(e)}});const ps=Be({max_:function(t,e=null,n=!1){const s={x:Me(t,"x","max")},i={reductionIndices:e,keepDims:n};return Ee.runKernel("Max",s,i)}});const ds=Be({sub_:function(t,e){let n=Me(t,"a","sub"),s=Me(e,"b","sub");[n,s]=ze(n,s);const i={a:n,b:s};return Ee.runKernel("Sub",i)}});const fs=Be({sum_:function(t,e=null,n=!1){let s=Me(t,"x","sum");"bool"===s.dtype&&(s=Qe(s,"int32"));const i={x:s},r={axis:e,keepDims:n};return Ee.runKernel("Sum",i,r)}});const gs=Be({logSoftmax_:function(t,e=-1){const n=Me(t,"logits","logSoftmax");if(-1===e&&(e=n.rank-1),e!==n.rank-1)throw Error(`Log Softmax along a non-last dimension is not yet supported. Logits was rank ${n.rank} and axis was ${e}`);return ls(((t,n)=>{const s=ps(t,e,!0),i=ds(t,s),r=ds(Qe(i,"float32"),as(fs(Vn(i),e,!0)));n([r]);return{value:r,gradFunc:(t,n)=>{const[s]=n,i=Vn(s);return ds(t,pn(fs(t,e,!0),i))}}}))(n)}});function ms(t,e){return function(t,e,n){const s=t.length+e.length,i=[];let r=0,a=0;for(let o=0;o<s;o++)-1===n.indexOf(o)?i.push(t[r++]):i.push(e[a++]);return i}(t,e.map((t=>1)),e)}function ys(t){return t.map(((t,e)=>[e,t])).sort(((t,e)=>t[1]-e[1])).map((t=>t[0]))}const bs=Be({logSumExp_:function(t,e=null,n=!1){const s=Me(t,"x","logSumExp"),i=u(e,s.shape),r=ps(s,i,!0),a=ds(s,r),o=Vn(a),l=fs(o,i),h=as(l),c=Ke(on(r,h.shape),h);if(n){const t=ms(c.shape,i);return on(c,t)}return c}});const ws=Be({logicalAnd_:function(t,e){const n=Me(t,"a","logicalAnd","bool"),s=Me(e,"b","logicalAnd","bool");Rn(n.shape,s.shape);const i={a:n,b:s};return Ee.runKernel("LogicalAnd",i)}});const ks=Be({logicalNot_:function(t){const e={x:Me(t,"x","logicalNot","bool")};return Ee.runKernel("LogicalNot",e)}});const xs=Be({logicalOr_:function(t,e){const n=Me(t,"a","logicalOr","bool"),s=Me(e,"b","logicalOr","bool");Rn(n.shape,s.shape);const i={a:n,b:s};return Ee.runKernel("LogicalOr",i)}});const vs=Be({logicalXor_:function(t,e){const n=Me(t,"a","logicalXor","bool"),s=Me(e,"b","logicalXor","bool");return Rn(n.shape,s.shape),ws(xs(t,e),ks(ws(t,e)))}});const Ss=Be({maxPool_:function(t,e,n,i,r){const a=Me(t,"x","maxPool");let l=a,u=!1;3===a.rank&&(u=!0,l=on(a,[1,a.shape[0],a.shape[1],a.shape[2]])),s(4===l.rank,(()=>`Error in maxPool: input must be rank 4 but got rank ${l.rank}.`)),s(an(n,1),(()=>`Error in maxPool: Either strides or dilations must be 1. Got strides ${n} and dilations '1'`)),null!=r&&s(o(i),(()=>`Error in maxPool: pad must be an integer when using, dimRoundingMode ${r} but got pad ${i}.`));const h={x:l},c={filterSize:e,strides:n,pad:i,dimRoundingMode:r},p=Ee.runKernel(ft,h,c);return u?on(p,[p.shape[1],p.shape[2],p.shape[3]]):p}});const Is=Be({maximum_:function(t,e){let n=Me(t,"a","maximum"),s=Me(e,"b","maximum");[n,s]=ze(n,s),"bool"===n.dtype&&(n=Qe(n,"int32"),s=Qe(s,"int32")),Rn(n.shape,s.shape);const i={a:n,b:s};return Ee.runKernel(dt,i)}});const Ns=Be({mean_:function(t,e=null,n=!1){const s={x:Me(t,"x","mean")},i={axis:e,keepDims:n};return Ee.runKernel(gt,s,i)}});function zs(t,e="float32"){if("complex64"===e){const e=zs(t,"float32"),n=zs(t,"float32");return xn(e,n)}const n=w(r(t),e);return Ee.makeTensor(n,t,e)}function As(t,e="float32"){if("complex64"===e){const e=As(t,"float32"),n=zs(t,"float32");return xn(e,n)}const n=b(r(t),e);return Ee.makeTensor(n,t,e)}const Cs=Be({min_:function(t,e=null,n=!1){const s={x:Me(t,"x","min")},i={axis:e,keepDims:n};return Ee.runKernel("Min",s,i)}});const $s=Be({minimum_:function(t,e){let n=Me(t,"a","minimum"),s=Me(e,"b","minimum");[n,s]=ze(n,s),"bool"===n.dtype&&(n=Qe(n,"int32"),s=Qe(s,"int32")),Rn(n.shape,s.shape);const i={a:n,b:s};return Ee.runKernel(mt,i)}});const Ds=Be({mirrorPad_:function(t,e,n){s("reflect"===n||"symmetric"===n,(()=>`Invalid mode. Mode must be either reflect or symmetric. Got ${n}.`));const i=Me(t,"x","mirrorPad");if(0===i.rank)throw new Error("mirrorPad(scalar) is not defined. Pass non-scalar to mirrorPad");s(e.length===i.rank,(()=>`Padding doesn't match input. Must be ${i.rank}. Got ${e.length}.`));const r="reflect"===n?1:0;for(let t=0;t<i.rank;t++)s(2===e[t].length,(()=>"Invalid number of paddings. Must be length of 2 each.")),s(e[t][0]>=0&&e[t][0]<=i.shape[t]-r&&e[t][1]>=0&&e[t][1]<=i.shape[t]-r,(()=>`Padding in dimension ${t} cannot be greater than or equal to ${i.shape[t]-r} or less than 0 for input of shape ${i.shape}`));const a={paddings:e,mode:n},o={x:i};return Ee.runKernel(yt,o,a)}});const Ts=Be({mod_:function(t,e){let n=Me(t,"a","mod"),s=Me(e,"b","mod");[n,s]=ze(n,s);const i={a:n,b:s};return Ee.runKernel("Mod",i)}});const Es=Be({square_:function(t){const e=Me(t,"x","square");return Ee.runKernel("Square",{x:e},{})}});const Fs=Be({notEqual_:function(t,e){let n=Me(t,"a","notEqual"),s=Me(e,"b","notEqual");[n,s]=ze(n,s),Rn(n.shape,s.shape);const i={a:n,b:s};return Ee.runKernel("NotEqual",i)}});const Ls=Be({oneHot_:function(t,e,n=1,s=0){if(e<2)throw new Error(`Error in oneHot: depth must be >=2, but it is ${e}`);const i={indices:Me(t,"indices","oneHot","int32")},r={depth:e,onValue:n,offValue:s};return Ee.runKernel(kt,i,r)}});const _s=Be({onesLike_:function(t){const e={x:Me(t,"x","onesLike")};return Ee.runKernel(wt,e)}});const Rs=Be({pad_:function(t,e,n=0){const s=Me(t,"x","pad");if(0===s.rank)throw new Error("pad(scalar) is not defined. Pass non-scalar to pad");const i={paddings:e,constantValue:n},r={x:s};return Ee.runKernel(vt,r,i)}});const Ms=Be({spaceToBatchND_:function(t,e,n){const i=Me(t,"x","spaceToBatchND");s(i.rank>=1+e.length,(()=>`input rank ${i.rank} should be > than [blockShape] ${e.length}`)),s(n.length===e.length,(()=>`paddings.shape[0] ${n.length} must be equal to [blockShape] ${e.length}`)),s(i.shape.reduce(((t,s,i)=>i>0&&i<=e.length?t&&(s+n[i-1][0]+n[i-1][1])%e[i-1]==0:t),!0),(()=>`input spatial dimensions ${i.shape.slice(1)} with paddings ${n.toString()} must be divisible by blockShapes ${e.toString()}`));const r={x:i},a={blockShape:e,paddings:n};return Ee.runKernel(Pt,r,a)}});const Os=Be({pool_:function(t,e,n,i,r,a){null==r&&(r=[1,1]),null==a&&(a=1),0===i&&(i="valid");const o=Me(t,"x","maxPool");let l=o,u=!1;3===o.rank&&(u=!0,l=on(o,[1,o.shape[0],o.shape[1],o.shape[2]])),s(an(a,r),(()=>`Error in pool: Either strides or dilations must be 1. Got strides ${a} and dilations '${r}'`));const h=tn(l.shape,e,a,r,i),c=[h.dilationHeight,h.dilationWidth];let p;p="same"===i?function(t,e){const n=t.map(((t,n)=>t+(t-1)*(e[n]-1))).map((t=>t-1)),s=n.map((t=>Math.floor(t/2))),i=n.map(((t,e)=>t-s[e]));return n.map(((t,e)=>[s[e],i[e]]))}([h.filterHeight,h.filterWidth],c):[[0,0],[0,0]];const d=1===c[0]&&1===c[1],[f,g]=function(t,e,n){const s=n.map((t=>t[0])),i=n.map((t=>t[1])),r=t.concat(s,i),a=e.map(((t,e)=>(t-r[e]%t)%t)),o=i.map(((t,e)=>t+a[e])),l=e.map(((t,e)=>[s[e],o[e]])),u=e.map(((t,e)=>[0,a[e]]));return[l,u]}([h.inHeight,h.inWidth],c,p),m=d?i:"valid",y=d?l:Ms(l,c,f),b=("avg"===n?()=>ln(y,e,a,m):()=>Ss(y,e,a,m))(),w=d?b:mn(b,c,g);return u?on(w,[w.shape[1],w.shape[2],w.shape[3]]):w}});const Bs=Be({pow_:function(t,e){let n=Me(t,"base","pow"),s=Me(e,"exp","pow");[n,s]=ze(n,s);const i={a:n,b:s};return Ee.runKernel("Pow",i)}});const Ws=Be({prelu_:function(t,e){const n={x:Me(t,"x","prelu"),alpha:Me(e,"alpha","prelu")};return Ee.runKernel(St,n)}});const Ps=Be({prod_:function(t,e=null,n=!1){let s=Me(t,"x","prod");"bool"===s.dtype&&(s=Qe(s,"int32"));const i={x:s},r={axis:e,keepDims:n};return Ee.runKernel("Prod",i,r)}});const Us=Be({real_:function(t){const e={input:Me(t,"input","real")};return Ee.runKernel("Real",e)}});const Ks=Be({reciprocal_:function(t){const e={x:Me(t,"x","reciprocal")};return Ee.runKernel(It,e)}});const Vs=Be({relu_:function(t){const e={x:Me(t,"x","relu")};return Ee.runKernel(Nt,e)}});const js=Be({relu6_:function(t){const e={x:Me(t,"x","relu6")};return Ee.runKernel($t,e)}});const qs=Be({reverse_:function(t,e){const n={x:Me(t,"x","reverse")},s={dims:e};return Ee.runKernel(Dt,n,s)}});const Gs=Be({round_:function(t){const e={x:Me(t,"x","round")};return Ee.runKernel(Tt,e)}});const Hs=Be({rsqrt_:function(t){const e={x:Me(t,"x","rsqrt")};return Ee.runKernel(Et,e)}});function Js(t,e,n,a){if(null==a&&(a=d(t)),"complex64"===a)throw new Error("Cannot construct a complex64 tensor directly. Please use tf.complex(real, imag).");if(!h(t)&&!Array.isArray(t)&&"number"!=typeof t&&"boolean"!=typeof t&&"string"!=typeof t)throw new Error("values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings, or a TypedArray");if(null!=e){!function(t){t.forEach((e=>{s(Number.isInteger(e)&&e>=0,(()=>`Tensor must have a shape comprised of positive integers but got shape [${t}].`))}))}(e);const t=r(e),i=r(n);s(t===i,(()=>`Based on the provided shape, [${e}], the tensor should have ${t} values but has ${i}`));for(let t=0;t<n.length;++t){const i=n[t],a=t!==n.length-1||i!==r(e.slice(t));s(n[t]===e[t]||!a,(()=>`Error creating a new Tensor. Inferred shape (${n}) does not match the provided shape (${e}). `))}}return h(t)||Array.isArray(t)||(t=[t]),e=e||n,t="string"!==a?ie(t,a):i(t,[],!0),Ee.makeTensor(t,e,a)}function Zs(t,e){if((h(t)&&"string"!==e||Array.isArray(t))&&"complex64"!==e)throw new Error("Error creating a new Scalar: value must be a primitive (number|boolean|string)");if("string"===e&&h(t)&&!(t instanceof Uint8Array))throw new Error("When making a scalar from encoded string, the value must be `Uint8Array`.");return Js(t,[],[],e)}const Xs=Be({selu_:function(t){const e={x:Me(t,"x","selu")};return Ee.runKernel(Lt,e)}});const Ys=Be({separableConv2d_:function(t,e,n,i,r,a=[1,1],o="NHWC"){const l=Me(t,"x","separableConv2d"),u=Me(e,"depthwiseFilter","separableConv2d"),h=Me(n,"pointwiseFilter","separableConv2d");let c=l,p=!1;if(3===l.rank&&(p=!0,c=on(l,[1,l.shape[0],l.shape[1],l.shape[2]])),"NCHW"===o)throw new Error("separableConv2d currently does not support dataFormat NCHW; only NHWC is supported");s(4===c.rank,(()=>`Error in separableConv2d: input must be rank 4, but got rank ${c.rank}.`)),s(4===u.rank,(()=>`Error in separableConv2d: depthwise filter must be rank 4, but got rank ${u.rank}.`)),s(4===h.rank,(()=>`Error in separableConv2d: pointwise filter must be rank 4, but got rank ${u.rank}.`)),s(1===h.shape[0],(()=>`Error in separableConv2d: the first dimension of pointwise filter  must be 1, but got ${h.shape[0]}.`)),s(1===h.shape[1],(()=>`Error in separableConv2d: the second dimension of pointwise filter must be 1, but got ${h.shape[1]}.`));const d=u.shape[2],f=u.shape[3];s(h.shape[2]===d*f,(()=>`Error in separableConv2d: the third dimension of pointwise filter must be ${d*f}, but got ${h.shape[2]}.`));const g=Tn(c,u,i,r,o,a),m=vn(g,h,1,"valid",o);return p?on(m,[m.shape[1],m.shape[2],m.shape[3]]):m}});const Qs=Be({sign_:function(t){const e={x:Me(t,"x","sign")};return Ee.runKernel(Mt,e)}});const ti=Be({sin_:function(t){const e={x:Me(t,"x","sin")};return Ee.runKernel("Sin",e)}});const ei=Be({sinh_:function(t){const e={x:Me(t,"x","sinh")};return Ee.runKernel(Rt,e)}});const ni=Be({softmax_:function(t,e=-1){const n=Me(t,"logits","softmax","float32");if(-1===e&&(e=n.rank-1),e!==n.rank-1)throw Error(`Softmax along a non-last dimension is not yet supported. Logits was rank ${n.rank} and dim was ${e}`);const s={logits:n},i={dim:e};return Ee.runKernel(Kt,s,i)}});const si=Be({fft_:function(t){s("complex64"===t.dtype,(()=>`The dtype for tf.spectral.fft() must be complex64 but got ${t.dtype}.`));const e={input:t};return Ee.runKernel("FFT",e)}});const ii=Be({ifft_:function(t){s("complex64"===t.dtype,(()=>`The dtype for tf.spectral.ifft() must be complex64 but got ${t.dtype}.`));const e={input:t};return Ee.runKernel("IFFT",e)}});const ri=Be({irfft_:function(t){const e=t.shape[t.shape.length-1],n=t.size/e;let s;if(e<=2){const i=on(t,[n,e]);s=ii(i)}else{const i=[n,2*(e-1)],r=on(Us(t),[n,e]),a=on(Yn(t),[n,e]),o=qs(fn(r,[0,1],[n,e-2]),1),l=pn(qs(fn(a,[0,1],[n,e-2]),1),Zs(-1)),u=hn([r,o],1),h=hn([a,l],1),c=on(xn(u,h),[i[0],i[1]]);s=ii(c)}if(s=Us(s),3===t.rank&&0!==t.shape[0]){const e=s,n=t.shape[0];s=on(s,[n,s.shape[0]/n,s.shape[1]]),e.dispose()}return s}});const ai=Be({split_:function(t,e,n=0){const s={x:Me(t,"x","split")},i={numOrSizeSplits:e,axis:n};return Ee.runKernel(Ut,s,i)}});const oi=Be({rfft_:function(t,e){s("float32"===t.dtype,(()=>`The dtype for rfft() must be real value but got ${t.dtype}`));let n=t.shape[t.shape.length-1];const i=t.size/n;let r;if(null!=e&&e<n){const s=t.shape.map((t=>0)),i=t.shape.map((t=>t));i[t.shape.length-1]=e,r=fn(t,s,i),n=e}else if(null!=e&&e>n){const s=t.shape.map((t=>t));s[t.shape.length-1]=e-n,r=hn([t,zs(s)],t.shape.length-1),n=e}else r=t;const a=Bn(r),o=on(xn(r,a),[i,n]),l=si(o),u=Math.floor(n/2)+1,h=Us(l),c=Yn(l),p=ai(h,[u,n-u],h.shape.length-1),d=ai(c,[u,n-u],c.shape.length-1),f=r.shape.slice();return f[r.shape.length-1]=u,on(xn(p[0],d[0]),f)}});const li=Be({sqrt_:function(t){const e={x:Me(t,"x","sqrt")};return Ee.runKernel(Wt,e)}});const ui=Be({squaredDifference_:function(t,e){let n=Me(t,"a","squaredDifference"),s=Me(e,"b","squaredDifference");[n,s]=ze(n,s),Rn(n.shape,s.shape);const i={a:n,b:s};return Ee.runKernel(Vt,i,{})}});const hi=Be({squeeze_:function(t,e){const n=Me(t,"x","squeeze");return on(n,function(t,e){const n=[],s=[],i=null!=e&&Array.isArray(e)&&0===e.length,r=null==e||i?null:u(e,t).sort();let a=0;for(let e=0;e<t.length;++e){if(null!=r){if(r[a]===e&&1!==t[e])throw new Error(`Can't squeeze axis ${e} since its dim '${t[e]}' is not 1`);(null==r[a]||r[a]>e)&&1===t[e]&&(n.push(t[e]),s.push(e)),r[a]<=e&&a++}1!==t[e]&&(n.push(t[e]),s.push(e))}return{newShape:n,keptDims:s}}(n.shape,e).newShape)}});const ci=Be({stack_:function(t,e=0){const n=Oe(t,"tensors","stack","string_or_numeric");s(n.length>=1,(()=>"Pass at least one tensor to tf.stack")),n.length>0&&s(e<=n[0].rank,(()=>"Axis must be <= rank of the tensor"));const i=n,r={axis:e};return Ee.runKernel(xt,i,r)}});const pi=Be({step_:function(t,e=0){const n={x:Me(t,"x","step")},s={alpha:e};return Ee.runKernel(Xt,n,s)}});const di=Be({stridedSlice_:function(t,e,n,s,i=0,r=0,a=0,o=0,l=0){const u={x:Me(t,"x","stridedSlice")},h={begin:e,end:n,strides:s,beginMask:i,endMask:r,ellipsisMask:a,newAxisMask:o,shrinkAxisMask:l};return Ee.runKernel("StridedSlice",u,h)}});const fi=Be({tan_:function(t){const e={x:Me(t,"x","tan")};return Ee.runKernel("Tan",e)}});const gi=Be({topk_:function(t,e=1,n=!0){const s=Me(t,"x","topk");if(0===s.rank)throw new Error("topk() expects the input to be of rank 1 or higher");const i=s.shape[s.shape.length-1];if(e>i)throw new Error(`'k' passed to topk() must be <= the last dimension (${i}) but got ${e}`);const r={x:s},a={k:e,sorted:n},[o,l]=Ee.runKernel("TopK",r,a);return{values:o,indices:l}}});const mi=Be({unique_:function(t,e=0){const n=Me(t,"x","unique","string_or_numeric");s(n.rank>0,(()=>"The input tensor must be at least 1D"));const i={x:n},r={axis:e},[a,o]=Ee.runKernel("Unique",i,r);return{values:a,indices:o}}});const yi=Be({unsortedSegmentSum_:function(t,e,n){const i=Me(t,"x","unsortedSegmentSum"),r=Me(e,"segmentIds","unsortedSegmentSum","int32");s(o(n),(()=>"numSegments must be of dtype int"));const a={x:i,segmentIds:r},l={numSegments:n};return Ee.runKernel(Jt,a,l)}});const bi=Be({unstack_:function(t,e=0){const n=Me(t,"x","unstack","string_or_numeric");s(e>=-n.shape.length&&e<n.shape.length,(()=>`Axis = ${e} is not in [-${n.shape.length}, ${n.shape.length})`));const i={value:n},r={axis:e};return Ee.runKernel(Ht,i,r)}});const wi=Be({transpose_:function(t,e){const n=Me(t,"x","transpose");if(null==e&&(e=n.shape.map(((t,e)=>e)).reverse()),s(n.rank===e.length,(()=>`Error in transpose: rank of input ${n.rank} must match length of perm ${e}.`)),e.forEach((t=>{s(t>=0&&t<n.rank,(()=>"All entries in 'perm' must be between 0 and "+(n.rank-1)+` but got ${e}`))})),n.rank<=1)return n.clone();const i={x:n},r={perm:e};return Ee.runKernel(Gt,i,r)}});function ki(t,e,n=null){if(0===t.rank)return We(t);if(1!==t.rank&&null===n)return ki(on(t,[-1]),e,n);if(1===t.rank||"number"==typeof n||Array.isArray(n)&&1===n.length){if(1===e)return fs(We(t),n);if(e===1/0)return ps(We(t),n);if(e===-1/0)return Cs(We(t),n);if("euclidean"===e||2===e)return li(fs(Bs(We(t),Zs(2,"int32")),n));throw new Error(`Error in norm: invalid ord value: ${e}`)}if(Array.isArray(n)&&2===n.length){if(1===e)return ps(fs(We(t),n[0]),n[1]-1);if(e===1/0)return ps(fs(We(t),n[1]),n[0]);if(e===-1/0)return Cs(fs(We(t),n[1]),n[0]);if("fro"===e||"euclidean"===e)return li(fs(Es(t),n));throw new Error(`Error in norm: invalid ord value: ${e}`)}throw new Error(`Error in norm: invalid axis: ${n}`)}const xi=Be({norm_:function(t,e="euclidean",n=null,s=!1){const i=ki(t=Me(t,"x","norm"),e,n);let r=i.shape;if(s){const e=u(n,t.shape);r=ms(i.shape,e)}return on(i,r)}});const vi=Be({conv2DBackpropFilter_:function(t,e,n,i,r,a="NHWC",l){let u=t;3===t.rank&&(u=on(t,[1,t.shape[0],t.shape[1],t.shape[2]]));let h=e;3===h.rank&&(h=on(e,[1,e.shape[0],e.shape[1],e.shape[2]])),s(4===u.rank,(()=>`Error in conv2dDerFilter: input must be rank 4, but got shape ${u.shape}.`)),s(4===h.rank,(()=>`Error in conv2dDerFilter: dy must be rank 4, but got shape ${h.shape}.`)),s(4===n.length,(()=>`Error in conv2dDerFilter: filterShape must be length 4, but got ${n}.`));const c="NHWC"===a?u.shape[3]:u.shape[1],p="NHWC"===a?h.shape[3]:h.shape[1];s(c===n[2],(()=>`Error in conv2dDerFilter: depth of input ${c}) must match input depth in filter (${n[2]}.`)),s(p===n[3],(()=>`Error in conv2dDerFilter: depth of dy (${p}) must match output depth for filter (${n[3]}).`)),null!=l&&s(o(r),(()=>`Error in conv2dDerFilter: pad must be an integer when using, dimRoundingMode ${l} but got pad ${r}.`));const d={x:u,dy:h},f={strides:i,pad:r,dataFormat:a,dimRoundingMode:l,filterShape:n};return Ee.runKernel("Conv2DBackpropFilter",d,f)}});const Si=Be({depthwiseConv2dNativeBackpropFilter_:function(t,e,n,s,i,r=[1,1],a){let o=t;3===t.rank&&(o=on(t,[1,t.shape[0],t.shape[1],t.shape[2]]));let l=e;3===l.rank&&(l=on(e,[1,e.shape[0],e.shape[1],e.shape[2]]));const u={x:o,dy:l},h={strides:s,pad:i,dimRoundingMode:a,dilations:r,filterShape:n};return Ee.runKernel("DepthwiseConv2dNativeBackpropFilter",u,h)}});const Ii=Be({depthwiseConv2dNativeBackpropInput_:function(t,e,n,s,i,r=[1,1],a){let o=e,l=!1;3===e.rank&&(l=!0,o=on(e,[1,e.shape[0],e.shape[1],e.shape[2]]));const u={dy:o,filter:n},h={strides:s,pad:i,dimRoundingMode:a,dilations:r,inputShape:t},c=Ee.runKernel("DepthwiseConv2dNativeBackpropInput",u,h);return l?on(c,[c.shape[1],c.shape[2],c.shape[3]]):c}});const Ni=Be({resizeBilinear_:function(t,e,n=!1,i=!1){const r=Me(t,"images","resizeBilinear");s(3===r.rank||4===r.rank,(()=>`Error in resizeBilinear: x must be rank 3 or 4, but got rank ${r.rank}.`)),s(2===e.length,(()=>`Error in resizeBilinear: new shape must 2D, but got shape ${e}.`)),s(!1===i||!1===n,(()=>"Error in resizeBilinear: If halfPixelCenters is true, alignCorners must be false."));let a=r,o=!1;3===r.rank&&(o=!0,a=on(r,[1,r.shape[0],r.shape[1],r.shape[2]]));const l={images:a},u={alignCorners:n,halfPixelCenters:i,size:e},h=Ee.runKernel(Ct,l,u);return o?on(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});const zi=Be({resizeNearestNeighbor_:function(t,e,n=!1,i=!1){const r=Me(t,"images","resizeNearestNeighbor");s(3===r.rank||4===r.rank,(()=>`Error in resizeNearestNeighbor: x must be rank 3 or 4, but got rank ${r.rank}.`)),s(2===e.length,(()=>`Error in resizeNearestNeighbor: new shape must 2D, but got shape ${e}.`)),s("float32"===r.dtype||"int32"===r.dtype,(()=>"`images` must have `int32` or `float32` as dtype")),s(!1===i||!1===n,(()=>"Error in resizeNearestNeighbor: If halfPixelCenters is true, alignCorners must be false."));let a=r,o=!1;3===r.rank&&(o=!0,a=on(r,[1,r.shape[0],r.shape[1],r.shape[2]]));const l={images:a},u={alignCorners:n,halfPixelCenters:i,size:e},h=Ee.runKernel(At,l,u);return o?on(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});be().prototype.abs=function(){return this.throwIfDisposed(),We(this)},be().prototype.acos=function(){return this.throwIfDisposed(),Pe(this)},be().prototype.acosh=function(){return this.throwIfDisposed(),Ue(this)},be().prototype.add=function(t){return this.throwIfDisposed(),Ke(this,t)},be().prototype.all=function(t,e){return this.throwIfDisposed(),Ve(this,t,e)},be().prototype.any=function(t,e){return this.throwIfDisposed(),je(this,t,e)},be().prototype.argMax=function(t){return this.throwIfDisposed(),qe(this,t)},be().prototype.argMin=function(t){return this.throwIfDisposed(),Ge(this,t)},be().prototype.asScalar=function(){return this.throwIfDisposed(),s(1===this.size,(()=>"The array must have only 1 element.")),on(this,[])},be().prototype.asType=function(t){return this.throwIfDisposed(),Qe(this,t)},be().prototype.as1D=function(){return this.throwIfDisposed(),on(this,[this.size])},be().prototype.as2D=function(t,e){return this.throwIfDisposed(),on(this,[t,e])},be().prototype.as3D=function(t,e,n){return this.throwIfDisposed(),on(this,[t,e,n])},be().prototype.as4D=function(t,e,n,s){return this.throwIfDisposed(),on(this,[t,e,n,s])},be().prototype.as5D=function(t,e,n,s,i){return this.throwIfDisposed(),on(this,[t,e,n,s,i])},be().prototype.asin=function(){return this.throwIfDisposed(),He(this)},be().prototype.asinh=function(){return this.throwIfDisposed(),Je(this)},be().prototype.atan=function(){return this.throwIfDisposed(),Ze(this)},be().prototype.atan2=function(t){return this.throwIfDisposed(),Xe(this,t)},be().prototype.atanh=function(){return this.throwIfDisposed(),Ye(this)},be().prototype.avgPool=function(t,e,n,s){return this.throwIfDisposed(),ln(this,t,e,n,s)},be().prototype.batchToSpaceND=function(t,e){return this.throwIfDisposed(),mn(this,t,e)},be().prototype.batchNorm=function(t,e,n,s,i){return this.throwIfDisposed(),yn(this,t,e,n,s,i)},be().prototype.broadcastTo=function(t){return this.throwIfDisposed(),bn(this,t)},be().prototype.cast=function(t){return this.throwIfDisposed(),Qe(this,t)},be().prototype.ceil=function(){return this.throwIfDisposed(),wn(this)},be().prototype.clipByValue=function(t,e){return this.throwIfDisposed(),kn(this,t,e)},be().prototype.concat=function(t,e){return this.throwIfDisposed(),t instanceof ye&&(t=[t]),hn([this,...t],e)},be().prototype.conv1d=function(t,e,n,s,i,r){return this.throwIfDisposed(),Sn(this,t,e,n,s,i,r)},be().prototype.conv2dTranspose=function(t,e,n,s,i){return this.throwIfDisposed(),Nn(this,t,e,n,s,i)},be().prototype.conv2d=function(t,e,n,s,i,r){return this.throwIfDisposed(),vn(this,t,e,n,s,i,r)},be().prototype.cos=function(){return this.throwIfDisposed(),An(this)},be().prototype.cosh=function(){return this.throwIfDisposed(),Cn(this)},be().prototype.cumsum=function(t,e,n){return this.throwIfDisposed(),$n(this,t,e,n)},be().prototype.depthToSpace=function(t,e){return this.throwIfDisposed(),Dn(this,t,e)},be().prototype.depthwiseConv2d=function(t,e,n,s,i,r){return this.throwIfDisposed(),Tn(this,t,e,n,s,i,r)},be().prototype.dilation2d=function(t,e,n,s,i){return this.throwIfDisposed(),En(this,t,e,n,s,i)},be().prototype.divNoNan=function(t){return this.throwIfDisposed(),Wn(this,t)},be().prototype.div=function(t){return this.throwIfDisposed(),Ln(this,t)},be().prototype.dot=function(t){return this.throwIfDisposed(),Pn(this,t)},be().prototype.elu=function(){return this.throwIfDisposed(),Un(this)},be().prototype.equal=function(t){return this.throwIfDisposed(),Mn(this,t)},be().prototype.erf=function(){return this.throwIfDisposed(),Kn(this)},be().prototype.exp=function(){return this.throwIfDisposed(),Vn(this)},be().prototype.expandDims=function(t){return this.throwIfDisposed(),jn(this,t)},be().prototype.expm1=function(){return this.throwIfDisposed(),qn(this)},be().prototype.fft=function(){return this.throwIfDisposed(),si(this)},be().prototype.flatten=function(){return this.throwIfDisposed(),on(this,[this.size])},be().prototype.floor=function(){return this.throwIfDisposed(),Hn(this)},be().prototype.floorDiv=function(t){return this.throwIfDisposed(),Fn(this,t)},be().prototype.gather=function(t,e){return this.throwIfDisposed(),Jn(this,t,e)},be().prototype.greaterEqual=function(t){return this.throwIfDisposed(),Xn(this,t)},be().prototype.greater=function(t){return this.throwIfDisposed(),Zn(this,t)},be().prototype.ifft=function(){return this.throwIfDisposed(),ii(this)},be().prototype.irfft=function(){return this.throwIfDisposed(),ri(this)},be().prototype.isFinite=function(){return this.throwIfDisposed(),Qn(this)},be().prototype.isInf=function(){return this.throwIfDisposed(),ts(this)},be().prototype.isNaN=function(){return this.throwIfDisposed(),es(this)},be().prototype.leakyRelu=function(t){return this.throwIfDisposed(),ns(this,t)},be().prototype.lessEqual=function(t){return this.throwIfDisposed(),is(this,t)},be().prototype.less=function(t){return this.throwIfDisposed(),ss(this,t)},be().prototype.localResponseNormalization=function(t,e,n,s){return this.throwIfDisposed(),rs(this,t,e,n,s)},be().prototype.logSigmoid=function(){return this.throwIfDisposed(),cs(this)},be().prototype.logSoftmax=function(t){return this.throwIfDisposed(),gs(this,t)},be().prototype.logSumExp=function(t,e){return this.throwIfDisposed(),bs(this,t,e)},be().prototype.log=function(){return this.throwIfDisposed(),as(this)},be().prototype.log1p=function(){return this.throwIfDisposed(),os(this)},be().prototype.logicalAnd=function(t){return this.throwIfDisposed(),ws(this,t)},be().prototype.logicalNot=function(){return this.throwIfDisposed(),ks(this)},be().prototype.logicalOr=function(t){return this.throwIfDisposed(),xs(this,t)},be().prototype.logicalXor=function(t){return this.throwIfDisposed(),vs(this,t)},be().prototype.matMul=function(t,e,n){return this.throwIfDisposed(),cn(this,t,e,n)},be().prototype.maxPool=function(t,e,n,s){return this.throwIfDisposed(),Ss(this,t,e,n,s)},be().prototype.max=function(t,e){return this.throwIfDisposed(),ps(this,t,e)},be().prototype.maximum=function(t){return this.throwIfDisposed(),Is(this,t)},be().prototype.mean=function(t,e){return this.throwIfDisposed(),Ns(this,t,e)},be().prototype.min=function(t,e){return this.throwIfDisposed(),Cs(this,t,e)},be().prototype.minimum=function(t){return this.throwIfDisposed(),$s(this,t)},be().prototype.mirrorPad=function(t,e){return this.throwIfDisposed(),Ds(this,t,e)},be().prototype.mod=function(t){return this.throwIfDisposed(),Ts(this,t)},be().prototype.mul=function(t){return this.throwIfDisposed(),pn(this,t)},be().prototype.neg=function(){return this.throwIfDisposed(),us(this)},be().prototype.norm=function(t,e,n){return this.throwIfDisposed(),xi(this,t,e,n)},be().prototype.notEqual=function(t){return this.throwIfDisposed(),Fs(this,t)},be().prototype.oneHot=function(t,e=1,n=0){return this.throwIfDisposed(),Ls(this,t,e,n)},be().prototype.onesLike=function(){return this.throwIfDisposed(),_s(this)},be().prototype.pad=function(t,e){return this.throwIfDisposed(),Rs(this,t,e)},be().prototype.pool=function(t,e,n,s,i){return this.throwIfDisposed(),Os(this,t,e,n,s,i)},be().prototype.pow=function(t){return this.throwIfDisposed(),Bs(this,t)},be().prototype.prelu=function(t){return this.throwIfDisposed(),Ws(this,t)},be().prototype.prod=function(t,e){return this.throwIfDisposed(),Ps(this,t,e)},be().prototype.reciprocal=function(){return this.throwIfDisposed(),Ks(this)},be().prototype.relu=function(){return this.throwIfDisposed(),Vs(this)},be().prototype.relu6=function(){return this.throwIfDisposed(),js(this)},be().prototype.reshapeAs=function(t){return this.throwIfDisposed(),on(this,t.shape)},be().prototype.reshape=function(t){return this.throwIfDisposed(),on(this,t)},be().prototype.resizeBilinear=function(t,e,n){return this.throwIfDisposed(),Ni(this,t,e,n)},be().prototype.resizeNearestNeighbor=function(t,e,n){return this.throwIfDisposed(),zi(this,t,e,n)},be().prototype.reverse=function(t){return this.throwIfDisposed(),qs(this,t)},be().prototype.rfft=function(){return this.throwIfDisposed(),oi(this)},be().prototype.round=function(){return this.throwIfDisposed(),Gs(this)},be().prototype.rsqrt=function(){return this.throwIfDisposed(),Hs(this)},be().prototype.selu=function(){return this.throwIfDisposed(),Xs(this)},be().prototype.separableConv2d=function(t,e,n,s,i,r){return this.throwIfDisposed(),Ys(this,t,e,n,s,i,r)},be().prototype.sigmoid=function(){return this.throwIfDisposed(),dn(this)},be().prototype.sign=function(){return this.throwIfDisposed(),Qs(this)},be().prototype.sin=function(){return this.throwIfDisposed(),ti(this)},be().prototype.sinh=function(){return this.throwIfDisposed(),ei(this)},be().prototype.slice=function(t,e){return this.throwIfDisposed(),fn(this,t,e)},be().prototype.softmax=function(t){return this.throwIfDisposed(),ni(this,t)},be().prototype.softplus=function(){return this.throwIfDisposed(),hs(this)},be().prototype.spaceToBatchND=function(t,e){return this.throwIfDisposed(),Ms(this,t,e)},be().prototype.split=function(t,e){return this.throwIfDisposed(),ai(this,t,e)},be().prototype.sqrt=function(){return this.throwIfDisposed(),li(this)},be().prototype.square=function(){return this.throwIfDisposed(),Es(this)},be().prototype.squaredDifference=function(t){return this.throwIfDisposed(),ui(this,t)},be().prototype.squeeze=function(t){return this.throwIfDisposed(),hi(this,t)},be().prototype.stack=function(t,e){this.throwIfDisposed();const n=t instanceof ye?[this,t]:[this,...t];return ci(n,e)},be().prototype.step=function(t){return this.throwIfDisposed(),pi(this,t)},be().prototype.stridedSlice=function(t,e,n,s,i,r,a,o){return this.throwIfDisposed(),di(this,t,e,n,s,i,r,a,o)},be().prototype.sub=function(t){return this.throwIfDisposed(),ds(this,t)},be().prototype.sum=function(t,e){return this.throwIfDisposed(),fs(this,t,e)},be().prototype.tan=function(){return this.throwIfDisposed(),fi(this)},be().prototype.tanh=function(){return this.throwIfDisposed(),gn(this)},be().prototype.tile=function(t){return this.throwIfDisposed(),Gn(this,t)},be().prototype.toBool=function(){return this.throwIfDisposed(),Qe(this,"bool")},be().prototype.toFloat=function(){return this.throwIfDisposed(),Qe(this,"float32")},be().prototype.toInt=function(){return this.throwIfDisposed(),Qe(this,"int32")},be().prototype.topk=function(t,e){return this.throwIfDisposed(),gi(this,t,e)},be().prototype.transpose=function(t){return this.throwIfDisposed(),wi(this,t)},be().prototype.unique=function(t){return this.throwIfDisposed(),mi(this,t)},be().prototype.unsortedSegmentSum=function(t,e){return this.throwIfDisposed(),yi(this,t,e)},be().prototype.unstack=function(t){return this.throwIfDisposed(),bi(this,t)},be().prototype.where=function(t,e){return this.throwIfDisposed(),On(t,this,e)},be().prototype.zerosLike=function(){return this.throwIfDisposed(),Bn(this)};const Ai={kernelName:"Abs",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>pn(t,pi(Qe(n,"float32"),-1))}}},Ci={kernelName:$,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=Es(Qe(n,"float32")),s=li(ds(Zs(1),e));return us(Ln(t,s))}}}},$i={kernelName:D,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=li(ds(Es(Qe(n,"float32")),1));return Ln(t,e)}}}},Di={kernelName:T,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Rn(n.shape,s.shape);return{a:()=>{let e=t;const s=_n(n.shape,i);return s.length>0&&(e=fs(e,s)),on(e,n.shape)},b:()=>{let e=t;const n=_n(s.shape,i);return n.length>0&&(e=fs(e,n)),on(e,s.shape)}}}},Ti={kernelName:"AddN",saveAllInputs:!0,gradFunc:(t,e)=>{const n={};return e.forEach(((e,s)=>{n[s]=()=>t.clone()})),n}},Ei={kernelName:E,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Bn(n)}}},Fi={kernelName:F,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Bn(n)}}},Li={kernelName:L,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Ln(t,li(ds(Zs(1),Es(Qe(n,"float32")))))}}},_i={kernelName:_,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=li(Ke(Zs(1),Es(Qe(n,"float32"))));return Ln(t,e)}}}},Ri={kernelName:O,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Rn(n.shape,s.shape);return{a:()=>{const e=Ke(Es(n),Es(s));let r=pn(t,Ln(s,e));const a=_n(n.shape,i);return a.length>0&&(r=fs(r,a)),on(r,n.shape)},b:()=>{const e=Ke(Es(n),Es(s));let r=us(pn(t,Ln(n,e)));const a=_n(s.shape,i);return a.length>0&&(r=fs(r,a)),on(r,s.shape)}}}},Mi={kernelName:R,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Ln(t,Ke(Es(Qe(n,"float32")),1))}}},Oi={kernelName:M,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Ln(t,ds(Zs(1),Es(Qe(n,"float32"))))}}};const Bi=Be({avgPool3dGrad_:function(t,e,n,i,r,a){const l=Me(t,"dy","avgPool3dGrad"),u=Me(e,"input","avgPool3dGrad");let h=l,c=u,p=!1;4===u.rank&&(p=!0,h=on(l,[1,l.shape[0],l.shape[1],l.shape[2],l.shape[3]]),c=on(u,[1,u.shape[0],u.shape[1],u.shape[2],u.shape[3]])),s(5===h.rank,(()=>`Error in avgPool3dGrad: dy must be rank 5 but got rank ${h.rank}.`)),s(5===c.rank,(()=>`Error in avgPool3dGrad: input must be rank 5 but got rank ${c.rank}.`)),null!=a&&s(o(r),(()=>`Error in avgPool3dGrad: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`));const d={dy:h,input:c},f={filterSize:n,strides:i,pad:r,dimRoundingMode:a},g=Ee.runKernel("AvgPool3DGrad",d,f);return p?on(g,[g.shape[1],g.shape[2],g.shape[3],g.shape[4]]):g}}),Wi={kernelName:"AvgPool3D",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:r,pad:a,dimRoundingMode:o}=n;return{x:()=>Bi(t,s,i,r,a,o)}}};const Pi=Be({avgPoolGrad_:function(t,e,n,i,r){const a=Me(t,"dy","avgPoolGrad"),o=Me(e,"input","avgPoolGrad");s(o.rank===a.rank,(()=>`Rank of input (${o.rank}) does not match rank of dy (${a.rank})`));let l=o,u=a,h=!1;3===o.rank&&(h=!0,l=on(o,[1,o.shape[0],o.shape[1],o.shape[2]]),u=on(a,[1,a.shape[0],a.shape[1],a.shape[2]])),s(4===u.rank,(()=>`Error in avgPoolGrad: dy must be rank 4 but got rank ${u.rank}.`)),s(4===l.rank,(()=>`Error in avgPoolGrad: input must be rank 4 but got rank ${l.rank}.`));const c={dy:u,input:l},p={filterSize:n,strides:i,pad:r},d=Ee.runKernel("AvgPoolGrad",c,p);return h?on(d,[d.shape[1],d.shape[2],d.shape[3]]):d}}),Ui={kernelName:B,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:r,pad:a}=n;return{x:()=>Pi(t,s,i,r,a)}}},Ki={kernelName:W,inputsToSave:["a","b"],gradFunc:(t,e,n)=>{const[s,i]=e,{transposeA:r,transposeB:a}=n;return r||a?!r&&a?{a:()=>cn(t,i,!1,!1),b:()=>cn(t,s,!0,!1)}:r&&!a?{a:()=>cn(i,t,!1,!0),b:()=>cn(s,t,!1,!1)}:{a:()=>cn(i,t,!0,!0),b:()=>cn(t,s,!0,!0)}:{a:()=>cn(t,i,!1,!0),b:()=>cn(s,t,!0,!1)}}},Vi={kernelName:P,gradFunc:(t,e,n)=>{const{blockShape:s,crops:i}=n;return{x:()=>Ms(t,s,i)}}},ji={kernelName:"BroadcastTo",gradFunc:(t,e,n)=>{const s=n,i=s.inputShape,r=s.shape,a=Array.from(r);for(let t=i.length-1;t>=0;t--)if(i[t]===r[t])a[t]=1;else if(1!==i[t])throw new Error(`broadcastTo(): [${i}] cannot be broadcast to [${r}].`);const o=[];for(let t=0;t<a.length;t++)a[t]>1&&o.push(t);return{x:()=>fs(t,o,!0)}}},qi={kernelName:U,gradFunc:t=>({x:()=>t.clone()})},Gi={kernelName:K,gradFunc:t=>({x:()=>Bn(t)})},Hi={kernelName:V,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{clipValueMin:i,clipValueMax:r}=n;return{x:()=>On(ws(Xn(s,i),is(s,r)),t,Bn(t))}}},Ji={kernelName:j,inputsToSave:["x"],gradFunc:Ai.gradFunc},Zi={kernelName:q,saveAllInputs:!0,gradFunc:(t,e,n)=>{const s=e.map((t=>t.shape)),{axis:i}=n,r=u(i,e[0].shape)[0],a=s.map((t=>t[r]));return ai(t,a,r).map((t=>()=>t))}},Xi={kernelName:G,inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[i,r]=e,{dilations:a,strides:o,pad:l,dataFormat:u}=n;return s(rn(a),(()=>`Error in gradient of conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${a}'`)),{x:()=>In(i.shape,t,r,o,l,u),filter:()=>vi(i,t,r.shape,o,l,u)}}},Yi={kernelName:H,inputsToSave:["dy","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,{strides:r,pad:a,dataFormat:o,dimRoundingMode:l}=n;return{dy:()=>vn(t,i,r,a,o,1,l),filter:()=>vi(t,s,i.shape,r,a,o,l)}}};const Qi=Be({conv3DBackpropFilter_:function(t,e,n,i,r){let a=t;4===t.rank&&(a=on(t,[1,t.shape[0],t.shape[1],t.shape[2],t.shape[3]]));let o=e;4===o.rank&&(o=on(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]])),s(5===a.rank,(()=>`Error in conv3dDerFilter: input must be rank 5, but got shape ${a.shape}.`)),s(5===o.rank,(()=>`Error in conv3dDerFilter: dy must be rank 5, but got shape ${o.shape}.`)),s(5===n.length,(()=>`Error in conv3dDerFilter: filterShape must be length 5, but got ${n}.`)),s(a.shape[4]===n[3],(()=>`Error in conv3dDerFilter: depth of input ${a.shape[4]}) must match input depth in filter (${n[3]}.`)),s(o.shape[4]===n[4],(()=>`Error in conv3dDerFilter: depth of dy (${o.shape[4]}) must match output depth for filter (${n[4]}).`));const l={x:a,dy:o},u={strides:i,pad:r,filterShape:n};return Ee.runKernel("Conv3DBackpropFilterV2",l,u)}}),tr={kernelName:"Conv3D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:i,strides:r,pad:a}=n;s(rn(i),(()=>`Error in gradient of conv3D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${i}'`));const[o,l]=e;return{x:()=>zn(o.shape,t,l,r,a),filter:()=>Qi(o,t,l.shape,r,a)}}},er={kernelName:"Cos",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>pn(us(ti(Qe(n,"float32"))),t)}}},nr={kernelName:J,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>pn(ei(Qe(n,"float32")),t)}}},sr={kernelName:Z,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i,exclusive:r,reverse:a}=n;return{x:()=>{const e=function(t,e){if(function(t,e){for(let n=0;n<t.length;++n)if(t[t.length-n-1]!==e-1-n)return!1;return!0}(t,e))return null;const n=[];for(let s=0;s<e;++s)-1===t.indexOf(s)&&n.push(s);return t.forEach((t=>n.push(t))),n}([i],s.rank);let n=$n(t,i,r,!a);return null!=e&&(n=wi(n,e)),n}}}},ir={kernelName:X,inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:i,strides:r,pad:a,dimRoundingMode:l}=n,u=null==i?[1,1]:i;s(rn(u),(()=>`Error in gradient of depthwiseConv2dNative: dilation rates greater than 1 are not yet supported. Got dilations '${u}'`));const[h,c]=e;return s(4===h.rank,(()=>`Error in gradient of depthwiseConv2dNative: input must be rank 4, but got rank ${h.rank}.`)),s(4===c.rank,(()=>`Error in gradient of depthwiseConv2dNative: filter must be rank 4, but got rank ${c.rank}.`)),s(h.shape[3]===c.shape[2],(()=>`Error in gradient of depthwiseConv2d: number of input channels (${h.shape[3]}) must match the inChannels dimension in filter ${c.shape[2]}.`)),s(an(r,u),(()=>`Error in gradient of depthwiseConv2d: Either strides or dilations must be  1. Got strides ${r} and dilations '${u}'.`)),null!=l&&s(o(a),(()=>`Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode ${l} but got pad ${a}.`)),{x:()=>Ii(h.shape,t,c,r,a,i,l),filter:()=>Si(h,t,c.shape,r,a,i,l)}}},rr={kernelName:Y,inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,r={x:s,filter:i,dy:t},a={x:s,filter:i,dy:t};return{x:()=>Ee.runKernel("Dilation2DBackpropInput",r,n),filter:()=>Ee.runKernel("Dilation2DBackpropFilter",a,n)}}},ar={kernelName:"Elu",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e,s={dy:t,y:n};return{x:()=>Ee.runKernel("EluGrad",s)}}},or={kernelName:"Erf",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=pn(Vn(us(Es(n))),2/Math.sqrt(Math.PI));return{x:()=>pn(t,s)}}},lr={kernelName:"Exp",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>pn(t,n)}}},ur={kernelName:tt,inputsToSave:["input"],gradFunc:(t,e)=>{const[n]=e;return{input:()=>on(t,n.shape)}}},hr={kernelName:et,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>pn(t,Vn(n))}}},cr={kernelName:nt,gradFunc:t=>({x:()=>Bn(t)})},pr={kernelName:st,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Rn(n.shape,s.shape);return{a:()=>{const e=Ln(t,Qe(s,"float32")),r=_n(n.shape,i);return r.length>0?on(fs(e,r),n.shape):e},b:()=>{let e=pn(t,Qe(n,"float32"));const r=_n(s.shape,i);r.length>0&&(e=on(fs(e,r),s.shape));const a=Es(s);return us(Ln(e,Qe(a,"float32")))}}}},dr={kernelName:it,inputsToSave:["x","mean","variance","scale"],gradFunc:(t,e,n)=>{const{varianceEpsilon:s}=n,[i,r,a,o]=e,l=null==o?Zs(1):o,u=_n(r.shape,i.shape),h=[];if(1===r.rank){for(let t=0;t<i.shape.length-1;++t)h.push(i.shape[t]);h.push(1)}const c=ds(i,r),p=pn(t,l),d=Hs(Ke(a,Zs(s))),f=pn(pn(pn(d,d),d),Zs(-.5));return{x:()=>1===r.rank?on(pn(pn(t,Gn(on(d,[1,1,1,r.shape[0]]),h)),l),i.shape):on(pn(pn(t,d),l),i.shape),mean:()=>{let t=pn(pn(d,Zs(-1)),p);return 1===r.rank&&(t=fs(t,u)),on(t,r.shape)},variance:()=>{let t=pn(pn(f,c),p);return 1===r.rank&&(t=fs(t,u)),on(t,r.shape)},scale:()=>{const e=pn(c,d);let n=pn(t,e);return 1===r.rank&&(n=fs(n,u)),on(n,r.shape)},offset:()=>{let e=t;return 1===r.rank&&(e=fs(e,u)),on(e,r.shape)}}}},fr={kernelName:rt,inputsToSave:["x","indices"],gradFunc:(t,e,n)=>{const[s,i]=e,{axis:r}=n,a=u(r,s.shape)[0];return{x:()=>{const e=s.shape,n=i.size,o=e.slice(0,a),l=o.length,u=e.slice(r,e.length).slice(1),h=u.length,c=gr(0,l),p=gr(l+1,l+1+h),d=mr([o,[n],u]),f=on(t,d),g=on(i,[n]),m=mr([[l],c,p]),y=wi(f,m);let b=yi(y,g,s.shape[a]);const w=ys(m);return b=wi(b,w),b},indices:()=>i}}};function gr(t,e){const n=[];for(let s=t;s<e;++s)n.push(s);return n}function mr(t){const e=[];for(let n=0;n<t.length;++n)for(let s=0;s<t[n].length;++s)e.push(t[n][s]);return e}const yr={kernelName:at,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>Bn(n),b:()=>Bn(s)}}},br={kernelName:ot,gradFunc:t=>({x:()=>Qe(t,"float32")})},wr={kernelName:lt,gradFunc:t=>({x:()=>Bn(t)})},kr={kernelName:ut,gradFunc:t=>({x:()=>Bn(t)})},xr={kernelName:ht,gradFunc:t=>({x:()=>Bn(t)})},vr={kernelName:ct,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{alpha:i}=n,r=Zn(s,0);return{x:()=>On(r,t,pn(t,i))}}},Sr={kernelName:pt,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Ln(t,Ke(n,1))}}},Ir={kernelName:"Log",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Ln(t,Qe(n,"float32"))}}},Nr={kernelName:"LogSoftmax",inputsToSave:[],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n;return{logits:()=>{const e=Vn(s);return ds(t,pn(fs(t,i,!0),e))}}}};const zr=Be({localResponseNormalizationBackprop_:function(t,e,n,s=5,i=1,r=1,a=.5){const o={x:t,y:e,dy:n},l={depthRadius:s,bias:i,alpha:r,beta:a};return Ee.runKernel("LRNGrad",o,l)}}),Ar={kernelName:"LRN",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{depthRadius:r,bias:a,alpha:o,beta:l}=n;return{x:()=>zr(s,i,t,r,a,o,l)}}};function Cr(t,e,n,s){return e.rank<n.rank&&(e=on(e,ms(e.shape,s))),t.rank<n.rank&&(t=on(t,ms(t.shape,s))),{x:()=>pn(t,Qe(Mn(n,e),t.dtype))}}const $r={kernelName:"Max",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{reductionIndices:i}=s,r=e[0],a=Cr(t,e[1],r,u(i,r.shape));return{x:()=>a.x()}}},Dr={kernelName:dt,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>pn(t,Qe(Xn(n,s),"float32")),b:()=>pn(t,Qe(ss(n,s),"float32"))}}};const Tr=Be({maxPool3dGrad_:function(t,e,n,i,r,a,l){const u=Me(t,"dy","maxPool3dGrad"),h=Me(e,"input","maxPool3dGrad"),c=Me(n,"output","maxPool3dGrad");let p=u,d=h,f=c,g=!1;4===h.rank&&(g=!0,p=on(u,[1,u.shape[0],u.shape[1],u.shape[2],u.shape[3]]),d=on(h,[1,h.shape[0],h.shape[1],h.shape[2],h.shape[3]]),f=on(c,[1,c.shape[0],c.shape[1],c.shape[2],c.shape[3]])),s(5===p.rank,(()=>`Error in maxPool3dGrad: dy must be rank 5 but got rank ${p.rank}.`)),s(5===d.rank,(()=>`Error in maxPool3dGrad: input must be rank 5 but got rank ${d.rank}.`)),s(5===f.rank,(()=>`Error in maxPool3dGrad: output must be rank 5 but got rank ${f.rank}.`)),null!=l&&s(o(a),(()=>`Error in maxPool3dGrad: pad must be an integer when using, dimRoundingMode ${l} but got pad ${a}.`));const m={dy:p,input:d,output:f},y={filterSize:i,strides:r,pad:a,dimRoundingMode:l},b=Ee.runKernel("MaxPool3DGrad",m,y);return g?on(b,[b.shape[1],b.shape[2],b.shape[3],b.shape[4]]):b}}),Er={kernelName:"MaxPool3D",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:r,strides:a,pad:o,dimRoundingMode:l}=n;return{x:()=>Tr(t,s,i,r,a,o,l)}}};const Fr=Be({maxPoolGrad_:function(t,e,n,i,r,a,l){const u=Me(t,"dy","maxPoolGrad"),h=Me(e,"input","maxPoolGrad"),c=Me(n,"output","maxPoolGrad");s(h.rank===u.rank,(()=>`Rank of input (${h.rank}) does not match rank of dy (${u.rank})`)),s(4===u.rank,(()=>`Error in maxPoolGrad: dy must be rank 4 but got rank ${u.rank}.`)),s(4===h.rank,(()=>`Error in maxPoolGrad: input must be rank 4 but got rank ${h.rank}.`)),null!=l&&s(o(a),(()=>`Error in maxPoolGrad: pad must be an integer when using, dimRoundingMode ${l} but got pad ${a}.`));const p={dy:u,input:h,output:c},d={filterSize:i,strides:r,pad:a,dimRoundingMode:l};return Ee.runKernel("MaxPoolGrad",p,d)}}),Lr={kernelName:vt,inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,r=i.map((t=>t[0]));return{x:()=>fn(t,r,s.shape)}}};const _r={kernelName:Pt,gradFunc:(t,e,n)=>{const{blockShape:s,paddings:i}=n;return{x:()=>mn(t,s,i)}}},Rr={kernelName:Ut,gradFunc:(t,e,n)=>{const{axis:s}=n;return{x:()=>hn(t,s)}}};const Mr=[Ai,Ci,$i,Di,Ti,Ei,Fi,Li,_i,Ri,Mi,Oi,Wi,Ui,Ki,Vi,ji,qi,Gi,Hi,Ji,Zi,Yi,Xi,tr,er,nr,sr,ir,rr,{kernelName:Q,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Rn(n.shape,s.shape);return{a:()=>{const e=Ln(t,Qe(s,"float32")),r=_n(n.shape,i);return r.length>0?on(fs(e,r),n.shape):e},b:()=>{let e=pn(t,Qe(n,"float32"));const r=_n(s.shape,i);r.length>0&&(e=on(fs(e,r),s.shape));const a=Es(s);return us(Ln(e,Qe(a,"float32")))}}}},ar,or,lr,ur,hr,pr,cr,dr,fr,yr,br,wr,kr,xr,vr,Sr,Ir,Nr,Ar,$r,$r,Dr,Er,{kernelName:ft,inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:r,strides:a,pad:o}=n;return{x:()=>Fr(t,s,i,r,a,o)}}},{kernelName:gt,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n,a=u(i,s.shape),o=r(function(t,e){const n=[],s=t.length;for(let i=0;i<s;i++)-1===e.indexOf(i)&&n.push(t[i]);return[n,e.map((e=>t[e]))]}(s.shape,a)[1]);return{x:()=>{const e=s.shape.slice();a.forEach((t=>{e[t]=1}));const n=on(t,e);return Ln(pn(n,As(s.shape,"float32")),o)}}}},{kernelName:"Min",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{axis:i}=s,[r,a]=e,o=Cr(t,a,r,u(i,r.shape));return{x:()=>o.x()}}},{kernelName:mt,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>pn(t,Qe(is(n,s),"float32")),b:()=>pn(t,Qe(Zn(n,s),"float32"))}}},{kernelName:yt,inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,r=i.map((t=>t[0]));return{x:()=>fn(t,r,s.shape)}}},{kernelName:"Mod",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Rn(n.shape,s.shape);return{a:()=>{const e=_n(n.shape,i);return e.length>0?on(fs(t,e),n.shape):t},b:()=>{const e=pn(t,us(Hn(Ln(n,s)))),r=_n(s.shape,i);return r.length>0?on(fs(e,r),s.shape):e}}}},{kernelName:bt,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Rn(n.shape,s.shape);return{a:()=>{const e=pn(t,Qe(s,"float32")),r=_n(n.shape,i);return r.length>0?on(fs(e,r),n.shape):e},b:()=>{const e=pn(t,Qe(n,"float32")),r=_n(s.shape,i);return r.length>0?on(fs(e,r),s.shape):e}}}},{kernelName:"Neg",gradFunc:t=>({x:()=>us(t)})},{kernelName:kt,inputsToSave:["indices"],gradFunc:(t,e)=>{const n=e[0];return{indices:()=>zs(n.shape,"float32")}}},{kernelName:wt,gradFunc:t=>({x:()=>Bn(t)})},{kernelName:xt,saveAllInputs:!0,gradFunc:(t,e,n)=>{const{axis:s}=n;return bi(t,s).map((t=>()=>t))}},Lr,Lr,{kernelName:"Pow",inputsToSave:["a","b"],outputsToSave:[!0],gradFunc:(t,e)=>{const[n,s,i]=e,r=n,a=s,o=Rn(r.shape,a.shape);return{a:()=>{const e=Qe(a,"float32");let n=pn(t,pn(e,Bs(r,ds(e,Zs(1)))));const s=_n(r.shape,o);return s.length>0&&(n=fs(n,s)),on(n,r.shape)},b:()=>{const e=Zn(r,0),n=On(e,as(r),Bn(r));let s=pn(t,pn(i,n));const l=_n(a.shape,o);return l.length>0&&(s=fs(s,l)),on(s,a.shape)}}}},{kernelName:St,inputsToSave:["x","alpha"],gradFunc:(t,e)=>{const[n,s]=e,i=Zn(n,0);return{x:()=>On(i,t,pn(t,s)),alpha:()=>{let e=On(i,Bn(t),pn(t,n));const r=_n(s.shape,t.shape);return r.length>0&&(e=fs(e,r)),on(e,s.shape)}}}},{kernelName:It,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Ln(t,us(Es(n)))}}},{kernelName:$t,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=pn(is(n,6),pi(n));return{x:()=>pn(t,Qe(s,"float32"))}}},{kernelName:Nt,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>pn(t,Qe(pi(n),"float32"))}}},{kernelName:zt,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>on(t,n.shape)}}},{kernelName:Ct,inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>Ee.runKernel("ResizeBilinearGrad",i,n)}}},{kernelName:At,inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>Ee.runKernel("ResizeNearestNeighborGrad",i,n)}}},{kernelName:Dt,gradFunc:(t,e,n)=>{const{dims:s}=n,i=u(s,t.shape);return{x:()=>qs(t,i)}}},{kernelName:Tt,gradFunc:t=>({x:()=>Bn(t)})},{kernelName:Et,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>us(Ln(t,pn(Bs(n,1.5),2)))}}},{kernelName:Ft,inputsToSave:["condition"],gradFunc:(t,e)=>{const[n]=e;return{condition:()=>Qe(Bn(n),"float32"),t:()=>pn(t,Qe(n,t.dtype)),e:()=>pn(t,Qe(ks(n),t.dtype))}}},{kernelName:Lt,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=Zn(n,Zs(0)),s=Zs(1.7580993408473768),i=Zs(1.0507009873554805),r=pn(t,i),a=pn(pn(t,s),Vn(Qe(n,"float32")));return On(e,r,a)}}}},{kernelName:Ot,outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>pn(t,pn(n,ds(Zs(1),n)))}}},{kernelName:Mt,gradFunc:t=>({x:()=>Bn(t)})},{kernelName:"Sin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>pn(An(Qe(n,"float32")),t)}}},{kernelName:Rt,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>pn(Cn(Qe(n,"float32")),t)}}},{kernelName:_t,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[i]=e,{begin:r,size:a}=n,o=i.shape,[l,u]=function(t,e,n){let i;const r=t.shape.length;let a;return i="number"==typeof e?[e,...new Array(r-1).fill(0)]:e.length<r?e.concat(new Array(r-e.length).fill(0)):e.slice(),i.forEach((t=>{s(-1!==t,(()=>"slice() does not support negative begin indexing."))})),a=null==n?new Array(r).fill(-1):"number"==typeof n?[n,...new Array(r-1).fill(-1)]:n.length<r?n.concat(new Array(r-n.length).fill(-1)):n,a=a.map(((e,n)=>e>=0?e:(s(-1===e,(()=>`Negative size values should be exactly -1 but got ${e} for the slice() size at index ${n}.`)),t.shape[n]-i[n]))),[i,a]}(i,r,a),h=[];for(let e=0;e<t.rank;e++)h.push([l[e],o[e]-l[e]-u[e]]);return{x:()=>Rs(t,h)}}},{kernelName:Kt,outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{dim:i}=n,r=pn(t,s);return{logits:()=>ds(r,pn(fs(r,[i],true),s))}}},{kernelName:Bt,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>pn(t,dn(n))}}},_r,_r,Rr,Rr,{kernelName:Wt,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Ln(t,pn(li(Qe(n,"float32")),2))}}},{kernelName:Vt,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Zs(2);return{a:()=>pn(t,pn(i,ds(n,s))),b:()=>pn(t,pn(i,ds(s,n)))}}},{kernelName:"Square",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>pn(t,pn(Qe(n,"float32"),2))}}},{kernelName:Xt,gradFunc:t=>({x:()=>Bn(t)})},{kernelName:"Sub",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Rn(n.shape,s.shape);return{a:()=>{let e=t;const s=_n(n.shape,i);return s.length>0&&(e=fs(e,s)),on(e,n.shape)},b:()=>{let e=t;const n=_n(s.shape,i);return n.length>0&&(e=fs(e,n)),on(us(e),s.shape)}}}},{kernelName:"Sum",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,i=s.shape.slice(),{axis:r}=n;u(r,s.shape).forEach((t=>{i[t]=1}));const a=on(t,i),o=pn(a,As(s.shape,"float32"));return{x:()=>o}}},{kernelName:"Tan",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Ln(t,Es(An(n)))}}},{kernelName:jt,outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>pn(ds(Zs(1),Es(n)),t)}}},{kernelName:qt,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{reps:i}=n;return{x:()=>{let e=Bn(s);if(1===s.rank)for(let n=0;n<i[0];++n)e=Ke(e,fn(t,[n*s.shape[0]],[s.shape[0]]));else if(2===s.rank)for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)e=Ke(e,fn(t,[n*s.shape[0],r*s.shape[1]],[s.shape[0],s.shape[1]]));else if(3===s.rank)for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)for(let a=0;a<i[2];++a)e=Ke(e,fn(t,[n*s.shape[0],r*s.shape[1],a*s.shape[2]],[s.shape[0],s.shape[1],s.shape[2]]));else{if(4!==s.rank)throw new Error(`Gradient for tile operation is not implemented for rank-${s.rank} tensors yet.`);for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)for(let a=0;a<i[2];++a)for(let o=0;o<i[3];++o)e=Ke(e,fn(t,[n*s.shape[0],r*s.shape[1],a*s.shape[2],o*s.shape[3]],[s.shape[0],s.shape[1],s.shape[2],s.shape[3]]))}return e}}}},{kernelName:Gt,gradFunc:(t,e,n)=>{const s=n,{perm:i}=s,r=ys(i);return{x:()=>wi(t,r)}}},{kernelName:Ht,gradFunc:(t,e,n)=>{const s=n,{axis:i}=s;return{value:()=>ci(t,i)}}},{kernelName:Jt,inputsToSave:["segmentIds"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>function(t,e){const n=Is(e,Bn(e)),s=Jn(t,n);let i=Xn(e,Zs(0,"int32"));const r=s.rank-i.rank;for(let t=0;t<r;++t)i=jn(i,t+1);i=ws(i,As(s.shape,"bool"));const a=Bn(s);return On(i,s,a)}(t,n)}}},{kernelName:Zt,gradFunc:t=>({x:()=>Bn(t)})}];for(const t of Mr)se(t);let Or;function Br(){return null==Or&&(Or=e.backend().epsilon()),Or}class Wr extends Error{constructor(t){super(t),Object.setPrototypeOf(this,Wr.prototype)}}class Pr extends Error{constructor(t){super(t),Object.setPrototypeOf(this,Pr.prototype)}}class Ur extends Error{constructor(t){super(t),Object.setPrototypeOf(this,Ur.prototype)}}class Kr extends Error{constructor(t){super(t),Object.setPrototypeOf(this,Kr.prototype)}}class Vr extends Error{constructor(t){super(t),Object.setPrototypeOf(this,Vr.prototype)}}function jr(t,e){if(Array.isArray(t)){let n=[];for(let s=0;s<e;s++)n=n.concat(t);return n}{const n=new Array(e);return n.fill(t),n}}function qr(t,e){if(!t)throw new Vr(e)}function Gr(t,e){let n=0;for(const s of t)s===e&&n++;return n}function Hr(t){return 1===t.length?t[0]:t}function Jr(t){return Array.isArray(t)?t:[t]}function Zr(t){const e=t.replace(/(.)([A-Z][a-z0-9]+)/g,"$1_$2").replace(/([a-z])([A-Z])/g,"$1_$2").toLowerCase();return"_"!==e[0]?e:"private"+e}function Xr(t){return t.length<=1||-1===t.indexOf("_")?t:t.replace(/[_]+(\w|$)/g,((t,e)=>e.toUpperCase()))}let Yr={};function Qr(t){if(null==t)return null;const e={};return e.className=t.getClassName(),e.config=t.getConfig(),e}function ta(t){if(null!=t&&"object"==typeof t)if(Array.isArray(t))t.forEach((t=>ta(t)));else{const e=Object.keys(t);for(const n of e){const e=t[n];null!=e&&"object"==typeof e&&(Array.isArray(e)||"ndarray"!==e.type||"number"!=typeof e.value?ta(e):t[n]=e.value)}}}function ea(t,e={},n={},s="object",i=!1){if("string"==typeof t){const i=t;let r;if(i in n)r=n[i];else if(i in Yr)r=Yr[i];else if(r=e[i],null==r)throw new Ur(`Unknown ${s}: ${t}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);return r}{const r=t;if(null==r.className||null==r.config)throw new Ur(`${s}: Improper config format: ${JSON.stringify(r)}.\n'className' and 'config' must set.`);const a=r.className;let o,l;if(a in n?[o,l]=n[a]:a in Yr?[o,l]=Yr.className:a in e&&([o,l]=e[a]),null==o)throw new Ur(`Unknown ${s}: ${a}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);if(null!=l){const t={};for(const e of Object.keys(Yr))t[e]=Yr[e];for(const e of Object.keys(n))t[e]=n[e];r.config.customObjects=t;const e=Object.assign({},Yr);for(const t of Object.keys(n))Yr[t]=n[t];ta(r.config);const s=l(o,r.config,n,i);return Yr=Object.assign({},e),s}{const t=Object.assign({},Yr);for(const t of Object.keys(n))Yr[t]=n[t];const e=new o(r.config);return Yr=Object.assign({},t),e}}}function na(t,e){return-1*function(t,e){return t<e?-1:t>e?1:0}(t,e)}function sa(t){if(null==t)return t;const e=[];for(const n of t)-1===e.indexOf(n)&&e.push(n);return e}function ia(t){if(null==t)throw new Ur(`Invalid value in obj: ${JSON.stringify(t)}`);for(const e in t)if(t.hasOwnProperty(e))return!1;return!0}function ra(t,e,n){if(null!=n&&t.indexOf(n)<0)throw new Ur(`${n} is not a valid ${e}.  Valid values are ${t} or null/undefined.`)}function aa(t,e,n=0,s=1/0){return qr(n>=0),qr(s>=n),Array.isArray(t)&&t.length>=n&&t.length<=s&&t.every((t=>typeof t===e))}function oa(t,n){Array.isArray(t)?(e.util.assert(t.length>0,(()=>`${n} is unexpectedly an empty array.`)),t.forEach(((t,e)=>oa(t,`element ${e+1} of ${n}`)))):e.util.assert(Number.isInteger(t)&&t>0,(()=>`Expected ${n} to be a positive integer, but got ${la(t)}.`))}function la(t){return null===t?"null":Array.isArray(t)?"["+t.map((t=>la(t))).join(",")+"]":"string"==typeof t?`"${t}"`:`${t}`}function ua(t){return"relu"===t?"relu":"linear"===t?"linear":"elu"===t?"elu":null}function ha(t,n){return e.tidy((()=>e.sqrt(e.sum(e.mul(t,t),n,!0))))}class ca extends e.serialization.Serializable{getConfig(){return{}}}class pa extends ca{constructor(t){super(),this.defaultMaxValue=2,this.defaultAxis=0,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return e.tidy((()=>{const n=ha(t,this.axis),s=e.clipByValue(n,0,this.maxValue);return e.mul(t,e.div(s,e.add(Br(),n)))}))}getConfig(){return{maxValue:this.maxValue,axis:this.axis}}}pa.className="MaxNorm",e.serialization.registerClass(pa);class da extends ca{constructor(t){super(),this.defaultAxis=0,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return e.tidy((()=>e.div(t,e.add(Br(),ha(t,this.axis)))))}getConfig(){return{axis:this.axis}}}da.className="UnitNorm",e.serialization.registerClass(da);class fa extends ca{apply(t){return e.relu(t)}}fa.className="NonNeg",e.serialization.registerClass(fa);class ga extends ca{constructor(t){super(),this.defaultMinValue=0,this.defaultMaxValue=1,this.defaultRate=1,this.defaultAxis=0,this.minValue=null!=t.minValue?t.minValue:this.defaultMinValue,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.rate=null!=t.rate?t.rate:this.defaultRate,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return e.tidy((()=>{const n=ha(t,this.axis),s=e.add(e.mul(this.rate,e.clipByValue(n,this.minValue,this.maxValue)),e.mul(1-this.rate,n));return e.mul(t,e.div(s,e.add(Br(),n)))}))}getConfig(){return{minValue:this.minValue,maxValue:this.maxValue,rate:this.rate,axis:this.axis}}}ga.className="MinMaxNorm",e.serialization.registerClass(ga);const ma={maxNorm:"MaxNorm",minMaxNorm:"MinMaxNorm",nonNeg:"NonNeg",unitNorm:"UnitNorm"};function ya(t){return Qr(t)}function ba(t,n={}){return ea(t,e.serialization.SerializationMap.getMap().classNameMap,n,"constraint")}function wa(t){if(null==t)return null;if("string"==typeof t){return ba({className:t in ma?ma[t]:t,config:{}})}return t instanceof ca?t:ba(t)}var ka=Object.freeze({__proto__:null,maxNorm:function(t){return new pa(t)},unitNorm:function(t){return new da(t)},nonNeg:function(){return new fa},minMaxNorm:function(t){return new ga(t)}});const xa=["channelsFirst","channelsLast"],va=["nearest","bilinear"],Sa=["valid","same","causal"],Ia=["max","avg"],Na=["sum","mul","concat","ave"],za=new Map;function Aa(t){ra(xa,"DataFormat",t)}function Ca(t){ra(Sa,"PaddingMode",t)}function $a(t){ra(Ia,"PoolMode",t)}const Da=[];function Ta(t,e){Da.push(t);try{const t=e();return Da.pop(),t}catch(t){throw Da.pop(),t}}function Ea(t){if(!_a(t))throw new Error("Not a valid tensor name: '"+t+"'");return(0===Da.length?"":Da.join("/")+"/")+t}function Fa(t){if(!_a(t))throw new Error("Not a valid tensor name: '"+t+"'");za.has(t)||za.set(t,0);const e=za.get(t);if(za.set(t,za.get(t)+1),e>0){const n=`${t}_${e}`;return za.set(n,1),n}return t}const La=new RegExp(/^[A-Za-z0-9][-A-Za-z0-9\._\/]*$/);function _a(t){return!!t.match(La)}function Ra(t,e,n){null==e&&(e=0),null==n&&(n=t.length);let s=1;for(let i=e;i<n;++i)s*=t[i];return s}function Ma(t){return t=Array.isArray(t)?new Float32Array(t):t,e.tensor1d(t)}function Oa(t){return e.min(Ma(t)).dataSync()[0]}function Ba(t){return e.max(Ma(t)).dataSync()[0]}function Wa(t,e){if(e<t)throw new Ur(`end (${e}) < begin (${t}) is forbidden.`);const n=[];for(let s=t;s<e;++s)n.push(s);return n}function Pa(t,e){return t.asType(e)}function Ua(t,e=-1){const n=t.shape.slice();return e<0&&(e=n.length+e+1),n.splice(e,0,1),t.reshape(n)}function Ka(t,n,s){return e.tidy((()=>{switch(t.rank){case 1:return e.slice1d(t,n,s);case 2:return e.slice2d(t,[n,0],[s,t.shape[1]]);case 3:return e.slice3d(t,[n,0,0],[s,t.shape[1],t.shape[2]]);case 4:return e.slice4d(t,[n,0,0,0],[s,t.shape[1],t.shape[2],t.shape[3]]);case 5:return e.slice(t,[n,0,0,0,0],[s,t.shape[1],t.shape[2],t.shape[3],t.shape[4]]);case 6:return e.slice(t,[n,0,0,0,0,0],[s,t.shape[1],t.shape[2],t.shape[3],t.shape[4],t.shape[5]]);default:throw new Ur(`sliceAlongFirstAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function Va(t,n,s){return e.tidy((()=>{switch(t.rank){case 1:return e.slice1d(t,n,s);case 2:return e.slice2d(t,[0,n],[t.shape[0],s]);case 3:return e.slice3d(t,[0,0,n],[t.shape[0],t.shape[1],s]);case 4:return e.slice4d(t,[0,0,0,n],[t.shape[0],t.shape[1],t.shape[2],s]);default:throw new Ur(`sliceAlongLastAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function ja(t,n,s,i){return e.tidy((()=>{switch(t.rank){case 1:return e.slice1d(t,n,s);case 2:switch(i){case 1:return Ka(t,n,s);case 2:return Va(t,n,s);default:throw new Ur(`The axis is not within the rank of the tensor ${i}`)}case 3:switch(i){case 1:return Ka(t,n,s);case 2:return e.slice3d(t,[0,n,0],[t.shape[0],s,t.shape[2]]);case 3:return Va(t,n,s);default:throw new Ur(`The axis is not within the rank of the tensor ${i}`)}case 4:switch(i){case 1:return Ka(t,n,s);case 2:return e.slice4d(t,[0,n,0,0],[t.shape[0],s,t.shape[2],t.shape[3]]);case 3:return e.slice4d(t,[0,0,n,0],[t.shape[0],t.shape[1],s,t.shape[3]]);case 4:return Va(t,n,s);default:throw new Ur(`The axis is not within the rank of the tensor ${i}`)}default:throw new Ur(`sliceAlongLastAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function qa(t,n=-1){let s;return n<0&&(s=t[0].rank,n=0!==s?s:0),n===t[0].rank&&(n=-1),e.concat(t,n)}function Ga(t,n){switch(t.rank){case 1:return e.concat1d([t,n]);case 2:return e.concat2d([t,n],0);case 3:return e.concat3d([t,n],0);case 4:return e.concat4d([t,n],0);default:throw new Ur(`concatAlongFirstAxis() received an unsupported tensor rank: ${t.rank}`)}}function Ha(t,n){if(Array.isArray(n)||(n=[n]),t.rank!==n.length)throw new Ur(`The length of input n (${n.length}) does not match the number of dimensions in input x (${t.rank})`);return e.tile(t,n)}function Ja(t,n=0,s=1,i,r){return e.randomNormal(t,n,s,i,r)}function Za(t,n,s,i){if(t.rank<2||n.rank<2)throw new Kr(`dot requires both inputs to be rank >= 2 but got x shape = ${t.shape} and y shape = ${n.shape}`);if(n.rank>=3){if(t.shape.slice(-1)[0]!==n.shape.slice(-2)[0])throw new Kr(`If rank y >= 3, then the second last dim of y must equal the last dim of x but got x shape = ${t.shape} and  y shape = ${n.shape}`)}if(2===t.rank&&2===n.rank){const r=!1,a=!1;return e.fused.matMul({a:t,b:n,transposeA:r,transposeB:a,bias:i?Qa(t.rank,i,"channelsLast"):null,activation:s})}{const r=t.shape.slice(),a=r.pop();t=t.reshape([-1,a]);const o=n.shape.slice(),l=o.pop(),u=o.pop(),h=[...o,l],c=Array.from({length:n.rank},((t,e)=>0===e?n.rank-2:e<=n.rank-2?e-1:e));n=n.transpose(c).reshape([u,-1]);const p=[...r,...h],d=!1,f=!1;return e.fused.matMul({a:t,b:n,transposeA:d,transposeB:f,bias:i?Qa(t.rank,i,"channelsLast"):null,activation:s}).reshape(p)}}function Xa(t,n,s){return e.tidy((()=>(n=Array.isArray(n)?e.tensor1d(n,"int32"):n.toInt(),e.gather(t,n,s))))}function Ya(t){return e.mul(t,t)}function Qa(t,e,n){const s=e.shape;if(1!==e.rank&&e.rank!==t)throw new Ur(`Unexpected bias dimensions: ${e.rank}; expected it to be 1 or ${t}`);if(5===t){if("channelsFirst"===n)return 1===s.length?e.reshape([1,s[0],1,1,1]):e.reshape([1,s[3],s[0],s[1],s[2]]);if("channelsLast"===n)return 1===s.length?e.reshape([1,1,1,1,s[0]]):e.reshape([1].concat(s))}else if(4===t){if("channelsFirst"===n)return 1===s.length?e.reshape([1,s[0],1,1]):e.reshape([1,s[2],s[0],s[1]]);if("channelsLast"===n)return 1===s.length?e.reshape([1,1,1,s[0]]):e.reshape([1].concat(s))}else if(3===t){if("channelsFirst"===n)return 1===s.length?e.reshape([1,s[0],1]):e.reshape([1,s[1],s[0]]);if("channelsLast"===n)return 1===s.length?e.reshape([1,1,s[0]]):e.reshape([1].concat(s))}else if(t<3)return e;throw new Ur(`Unsupported input rank by biasAdd: ${e.rank}`)}function to(t,n,s){return e.tidy((()=>(null==s&&(s="channelsLast"),Aa(s),t.add(Qa(t.rank,n,s)))))}function eo(t,n,s,i){return e.tidy((()=>e.dropout(t,n,s,i)))}function no(t,e,n=!1){return n?t():e()}const so=["fanIn","fanOut","fanAvg"],io=["normal","uniform","truncatedNormal"];class ro extends e.serialization.Serializable{fromConfigUsesCustomObjects(){return!1}getConfig(){return{}}}class ao extends ro{apply(t,n){return e.zeros(t,n)}}ao.className="Zeros",e.serialization.registerClass(ao);class oo extends ro{apply(t,n){return e.ones(t,n)}}oo.className="Ones",e.serialization.registerClass(oo);class lo extends ro{constructor(t){if(super(),"object"!=typeof t)throw new Ur(`Expected argument of type ConstantConfig but got ${t}`);if(void 0===t.value)throw new Ur(`config must have value set but got ${t}`);this.value=t.value}apply(t,n){return e.tidy((()=>e.mul(e.scalar(this.value),e.ones(t,n))))}getConfig(){return{value:this.value}}}lo.className="Constant",e.serialization.registerClass(lo);class uo extends ro{constructor(t){super(),this.DEFAULT_MINVAL=-.05,this.DEFAULT_MAXVAL=.05,this.minval=t.minval||this.DEFAULT_MINVAL,this.maxval=t.maxval||this.DEFAULT_MAXVAL,this.seed=t.seed}apply(t,n){return e.randomUniform(t,this.minval,this.maxval,n)}getConfig(){return{minval:this.minval,maxval:this.maxval,seed:this.seed}}}uo.className="RandomUniform",e.serialization.registerClass(uo);class ho extends ro{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new Kr(`randomNormal does not support dType ${e}.`);return Ja(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}ho.className="RandomNormal",e.serialization.registerClass(ho);class co extends ro{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,n){if("float32"!==(n=n||"float32")&&"int32"!==n)throw new Kr(`truncatedNormal does not support dType ${n}.`);return e.truncatedNormal(t,this.mean,this.stddev,n,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}co.className="TruncatedNormal",e.serialization.registerClass(co);class po extends ro{constructor(t){super(),this.gain=null!=t.gain?t.gain:1}apply(t,n){return e.tidy((()=>{if(2!==t.length||t[0]!==t[1])throw new Ur("Identity matrix initializer can only be used for 2D square matrices.");return e.mul(this.gain,e.eye(t[0]))}))}getConfig(){return{gain:this.gain}}}po.className="Identity",e.serialization.registerClass(po);class fo extends ro{constructor(t){if(super(),t.scale<0)throw new Ur(`scale must be a positive float. Got: ${t.scale}`);var e;this.scale=null==t.scale?1:t.scale,this.mode=null==t.mode?"fanIn":t.mode,e=this.mode,ra(so,"FanMode",e),this.distribution=null==t.distribution?"normal":t.distribution,function(t){ra(io,"Distribution",t)}(this.distribution),this.seed=t.seed}apply(t,n){const s=function(t,e="channelsLast"){let n,s;if(Aa(e),2===t.length)n=t[0],s=t[1];else if(-1!==[3,4,5].indexOf(t.length)){if("channelsFirst"===e){const e=Ra(t,2);n=t[1]*e,s=t[0]*e}else if("channelsLast"===e){const e=Ra(t,0,t.length-2);n=t[t.length-2]*e,s=t[t.length-1]*e}}else{const e=Ra(t);n=Math.sqrt(e),s=Math.sqrt(e)}return[n,s]}(t),i=s[0],r=s[1];let a=this.scale;if("fanIn"===this.mode?a/=Math.max(1,i):"fanOut"===this.mode?a/=Math.max(1,r):a/=Math.max(1,(i+r)/2),"normal"===this.distribution){const s=Math.sqrt(a);if("float32"!==(n=n||"float32")&&"int32"!==n)throw new Kr(`${this.getClassName()} does not support dType ${n}.`);return e.truncatedNormal(t,0,s,n,this.seed)}{const s=Math.sqrt(3*a);return e.randomUniform(t,-s,s,n)}}getConfig(){return{scale:this.scale,mode:this.mode,distribution:this.distribution,seed:this.seed}}}fo.className="VarianceScaling",e.serialization.registerClass(fo);class go extends fo{constructor(t){super({scale:1,mode:"fanAvg",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return fo.className}}go.className="GlorotUniform",e.serialization.registerClass(go);class mo extends fo{constructor(t){super({scale:1,mode:"fanAvg",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return fo.className}}mo.className="GlorotNormal",e.serialization.registerClass(mo);class yo extends fo{constructor(t){super({scale:2,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return fo.className}}yo.className="HeNormal",e.serialization.registerClass(yo);class bo extends fo{constructor(t){super({scale:2,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return fo.className}}bo.className="HeUniform",e.serialization.registerClass(bo);class wo extends fo{constructor(t){super({scale:1,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return fo.className}}wo.className="LeCunNormal",e.serialization.registerClass(wo);class ko extends fo{constructor(t){super({scale:1,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return fo.className}}ko.className="LeCunNormal",e.serialization.registerClass(ko);class xo extends ro{constructor(t){if(super(),this.DEFAULT_GAIN=1,this.gain=null==t.gain?this.DEFAULT_GAIN:t.gain,this.seed=t.seed,null!=this.seed)throw new Kr("Random seed is not implemented for Orthogonal Initializer yet.")}apply(t,n){return e.tidy((()=>{if(t.length<2)throw new Kr("Shape must be at least 2D.");t[0]*t[1]>2e3&&console.warn(`Orthogonal initializer is being called on a matrix with more than 2000 (${t[0]*t[1]}) elements: Slowness may result.`);const n=Ja(t[0]>t[1]?[t[1],t[0]]:t,0,1,"float32");let s=e.linalg.gramSchmidt(n);return t[0]>t[1]&&(s=s.transpose()),e.mul(this.gain,s)}))}getConfig(){return{gain:this.gain,seed:this.seed}}}xo.className="Orthogonal",e.serialization.registerClass(xo);const vo={constant:"Constant",glorotNormal:"GlorotNormal",glorotUniform:"GlorotUniform",heNormal:"HeNormal",heUniform:"HeUniform",identity:"Identity",leCunNormal:"LeCunNormal",leCunUniform:"LeCunUniform",ones:"Ones",orthogonal:"Orthogonal",randomNormal:"RandomNormal",randomUniform:"RandomUniform",truncatedNormal:"TruncatedNormal",varianceScaling:"VarianceScaling",zeros:"Zeros"};function So(t,n={}){return ea(t,e.serialization.SerializationMap.getMap().classNameMap,n,"initializer")}function Io(t){return Qr(t)}function No(t){if("string"==typeof t){const e=t in vo?vo[t]:t;if("GlorotNormal"===e)return new mo;if("GlorotUniform"===e)return new go;if("HeNormal"===e)return new yo;if("HeUniform"===e)return new bo;if("LeCunNormal"===e)return new wo;if("LeCunUniform"===e)return new ko;{const t={};return t.className=e,t.config={},So(t)}}return t instanceof ro?t:So(t)}var zo=Object.freeze({__proto__:null,zeros:function(){return new ao},ones:function(){return new oo},constant:function(t){return new lo(t)},randomUniform:function(t){return new uo(t)},randomNormal:function(t){return new ho(t)},truncatedNormal:function(t){return new co(t)},identity:function(t){return new po(t)},varianceScaling:function(t){return new fo(t)},glorotUniform:function(t){return new go(t)},glorotNormal:function(t){return new mo(t)},heNormal:function(t){return new yo(t)},heUniform:function(t){return new bo(t)},leCunNormal:function(t){return new wo(t)},leCunUniform:function(t){return new ko(t)},orthogonal:function(t){return new xo(t)}});let Ao=0;function Co(){return Ao++}const $o={};function Do(t=""){return t in $o||($o[t]=0),$o[t]+=1,t+$o[t].toString()}function To(t){return Array.isArray(t)&&Array.isArray(t[0])}function Eo(t){return 0===t.length?[]:Array.isArray(t[0])?t:[t]}function Fo(t){let e;if(Array.isArray(t)){if(1!==t.length)throw new Ur(`Expected Tensor length to be 1; got ${t.length}`);e=t[0]}else e=t;return e}function Lo(t){if(Array.isArray(t)&&Array.isArray(t[0])){if(1===t.length)return(t=t)[0];throw new Ur(`Expected exactly 1 Shape; got ${t.length}`)}return t}function _o(t){let e=0;for(const n of t)0===n.shape.length?e+=1:e+=n.shape.reduce(((t,e)=>t*e));return e}const Ro="Variable";class Mo{constructor(t,n="float32",s="Variable",i=!0,r=null){this.dtype=null==n?"float32":n,this.shape=t.shape,this.id=Co(),s=null==s?Ro:s,this.originalName=Ea(s),this.name=Fa(this.originalName),this.trainable_=i,this.constraint=r,this.val=e.variable(t,this.trainable_,this.name,this.dtype)}read(){return this.assertNotDisposed(),this.val}write(t){return this.assertNotDisposed(),function(t,e){if(t.shape.toString()!==e.shape.toString())throw new Error("Shape mismatch: "+JSON.stringify(t.shape)+" vs. "+JSON.stringify(e.shape))}(this.val,t),this.val.id!==t.id&&(this.val.assign(t),null!=this.constraint&&this.val.assign(this.constraint.apply(this.val))),this}dispose(){this.assertNotDisposed(),this.val.dispose()}assertNotDisposed(){if(this.val.isDisposed)throw new Error(`LayersVariable ${this.name} is already disposed.`)}get trainable(){return this.trainable_}set trainable(t){this.trainable_=t,this.val.trainable=t}}function Oo(t){return t.map((t=>t.read()))}function Bo(t){t.forEach((t=>{t[0].write(t[1])}))}class Wo{constructor(t){this.dtype=t.dtype,this.shape=t.shape,null!=t.shape?this.ndim=t.shape.length:this.ndim=t.ndim,this.maxNDim=t.maxNDim,this.minNDim=t.minNDim,this.axes=t.axes||{}}}class Po{constructor(t,e,n,s,i,r,a){this.dtype=t,this.shape=e,this.sourceLayer=n,this.inputs=s,this.callArgs=i,this.outputTensorIndex=a,this.id=Co(),null!=r&&(this.originalName=Ea(r),this.name=Fa(this.originalName)),this.rank=e.length}}let Uo=0;class Ko{constructor(t,e){this.callArgs=e,this.id=Uo++,this.outboundLayer=t.outboundLayer,this.inboundLayers=t.inboundLayers,this.nodeIndices=t.nodeIndices,this.tensorIndices=t.tensorIndices,this.inputTensors=t.inputTensors,this.outputTensors=t.outputTensors,this.inputMasks=t.inputMasks,this.outputMasks=t.outputMasks,this.inputShapes=t.inputShapes,this.outputShapes=t.outputShapes;for(const e of t.inboundLayers)null!=e&&e.outboundNodes.push(this);t.outboundLayer.inboundNodes.push(this)}getConfig(){const t=[];for(const e of this.inboundLayers)null!=e?t.push(e.name):t.push(null);return{outboundLayer:this.outboundLayer?this.outboundLayer.name:null,inboundLayers:t,nodeIndices:this.nodeIndices,tensorIndices:this.tensorIndices}}}let Vo=0;class jo extends e.serialization.Serializable{constructor(t={}){super(),this._callHook=null,this._addedWeightNames=[],this._stateful=!1,this.id=Vo++,this.activityRegularizer=null,this.inputSpec=null,this.supportsMasking=!1,this._trainableWeights=[],this._nonTrainableWeights=[],this._losses=[],this._updates=[],this._built=!1,this.inboundNodes=[],this.outboundNodes=[];let e=t.name;if(!e){const t=this.getClassName();e=Zr(t)+"_"+Do(t)}if(this.name=e,this.trainable_=null==t.trainable||t.trainable,null!=t.inputShape||null!=t.batchInputShape){let e;if(null!=t.batchInputShape)e=t.batchInputShape;else if(null!=t.inputShape){let n=null;null!=t.batchSize&&(n=t.batchSize),e=[n].concat(t.inputShape)}this.batchInputShape=e;let n=t.dtype;null==n&&(n=t.inputDType),null==n&&(n="float32"),this.dtype=n}null!=t.weights?this.initialWeights=t.weights:this.initialWeights=null,this._refCount=null,this.fastWeightInitDuringBuild=!1}static nodeKey(t,e){return t.name+"_ib-"+e.toString()}getNodeAtIndex(t,e){if(0===this.inboundNodes.length)throw new Pr(`The layer has never been called and thus has no defined ${e}.`);if(this.inboundNodes.length<=t)throw new Ur(`Asked to get ${e} at node ${t}, but the layer has only ${this.inboundNodes.length} inbound nodes.`);return this.inboundNodes[t]}getInputAt(t){return Hr(this.getNodeAtIndex(t,"input").inputTensors)}getOutputAt(t){return Hr(this.getNodeAtIndex(t,"output").outputTensors)}get input(){if(this.inboundNodes.length>1)throw new Wr(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer input" is ill-defined. Use \`getInputAt(nodeIndex)\` instead.`);if(0===this.inboundNodes.length)throw new Wr(`Layer ${this.name} is not connected, no input to return.`);return Hr(this.getNodeAtIndex(0,"input").inputTensors)}get output(){if(0===this.inboundNodes.length)throw new Wr(`Layer ${this.name} has no inbound nodes.`);if(this.inboundNodes.length>1)throw new Wr(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use \`getOutputAt(nodeIndex)\` instead.`);return Hr(this.getNodeAtIndex(0,"output").outputTensors)}get losses(){return this._losses}calculateLosses(){return this.losses.map((t=>t()))}get updates(){return this._updates}get built(){return this._built}set built(t){this._built=t}get trainable(){return this.trainable_}set trainable(t){this._trainableWeights.forEach((e=>e.trainable=t)),this.trainable_=t}get trainableWeights(){return this.trainable_?this._trainableWeights.filter((t=>t.trainable)):[]}set trainableWeights(t){this._trainableWeights=t}get nonTrainableWeights(){return this.trainable?this._trainableWeights.filter((t=>!t.trainable)).concat(this._nonTrainableWeights):this._trainableWeights.concat(this._nonTrainableWeights)}set nonTrainableWeights(t){this._nonTrainableWeights=t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}get stateful(){return this._stateful}resetStates(){if(!this.stateful)throw new Error("Cannot call the resetStates() method of a non-stateful Layer object.")}assertInputCompatibility(t){if(t=Jr(t),null==this.inputSpec||0===this.inputSpec.length)return;const e=Jr(this.inputSpec);if(t.length!==e.length)throw new Ur(`Layer ${this.name} expects ${e.length} inputs, but it received ${t.length} input tensors. Input received: ${t}`);for(let n=0;n<t.length;n++){const s=t[n],i=e[n];if(null==i)continue;const r=s.rank;if(null!=i.ndim&&r!==i.ndim)throw new Ur(`Input ${n} is incompatible with layer ${this.name}: expected ndim=${i.ndim}, found ndim=${r}`);if(null!=i.maxNDim&&r>i.maxNDim)throw new Ur(`Input ${n} is incompatible with layer ${this.name}: expected max_ndim=${i.maxNDim}, found ndim=${r}`);if(null!=i.minNDim&&r<i.minNDim)throw new Ur(`Input ${n} is incompatible with layer ${this.name}: expected min_ndim=${i.minNDim}, found ndim=${r}.`);if(null!=i.dtype&&s.dtype!==i.dtype)throw new Ur(`Input ${n} is incompatible with layer ${this.name} : expected dtype=${i.dtype}, found dtype=${s.dtype}.`);if(i.axes){const t=s.shape;for(const e in i.axes){const s=Number(e),r=i.axes[e],a=s>=0?t[s]:t[t.length+s];if(null!=r&&-1===[r,null].indexOf(a))throw new Ur(`Input ${n} is incompatible with layer ${this.name}: expected axis ${s} of input shape to have value ${r} but got shape ${t}.`)}}if(null!=i.shape)for(let t=0;t<i.shape.length;++t){const e=i.shape[t],r=s.shape[t];if(null!=e&&null!=r&&e!==r)throw new Ur(`Input ${n} is incompatible with layer ${this.name}: expected shape=${i.shape}, found shape=${s.shape}.`)}}}call(t,e){return t}invokeCallHook(t,e){null!=this._callHook&&this._callHook(t,e)}setCallHook(t){this._callHook=t}clearCallHook(){this._callHook=null}apply(t,e){e=e||{},this.assertNotDisposed();const n=Jr(t);let s=!0;for(const t of n)if(!(t instanceof Po)){s=!1;break}let i=!0;for(const t of n)if(t instanceof Po){i=!1;break}if(s===i)throw new Ur("Arguments to apply() must be all SymbolicTensors or all Tensors");return Ta(this.name,(()=>{if(!this.built){this.assertInputCompatibility(t);const e=[];for(const n of Jr(t))e.push(n.shape);this.build(Hr(e)),this.built=!0,this.initialWeights&&this.setWeights(this.initialWeights),null===this._refCount&&i&&(this._refCount=1)}if(this.assertInputCompatibility(t),i){let s=this.call(t,e);const i=Jr(s),r=[];for(let t of i)-1!==n.indexOf(t)&&(t=t.clone()),r.push(t);if(s=Hr(r),null!=this.activityRegularizer)throw new Kr("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return s}{const n=function(t){t=Jr(t);const e=[];for(const n of t)e.push(n.shape);return Hr(e)}(t),s=this.computeOutputShape(n);let i;const r="float32";if(this.warnOnIncompatibleInputShape(Array.isArray(t)?n[0]:n),i=null!=s&&s.length>0&&Array.isArray(s[0])?s.map(((n,s)=>new Po(r,n,this,Jr(t),e,this.name,s))):new Po(r,s,this,Jr(t),e,this.name),this.addInboundNode(t,i,null,null,n,s,e),this._refCount++,null!=this.activityRegularizer)throw new Kr("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return i}}))}warnOnIncompatibleInputShape(t){if(null!=this.batchInputShape)if(t.length!==this.batchInputShape.length)console.warn(`The rank of the input tensor provided (shape: ${JSON.stringify(t)}) does not match that of the batchInputShape (${JSON.stringify(this.batchInputShape)}) of the layer ${this.name}`);else{let e=!1;this.batchInputShape.forEach(((n,s)=>{null!=n&&null!=t[s]&&t[s]!==n&&(e=!0)})),e&&console.warn(`The shape of the input tensor (${JSON.stringify(t)}) does not match the expectation of layer ${this.name}: ${JSON.stringify(this.batchInputShape)}`)}}get outputShape(){if(null==this.inboundNodes||0===this.inboundNodes.length)throw new Wr(`The layer ${this.name} has never been called and thus has no defined output shape.`);const t=[];for(const e of this.inboundNodes){const n=JSON.stringify(e.outputShapes);-1===t.indexOf(n)&&t.push(n)}if(1===t.length){const t=this.inboundNodes[0].outputShapes;return Array.isArray(t)&&Array.isArray(t[0])&&1===t.length?t[0]:t}throw new Wr(`The layer ${this.name} has multiple inbound nodes with different output shapes. Hence the notion of "output shape" is ill-defined for the layer.`)}countParams(){if(!this.built)throw new Pr(`You tried to call countParams() on ${this.name}, but the layer is not built yet. Build it first by calling build(batchInputShape).`);return _o(this.weights)}build(t){this.built=!0}getWeights(t=!1){return Oo(t?this.trainableWeights:this.weights)}setWeights(t){e.tidy((()=>{const n=this.weights;if(n.length!==t.length)throw new Ur(`You called setWeights(weights) on layer "${this.name}" with a weight list of length ${t.length}, but the layer was expecting ${n.length} weights. Provided weights: ${t}...`);if(0===n.length)return;const s=[],i=Oo(n);for(let r=0;r<i.length;++r){const a=i[r],o=n[r],l=t[r];if(!e.util.arraysEqual(a.shape,l.shape))throw new Ur(`Layer weight shape ${a.shape} not compatible with provided weight shape ${l.shape}`);s.push([o,l])}Bo(s)}))}addWeight(t,e,n,s,i,r,a){if(-1!==this._addedWeightNames.indexOf(t))throw new Ur(`Duplicate weight name ${t} for layer ${this.name}`);this._addedWeightNames.push(t),null==n&&(n="float32"),this.fastWeightInitDuringBuild&&(s=No("zeros"));const o=s.apply(e,n),l=new Mo(o,n,t,r,a);return o.dispose(),null!=i&&this.addLoss((()=>i.apply(l.read()))),null==r&&(r=!0),r?this._trainableWeights.push(l):this._nonTrainableWeights.push(l),l}setFastWeightInitDuringBuild(t){this.fastWeightInitDuringBuild=t}addLoss(t){null==t||Array.isArray(t)&&0===t.length||(t=Jr(t),void 0!==this._losses&&null!==this._losses&&this.losses.push(...t))}computeOutputShape(t){return t}computeMask(t,e){if(!this.supportsMasking){if(null!=e){if(!Array.isArray(e))throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`);e.forEach((t=>{if(null!=t)throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`)}))}return null}return e}addInboundNode(t,e,n,s,i,r,a=null){const o=Jr(t);e=Jr(e),n=Jr(n),s=Jr(s),i=Eo(i),r=Eo(r);const l=[],u=[],h=[];for(const t of o)l.push(t.sourceLayer),u.push(t.nodeIndex),h.push(t.tensorIndex);new Ko({outboundLayer:this,inboundLayers:l,nodeIndices:u,tensorIndices:h,inputTensors:o,outputTensors:e,inputMasks:n,outputMasks:s,inputShapes:i,outputShapes:r},a);for(let t=0;t<e.length;t++)e[t].sourceLayer=this,e[t].nodeIndex=this.inboundNodes.length-1,e[t].tensorIndex=t}getConfig(){const t={name:this.name,trainable:this.trainable};return null!=this.batchInputShape&&(t.batchInputShape=this.batchInputShape),null!=this.dtype&&(t.dtype=this.dtype),t}disposeWeights(){return this.weights.forEach((t=>t.dispose())),this.weights.length}assertNotDisposed(){if(0===this._refCount)throw new Error(`Layer '${this.name}' is already disposed.`)}dispose(){if(!this.built)throw new Error(`Cannot dispose Layer ${this.name} because it has not been built yet.`);if(null===this._refCount)throw new Error(`Cannot dispose Layer ${this.name} because it has not been used yet.`);this.assertNotDisposed();let t=0;return 0==--this._refCount&&(t=this.disposeWeights()),{refCountAfterDispose:this._refCount,numDisposedVariables:t}}}function qo(t,e,n){if((null==e||null!=n&&n>0)&&(e=t.sourceLayer,n=t.nodeIndex),0===e.inboundNodes.length)return[t];{const t=e.inboundNodes[n];if(0===t.inboundLayers.length)return t.inputTensors;{const e=[];for(let n=0;n<t.inboundLayers.length;n++){const s=qo(t.inputTensors[n],t.inboundLayers[n],t.nodeIndices[n]);for(const t of s)-1===e.indexOf(t)&&e.push(t)}return e}}}class Go extends jo{constructor(t){if(super({dtype:t.dtype,name:null!=t.name?t.name:Do("input").toString()}),null==t.batchSize&&(t.batchSize=null),null==t.sparse&&(t.sparse=!1),this.trainable=!1,this.built=!0,this.sparse=t.sparse,null!=t.inputShape&&null!=t.batchInputShape)throw new Ur("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");let e=t.batchInputShape;if(null==e){if(null==t.inputShape)throw new Ur("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");e=[t.batchSize].concat(t.inputShape)}else if(null!=t.batchSize)throw new Ur("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");const n=t.dtype||"float32";this.batchInputShape=e,this.dtype=n,this.inputSpec=[{shape:e}];const s=new Po(this.dtype,this.batchInputShape,this,[],{},this.name);s.nodeIndex=0,s.tensorIndex=0,new Ko({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:[s],outputTensors:[s],inputMasks:[null],outputMasks:[null],inputShapes:[e],outputShapes:[e]})}apply(t,e){throw new Ur(`Cannot pass any input to an InputLayer's apply() method. InputLayer name: ${this.name}`)}dispose(){return{refCountAfterDispose:this._refCount,numDisposedVariables:0}}getConfig(){return{batchInputShape:this.batchInputShape,dtype:this.dtype,sparse:this.sparse,name:this.name}}}function Ho(t){if(null==t.batchShape&&null==t.shape)throw new Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");if(null!=t.batchShape&&null!=t.shape)throw new Ur("Please provide either a `shape` or `batchShape` argument to Input, but not both.");let e=t.batchShape;null!=t.shape&&null==e&&(e=[null].concat(t.shape));let n=t.dtype;null==n&&(n="float32");return new Go({batchInputShape:e,name:t.name,dtype:n,sparse:t.sparse}).inboundNodes[0].outputTensors[0]}async function Jo(t){if(null==t)return;const n=[],s=[],i=[];for(const e in t){const r=t[e];if("number"!=typeof r){const t=r;n.push(t.data()),s.push(e),i.push(t)}}if(n.length>0){const r=await Promise.all(n);for(let e=0;e<r.length;++e)t[s[e]]=r[e][0];e.dispose(i)}}function Zo(t){if(null!=t)for(const e in t){const n=t[e];"number"!=typeof n&&n.dispose()}}var Xo;Go.className="InputLayer",e.serialization.registerClass(Go),function(t){t[t.SILENT=0]="SILENT",t[t.VERBOSE=1]="VERBOSE"}(Xo||(Xo={}));class Yo{constructor(){this.validationData=null}setParams(t){this.params=t}async onEpochBegin(t,e){}async onEpochEnd(t,e){}async onBatchBegin(t,e){}async onBatchEnd(t,e){}async onTrainBegin(t){}async onTrainEnd(t){}setModel(t){}}class Qo{constructor(t,e=10){null==t&&(t=[]),this.callbacks=t,this.queueLength=e}append(t){this.callbacks.push(t)}setParams(t){for(const e of this.callbacks)e.setParams(t)}setModel(t){for(const e of this.callbacks)e.setModel(t)}async onEpochBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochBegin(t,e)}async onEpochEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochEnd(t,e)}async onBatchBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchBegin(t,e)}async onBatchEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchEnd(t,e)}async onTrainBegin(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainBegin(t)}async onTrainEnd(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainEnd(t)}}class tl extends Yo{constructor(){super()}async onEpochBegin(t){this.seen=0,this.totals={}}async onBatchEnd(t,n){null==n&&(n={});const s=null==n.size?0:n.size;this.seen+=s;for(const t in n){const i=n[t];if("number"==typeof i)this.totals.hasOwnProperty(t)||(this.totals[t]=0),this.totals[t]=this.totals[t]+i*s;else{let n;t in this.totals?n=this.totals[t]:this.totals[t]=0;const r=e.tidy((()=>e.add(this.totals[t],e.mul(i,s))));this.totals[t]=r,null!=n&&n.dispose()}}}async onEpochEnd(t,n){if(null!=n)for(const t of this.params.metrics)null!=this.totals[t]&&("number"==typeof this.totals[t]?n[t]=this.totals[t]/this.seen:e.tidy((()=>{const s=e.mul(e.div(1,this.seen),this.totals[t]);n[t]=s,this.totals[t].dispose(),e.keep(n[t])})))}}class el extends Yo{async onTrainBegin(t){this.epoch=[],this.history={}}async onEpochEnd(t,e){null==e&&(e={}),this.epoch.push(t);for(const t in e)null==this.history[t]&&(this.history[t]=[]),this.history[t].push(e[t])}async syncData(){const t=[],e=[],n=[];for(const s in this.history){const i=this.history[s];for(let r=0;r<i.length;++r)if("number"!=typeof i[r]){const a=i[r];t.push(a.data()),e.push(s),n.push(r)}}const s=await Promise.all(t);for(let t=0;t<s.length;++t){this.history[e[t]][n[t]].dispose(),this.history[e[t]][n[t]]=s[t][0]}}}class nl extends Yo{constructor(t,n){if(super(),this.currentEpoch=0,this.yieldEvery=n||"auto","auto"===this.yieldEvery&&(this.yieldEvery=125),"never"===this.yieldEvery&&null!=t.onYield)throw new Error("yieldEvery is `never` but you provided an `onYield` callback. Either change `yieldEvery` or remove the callback");e.util.isNumber(this.yieldEvery)&&(this.maybeWait=function(t,n){let s,i=e.util.now();return(...r)=>{const a=e.util.now();return a-i<n||(i=a,s=t(...r)),s}}(this.maybeWait.bind(this),this.yieldEvery)),this.trainBegin=t.onTrainBegin,this.trainEnd=t.onTrainEnd,this.epochBegin=t.onEpochBegin,this.epochEnd=t.onEpochEnd,this.batchBegin=t.onBatchBegin,this.batchEnd=t.onBatchEnd,this.yield=t.onYield}async maybeWait(t,n,s){const i=[];null!=this.yield&&(await Jo(s),i.push(this.yield(t,n,s))),i.push(e.nextFrame()),await Promise.all(i)}async onEpochBegin(t,e){this.currentEpoch=t,null!=this.epochBegin&&(await Jo(e),await this.epochBegin(t,e))}async onEpochEnd(t,n){const s=[];null!=this.epochEnd&&(await Jo(n),s.push(this.epochEnd(t,n))),"epoch"===this.yieldEvery&&s.push(e.nextFrame()),await Promise.all(s)}async onBatchBegin(t,e){null!=this.batchBegin&&(await Jo(e),await this.batchBegin(t,e))}async onBatchEnd(t,n){const s=[];null!=this.batchEnd&&(await Jo(n),s.push(this.batchEnd(t,n))),"batch"===this.yieldEvery?s.push(e.nextFrame()):e.util.isNumber(this.yieldEvery)&&s.push(this.maybeWait(this.currentEpoch,t,n)),await Promise.all(s)}async onTrainBegin(t){null!=this.trainBegin&&(await Jo(t),await this.trainBegin(t))}async onTrainEnd(t){null!=this.trainEnd&&(await Jo(t),await this.trainEnd(t))}}function sl(t,e){if(null==t&&(t={}),t instanceof Yo)return[t];if(Array.isArray(t)&&t[0]instanceof Yo)return t;return Jr(t).map((t=>new nl(t,e)))}class il{constructor(){}static registerCallbackConstructor(t,n){e.util.assert(t>=0&&Number.isInteger(t),(()=>`Verbosity level is expected to be an integer >= 0, but got ${t}`)),il.checkForDuplicate(n),null==il.constructors[t]&&(il.constructors[t]=[]),il.constructors[t].push(n)}static checkForDuplicate(t){for(const e in il.constructors){il.constructors[+e].forEach((e=>{if(e===t)throw new Ur("Duplicate callback constructor.")}))}}static clear(){il.constructors={}}static createCallbacks(t){const e=[];for(const n in il.constructors){const s=+n;t>=s&&e.push(...il.constructors[s])}return e.map((t=>new t))}}function rl(t,e,n,s,i,r,a,o,l){const u=new el,h=[new tl,...il.createCallbacks(e)];null!=t&&h.push(...t),h.push(u);const c=new Qo(h);return c.setParams({epochs:n,initialEpoch:s,samples:i,steps:r,batchSize:a,verbose:e,doValidation:o,metrics:l}),{callbackList:c,history:u}}function al(t,n={},s=!1){return ea(t,e.serialization.SerializationMap.getMap().classNameMap,n,"layer",s)}function ol(t,n){return e.tidy((()=>{"float32"!==t.dtype&&(t=t.asType("float32"));const s=e.sum(Ya(t),n,!0),i=e.fill(s.shape,Br()),r=e.sqrt(e.maximum(s,i));return e.div(t,r)}))}function ll(t,n){return e.tidy((()=>e.mean(Ya(e.sub(n,t)),-1)))}function ul(t,n){return e.tidy((()=>e.mean(e.abs(e.sub(n,t)),-1)))}function hl(t,n){return e.tidy((()=>{const s=e.sub(t,n),i=e.clipByValue(e.abs(t),Br(),Number.MAX_VALUE),r=e.abs(e.div(s,i));return e.mul(100,e.mean(r,-1))}))}function cl(t,n,s=!1){return e.tidy((()=>{if(s)n=e.softmax(n);else{const t=e.sum(n,n.shape.length-1,!0);n=e.div(n,t)}return n=e.clipByValue(n,Br(),1-Br()),e.neg(e.sum(e.mul(t.toFloat(),e.log(n)),n.shape.length-1))}))}function pl(t,n,s=!1){return e.tidy((()=>{const i=e.floor(function(t){const e=[Ra(t.shape)];return t.reshape(e)}(t)).toInt(),r=(n=e.clipByValue(n,Br(),1-Br())).shape;return cl(e.oneHot(i,r[r.length-1]).reshape(r),n,s)}))}function dl(t,n){return e.tidy((()=>{let s;return s=e.clipByValue(n,Br(),1-Br()),s=e.log(e.div(s,e.sub(1,s))),e.mean(function(t,n){if(!e.util.arraysEqual(t.shape,n.shape))throw new Ur(`logits and labels must have the same shape, but got shapes ${JSON.stringify(t.shape)} and ${JSON.stringify(n.shape)}`);return e.tidy((()=>{const e=n.relu(),s=n.abs().neg();return e.sub(n.mul(t)).add(s.exp().log1p())}))}(t,s),-1)}))}function fl(t,n){return e.tidy((()=>{const s=ol(t,-1),i=ol(n,-1),r=e.mul(s,i);return e.neg(e.sum(r,-1))}))}il.constructors={};const gl={meanSquaredError:ll,meanAbsoluteError:ul,meanAbsolutePercentageError:hl,meanSquaredLogarithmicError:function(t,n){return e.tidy((()=>{const s=e.clipByValue(n,Br(),Number.MAX_VALUE),i=e.log(e.add(1,s)),r=e.clipByValue(t,Br(),Number.MAX_VALUE),a=e.log(e.add(1,r));return e.mean(Ya(e.sub(i,a)),-1)}))},squaredHinge:function(t,n){return e.tidy((()=>{const s=e.maximum(0,e.sub(1,e.mul(t,n)));return e.mean(Ya(s),-1)}))},hinge:function(t,n){return e.tidy((()=>{const s=e.maximum(0,e.sub(1,e.mul(t,n)));return e.mean(s,-1)}))},categoricalHinge:function(t,n){return e.tidy((()=>{const s=e.sum(e.mul(t,n),-1),i=e.max(e.mul(e.sub(1,t),n),-1);return e.maximum(0,e.add(1,e.sub(i,s)))}))},logcosh:function(t,n){return e.tidy((()=>{const s=Math.log(2),i=e.sub(n,t),r=e.sub(e.add(i,e.softplus(e.mul(-2,i))),s);return e.mean(r,-1)}))},categoricalCrossentropy:cl,sparseCategoricalCrossentropy:pl,binaryCrossentropy:dl,kullbackLeiblerDivergence:function(t,n){return e.tidy((()=>{const s=e.clipByValue(t,Br(),1),i=e.clipByValue(n,Br(),1);return e.sum(e.mul(t,e.log(e.div(s,i))),-1)}))},poisson:function(t,n){return e.tidy((()=>{const s=e.log(e.add(Br(),n));return e.mean(e.sub(n,e.mul(t,s)),-1)}))},cosineProximity:fl};function ml(t){if("string"==typeof t){if(t in gl)return gl[t];let e=`Unknown loss ${t}`;throw t.toLowerCase().includes("softmaxcrossentropy")&&(e=`Unknown loss ${t}. Use "categoricalCrossentropy" as the string name for tf.losses.softmaxCrossEntropy`),new Ur(e)}return t}function yl(t,n){return e.tidy((()=>{const s=e.mul(.5,e.onesLike(n)),i=Pa(e.greater(n,s),t.dtype);return e.mean(e.equal(t,i),-1)}))}function bl(t,n){return e.tidy((()=>Pa(e.equal(e.argMax(t,-1),e.argMax(n,-1)),"float32")))}function wl(t,n){return e.tidy((()=>e.logicalAnd(t.equal(1),n.equal(1)).sum().cast("float32")))}function kl(t,n){return e.tidy((()=>{const s=wl(t,n),i=function(t,n){return e.tidy((()=>e.logicalAnd(t.equal(0),n.equal(1)).sum().cast("float32")))}(t,n),r=s.add(i);return e.where(e.greater(r,0),s.div(r),0).cast("float32")}))}function xl(t,n){return e.tidy((()=>{const s=wl(t,n),i=function(t,n){return e.tidy((()=>e.logicalAnd(t.equal(1),n.equal(0)).sum().cast("float32")))}(t,n),r=s.add(i);return e.where(e.greater(r,0),s.div(r),0).cast("float32")}))}function vl(t,e){return dl(t,e)}function Sl(t,n){return t.rank===n.rank&&(t=t.squeeze([t.rank-1])),(n=n.argMax(-1)).dtype!==t.dtype&&(n=n.asType(t.dtype)),e.equal(t,n).asType("float32")}const Il=cl,Nl=pl,zl={binaryAccuracy:yl,categoricalAccuracy:bl,precision:kl,categoricalCrossentropy:Il,sparseCategoricalCrossentropy:Nl,mse:ll,MSE:ll,mae:ul,MAE:ul,mape:hl,MAPE:hl,cosine:fl};function Al(t){if("string"==typeof t&&t in zl)return zl[t];if("string"!=typeof t&&null!=t)return t;throw new Ur(`Unknown metric ${t}`)}function Cl(t){if(qr(null!==t,`Unknown LossOrMetricFn ${t}`),"string"==typeof t)return t;{let e;for(const n of Object.keys(gl))if(gl[n]===t){e=n;break}if(void 0!==e)return e;for(const n of Object.keys(zl))if(zl[n]===t){e=n;break}return void 0!==e?e:t.name}}const $l=1048576;function Dl(t,e,n=!1){if(null==t||"object"!=typeof t||Object.getPrototypeOf(t)!==Object.prototype||!Tl(t))throw new Error("User-defined metadata is expected to be a JSON object, but is not.");if(n){const n=JSON.stringify(t);n.length>$l&&console.warn(`User-defined metadata of model "${e}" is too large in size (length=${n.length} when serialized). It is not recommended to store such large objects in user-defined metadata. Please make sure its serialized length is <= 1048576.`)}}function Tl(t){if(null===t)return!0;if("object"==typeof t){if(Object.getPrototypeOf(t)===Object.prototype){const e=Object.keys(t);for(const n of e){if("string"!=typeof n)return!1;if(!Tl(t[n]))return!1}return!0}if(Array.isArray(t)){for(const e of t)if(!Tl(e))return!1;return!0}return!1}{const e=typeof t;return"string"===e||"number"===e||"boolean"===e}}function El(t,e,n,s=console.log){const i=function(t){let e=!0;const n=[],s=[];for(const e in t.nodesByDepth)n.push(t.nodesByDepth[e]);for(const t of n){if(t.length>1||1===t.length&&t[0].inboundLayers.length>1){e=!1;break}s.push(...t)}if(e)for(const n of t.layers){let t=!1;for(const i of n.inboundNodes)if(-1!==s.indexOf(i)){if(t){e=!1;break}t=!0}if(!e)break}return e}(t),r=["Layer (type)","Output shape","Param #"];let a;if(i?(e=e||65,n=n||[.45,.85,1]):(e=e||98,n=n||[.33,.55,.67,1]),n[n.length-1]<=1&&(n=n.map((t=>Math.floor(e*t)))),!i){r.push("Receives inputs"),a=[];for(const e in t.nodesByDepth)a.push(...t.nodesByDepth[e])}s("_".repeat(e)),Fl(r,n,s),s("=".repeat(e));const o=t.layers;for(let t=0;t<o.length;++t)i?Ll(o[t],n,s):_l(o[t],n,a,s),s((t===o.length-1?"=":"_").repeat(e));t.checkTrainableWeightsConsistency();const l=function(t){let e;e=null!=t.collectedTrainableWeights?_o(t.collectedTrainableWeights):_o(t.trainableWeights);return e}(t),u=_o(t.nonTrainableWeights);s(`Total params: ${l+u}`),s(`Trainable params: ${l}`),s(`Non-trainable params: ${u}`),s("_".repeat(e))}function Fl(t,e,n=console.log){let s="";for(let n=0;n<t.length;++n)n>0&&(s=s.slice(0,s.length-1)+" "),s+=t[n],s=s.slice(0,e[n]),s+=" ".repeat(e[n]-s.length);n(s)}function Ll(t,e,n){let s;try{s=JSON.stringify(t.outputShape)}catch(t){s="multiple"}Fl([`${t.name} (${t.getClassName()})`,s,t.countParams().toString()],e,n)}function _l(t,e,n,s){let i;try{i=JSON.stringify(t.outputShape)}catch(t){i="multiple"}const r=[];for(const e of t.inboundNodes)if(!(null!=n&&n.length>0&&-1===n.indexOf(e)))for(let t=0;t<e.inboundLayers.length;++t){const n=e.inboundLayers[t].name,s=e.nodeIndices[t],i=e.tensorIndices[t];r.push(`${n}[${s}][${i}]`)}const a=t.name,o=t.getClassName(),l=0===r.length?"":r[0];Fl([`${a} (${o})`,i,t.countParams().toString(),l],e,s);for(let t=1;t<r.length;++t)Fl(["","","",r[t]],e,s)}function Rl(t,e,n){return("inboundNodes"===t||"outputLayers"===t||"inputLayers"===t)&&0===e&&"string"==typeof n}function Ml(t,e){if(null===t)return null;if("string"==typeof t)return Xr(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const n=[],s=t.length;for(let i=0;i<s;++i){const s=t[i];Rl(e,i,s)?n.push(s):n.push(Ml(s,e))}return n}{const e={};for(const n of Object.keys(t)){const s=t[n];if("name"===n&&"string"==typeof s)e[n]=s;else{const t=Xr(n);e[t]=Ml(s,t)}}return e}}function Ol(t,e){if(null==t)return null;if("string"==typeof t)return Zr(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const n=[],s=t.length;for(let i=0;i<s;++i){const s=t[i];Rl(e,i,s)?n.push(s):n.push(Ol(s,e))}return n}{const e={};for(const n of Object.keys(t)){const s=t[n],i=Zr(n);e[i]="name"!==n&&"className"!==n||"string"!=typeof s?Ol(s,n):s}return e}}const Bl="3.6.0";class Wl{constructor(t){if(this.id2Value={},this.id2Mask={},this.name2Id={},t instanceof Wl)for(const e in t.id2Value)this.id2Value[e]=t.id2Value[e],e in t.id2Mask&&(this.id2Mask[e]=t.id2Mask[e]);else{if(null==t)return;for(const e of t)this.add(e.key,e.value)}}add(t,n,s){if(null!=this.id2Value[t.id])throw new Ur(`Duplicate key: name=${t.name}, id=${t.id}`);return this.id2Value[t.id]=function(t,n){if(null==t.dtype||t.dtype===n.dtype)return n;try{return e.cast(n,t.dtype)}catch(e){throw new Ur(`The dtype of the feed (${n.dtype}) can not be cast to the dtype of the key '${t.name}' (${t.dtype}).`)}}(t,n),this.name2Id[t.name]=t.id,null!=s&&(this.id2Mask[t.id]=s),this}addFeed(t){this.add(t.key,t.value)}hasKey(t){return null!=this.id2Value[t.id]}names(){return Object.keys(this.name2Id)}getValue(t){if(t instanceof Po){if(null==this.id2Value[t.id])throw new Ur(`Nonexistent key: ${t.name}`);return this.id2Value[t.id]}{const e=this.name2Id[t];if(null==e)throw new Ur(`Feed dict has no SymbolicTensor name: ${t}`);return this.id2Value[e]}}getMask(t){if(t instanceof Po){if(null==this.id2Value[t.id])throw new Ur(`Nonexistent key: ${t.name}`);return this.id2Mask[t.id]}{const e=this.name2Id[t];if(null==e)throw new Ur(`Feed dict has no SymbolicTensor name: ${t}`);return this.id2Mask[e]}}disposeMasks(){null!=this.id2Mask&&e.dispose(this.id2Mask)}}const Pl={},Ul={};function Kl(t,n,s,i){const r=null!=s&&s.training,a=Array.isArray(t),o=a?t:[t],l=o.map((t=>t.name)),u=[],h=n.names();for(const t of l)-1!==h.indexOf(t)?u.push(n.getValue(t)):u.push(null);null!=i&&(i.maxNumTensors=-1/0,i.minNumTensors=1/0);const c=l.join(",")+"|"+n.names().join(",");let p,d;if(null==Pl[c]){const t=function(t,n){e.util.assert(null!=t&&t.length>0,(()=>"Expected at least one fetch, got none"));let s=[],i={};if(1===t.length){const e=jl(t[0],n);s=e.sorted,i=e.recipientMap}else{const e=new Set;for(const r of t){const{sorted:t,recipientMap:a}=jl(r,n);for(const n of t)e.has(n.name)||(s.push(n),e.add(n.name));for(const t in a)null==i[t]&&(i[t]=new Set),a[t].forEach((e=>i[t].add(e)))}}return{sorted:s,recipientCounts:Vl(i)}}(o,n);p=t.sorted,d=t.recipientCounts,Pl[c]=p,Ul[c]=d}p=Pl[c],d={},r||Object.assign(d,Ul[c]);const f=new Wl(n);for(let t=0;t<p.length;++t){if(null!=i){const t=e.memory().numTensors;t>i.maxNumTensors&&(i.maxNumTensors=t),t<i.minNumTensors&&(i.minNumTensors=t)}const a=p[t],o=a.sourceLayer;if(o instanceof Go)continue;const h=[],c=[],g=[];let m=!1;for(const t of a.inputs){const e=f.getValue(t),s=f.getMask(t);h.push(e),c.push(s),null!=s&&(m=!0),r||(d[t.name]--,0!==d[t.name]||n.hasKey(t)||-1!==l.indexOf(t.name)||e.isDisposed||!0===t.sourceLayer.stateful||g.push(e))}m&&((s=s||{}).mask=c[0]);const y=Jr(o.apply(h,s));let b=null;o.supportsMasking&&(b=o.computeMask(h,c));const w=ql(a),k=Array.isArray(w)?w:[w];for(let t=0;t<k.length;++t){f.hasKey(k[t])||f.add(k[t],y[t],Array.isArray(b)?b[0]:b);const e=l.indexOf(k[t].name);-1!==e&&(u[e]=y[t])}r||e.dispose(g)}return f.disposeMasks(),a?u:u[0]}function Vl(t){const e={};for(const n in t)e[n]=t[n].size;return e}function jl(t,e){const n=new Set,s=[],i={};for(const t of e.names())n.add(t);const r=[],a=[];for(r.push(t);r.length>0;){const t=r[r.length-1];if(n.has(t.name)){r.pop();continue}const e=a[a.length-1]===r.length-1;if(0===t.inputs.length||e)r.pop(),s.push(t),n.add(t.name),e&&a.pop();else{a.push(r.length-1);for(const e of t.inputs)null==i[e.name]&&(i[e.name]=new Set),i[e.name].add(t.name),n.has(e.name)||r.push(e)}}return{sorted:s,recipientMap:i}}function ql(t){let e;if(1===t.sourceLayer.inboundNodes.length)e=t.sourceLayer.output;else{let n=null;for(let e=0;e<t.sourceLayer.inboundNodes.length;++e)for(const s of t.sourceLayer.inboundNodes[e].outputTensors)if(s.id===t.id){n=e;break}e=t.sourceLayer.getOutputAt(n)}return e}class Gl extends jo{constructor(t){if(super({}),this.containerNodes=new Set,this.name=t.name,null==this.name){const t=this.getClassName().toLowerCase();this.name=Do(t)}if(this.supportsMasking=!1,this.trainable_=!0,Array.isArray(t.inputs)?this.inputs=t.inputs.slice():this.inputs=[t.inputs],Array.isArray(t.outputs)?this.outputs=t.outputs.slice():this.outputs=[t.outputs],sa(this.inputs).length!==this.inputs.length)throw new Ur(`The list of inputs passed to the model is redundant. All inputs should only appear once. Found: ${this.inputs.map((t=>t.name))}`);sa(this.outputs).length!==this.outputs.length&&console.warn(`The list of outputs passed to the model is redundant. All outputs should only appear once. Found: ${this.outputs.map((t=>t.name))}`),this.inputLayers=[],this.inputLayersNodeIndices=[],this.inputLayersTensorIndices=[],this.outputLayers=[],this.outputLayersNodeIndices=[],this.outputLayersTensorIndices=[],this.layers=[],this.internalContainerRefs=[];for(const t of this.outputs){const e=t.sourceLayer,n=t.nodeIndex,s=t.tensorIndex;this.outputLayers.push(e),this.outputLayersNodeIndices.push(n),this.outputLayersTensorIndices.push(s)}for(const t of this.inputs){const e=t.sourceLayer,n=t.nodeIndex,s=t.tensorIndex;qr(0===n,"input layer has >1 nodes"),qr(0===s,"input layer has >1 tensors"),this.inputLayers.push(e),this.inputLayersNodeIndices.push(n),this.inputLayersTensorIndices.push(s)}this.inputNames=[],this.outputNames=[],this.feedInputShapes=[],this.feedInputNames=[],this.feedOutputNames=[];for(let e=0;e<this.inputLayers.length;e++){const n=this.inputLayers[e];if(!(n instanceof Go))throw new TypeError(`Input layers to a LayersModel must be InputLayer objects. Received inputs: ${t.inputs}. Input ${e} (0-based) originates from layer type ${n.getClassName()}.`);this.inputNames.push(n.name),this.feedInputShapes.push(n.batchInputShape),this.feedInputNames.push(n.name)}for(const t of this.outputLayers)this.outputNames.push(t.name);this.internalInputShapes=this.inputs.map((t=>t.shape)),this.internalOutputShapes=this.outputs.map((t=>t.shape));const e={},n={},s={},i={},r={},a=[],o=(t,e,n,s,i,l)=>{null!=s&&null!=i&&null!=l||(s=t.sourceLayer,i=t.nodeIndex,l=t.tensorIndex);const u=s.inboundNodes[i];if(-1!==n.indexOf(u))throw new Pr(`The tensor ${t.name} at layer "${s.name}" is part of a cycle.`);if(-1!==e.indexOf(u))return;this.containerNodes.add(Gl.nodeKey(s,i)),s.id in r||(r[s.id]=Object.keys(r).length),-1===n.indexOf(u)&&n.push(u);const h=u.inboundLayers.length;for(let t=0;t<h;t++){const s=u.inputTensors[t],i=u.inboundLayers[t],r=u.nodeIndices[t],a=u.tensorIndices[t];o(s,e,n,i,r,a)}for(e.push(u);n.indexOf(u)>=0;)n.splice(n.indexOf(u),1);a.push(u)},l=[],u=[];for(const t of this.outputs)o(t,l,u);const h=a.slice().reverse();for(const t of h){n[t.id]=t,t.id in e||(e[t.id]=0);let r=e[t.id];const a=null==s[t.outboundLayer.id]?0:s[t.outboundLayer.id];r=Math.max(r,a),s[t.outboundLayer.id]=r,i[t.outboundLayer.id]=t.outboundLayer,e[t.id]=r;for(let s=0;s<t.inboundLayers.length;s++){const i=t.inboundLayers[s],a=t.nodeIndices[s],o=i.inboundNodes[a],l=null==e[o.id]?0:e[o.id];e[o.id]=Math.max(r+1,l),n[o.id]=o}}const c={};for(const t in e){const s=e[t];s in c||(c[s]=[]),c[s].push(n[t])}const p={};for(const t in s){const e=s[t];e in p||(p[e]=[]),p[e].push(i[t])}let d=Object.keys(p).map((t=>parseInt(t,10))).sort(na);this.layers=[];for(const t of d){const e=p[t];e.sort(((t,e)=>{const n=r[t.id],s=r[e.id];return n<s?-1:n>s?1:0}));for(const t of e)t instanceof Gl&&this.internalContainerRefs.push(t),this.layers.push(t)}this.layersByDepth=p,d=Object.keys(c).map((t=>parseInt(t,10))).sort(na);const f=this.inputs.slice(),g=[];for(const t of d)for(const e of c[t]){const t=e.outboundLayer;if(null!=t){for(const n of e.inputTensors)if(-1===f.indexOf(n))throw new Pr(`Graph disconnected: cannot obtain value for tensor ${n} at layer "${t.name}". The following previous layers were accessed without issue: ${g}`);for(const t of e.outputTensors)f.push(t);g.push(t.name)}}this.nodesByDepth=c;const m=this.layers.map((t=>t.name));for(const t of m){const e=m.filter((e=>e===t)).length;if(1!==e)throw new Pr(`The name "${t}" is used ${e} times in the model. All layer names should be unique. Layer names: `+JSON.stringify(m))}this.outboundNodes=[],this.inboundNodes=[],new Ko({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:this.inputs.map((t=>null)),outputMasks:this.outputs.map((t=>null)),inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs.map((t=>t.shape))}),this.built=!0,this._refCount=1}assertNotDisposed(){if(0===this._refCount)throw new Error(`Container '${this.name}' is already disposed.`)}dispose(){this.assertNotDisposed();const t={refCountAfterDispose:null,numDisposedVariables:0};if(0==--this._refCount){for(const e of this.layers)t.numDisposedVariables+=e.dispose().numDisposedVariables;for(const e of this.internalContainerRefs)t.numDisposedVariables+=e.dispose().numDisposedVariables}return t.refCountAfterDispose=this._refCount,t}get trainable(){return this.trainable_}set trainable(t){this.layers.forEach((e=>{e._trainableWeights.forEach((e=>e.trainable=t))})),this.trainable_=t}get trainableWeights(){if(this._trainableWeights.length>0)throw new Ur("Container instance unexpectedly contains _trainableWeights.The trainable weights of a Container are a union of the trainable weights of its consituent Layers. Its own _trainableWeights must remain an empty Array.");if(!this.trainable)return[];let t=[];for(const e of this.layers)t=t.concat(e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.layers)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.layers)e.push(...t.trainableWeights);return e.concat(t)}return t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}loadWeights(t,e=!0){const n={};let s=0;for(const t of this.layers)for(const e of t.weights){if(null!=n[e.originalName])throw new Ur(`Duplicate weight name: ${e.originalName}`);n[e.originalName]=e,s++}const i=[];for(const s in t){let r=s;if(null==n[s]){const t=s.split("/");r=t.slice(0,-2).concat([t[t.length-1]]).join("/")}if(null!=n[r])i.push([n[r],t[s]]);else if(e)throw new Ur(`Provided weight data has no target variable: ${s}`);delete n[r]}if(e){const t=[];for(const e in n)t.push(e);if(t.length>0)throw new Ur(`${t.length} of ${s} weights are not set: ${t}`)}Bo(i)}updatedConfig(){const t=this.getConfig(),e={};return e.className=this.getClassName(),e.config=t,e.kerasVersion="tfjs-layers 3.6.0",e.backend="TensorFlow.js",e}toJSON(t,e=!0){const n=Ol(this.updatedConfig());return e?JSON.stringify(n):n}call(t,n){return e.tidy((()=>{t=Jr(t);const e=new Wl;for(let n=0;n<this.inputs.length;++n)e.add(this.inputs[n],t[n]);return Kl(this.outputs,e,n)}))}computeMask(t,n){return e.tidy((()=>{let e;return t=Jr(t),e=null==n?jr(null,t.length):Jr(n),this.runInternalGraph(t,e)[1]}))}computeOutputShape(t){const e=Eo(t);if(e.length!==this.inputLayers.length)throw new Ur(`Invalid inputShape argument ${t}: model has ${this.inputLayers.length} tensor inputs.`);const n={};for(let t=0;t<e.length;t++){const s=this.inputLayers[t],i=e[t];n[s.name+"_0_0"]=i}const s=Object.keys(this.nodesByDepth).map((t=>parseInt(t,10))).sort(na);if(s.length>1)for(const t of s){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer;if(-1!==this.inputLayers.map((t=>t.id)).indexOf(e.id))continue;const s=[];for(let e=0;e<t.inboundLayers.length;e++){const i=t.inboundLayers[e],r=t.nodeIndices[e],a=t.tensorIndices[e],o=n[`${i.name}_${r}_${a}`];s.push(o)}const i=Eo(e.computeOutputShape(Hr(s))),r=e.inboundNodes.indexOf(t);for(let t=0;t<i.length;t++){n[`${e.name}_${r}_${t}`]=i[t]}}}const i=[],r=[];for(let t=0;t<this.outputLayers.length;t++){const e=this.outputLayers[t],n=this.outputLayersNodeIndices[t],s=this.outputLayersTensorIndices[t],i=`${e.name}_${n}_${s}`;r.push(i)}for(let t=0;t<r.length;t++){const e=r[t];qr(e in n),i.push(n[e])}return Hr(i)}runInternalGraph(t,e){null==e&&(e=jr(null,t.length));const n={};for(let s=0;s<this.inputs.length;++s){const i=this.inputs[s],r=t[s],a=e[s];n[i.id]=[r,a]}const s=Object.keys(this.nodesByDepth).map((t=>parseInt(t,10))).sort(na);for(const t of s){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer,s=t.inputTensors,i=t.outputTensors,r=new Array;for(const t of s)t.id in n&&r.push(n[t.id]);if(r.length===s.length){let s,a,o,l,u={};if(null!=t.callArgs&&(u=t.callArgs),1===r.length){const[t,n]=r[0];null==u.mask&&(u.mask=n),o=Jr(e.call(t,u)),l=Jr(e.computeMask(t,n)),s=[t],a=[n]}else s=r.map((t=>t[0])),a=r.map((t=>t[1])),null==u.mask&&(u.mask=a),o=Jr(e.call(s,u)),l=Jr(e.computeMask(s,a));if(e.activityRegularizer)throw new Kr("LayersModel invocation with concrete Tensor value(s) in the presence of activity regularizer(s) is not supported yet.");for(let t=0;t<i.length;++t){const e=i[t],s=o[t],r=l[t];n[e.id]=[s,r]}}}}const i=[],r=[],a=[];for(const t of this.outputs){qr(t.id in n,`Could not compute output ${t.name} : ${t.id}`);const[e,s]=n[t.id];a.push(e.shape),i.push(e),r.push(s)}return[i,r,a]}buildNodeConversionMap(t){const e={};let n;for(const t of this.layers){n=t instanceof Gl?1:0;for(let s=0;s<t.inboundNodes.length;s++){const i=Gl.nodeKey(t,s);this.containerNodes.has(i)&&(e[i]=n,n+=1)}}return e}getLayer(t,e){if(null!=e){if(this.layers.length<=e)throw new Ur(`Was asked to retrieve layer at index ${e}, but model only has ${this.layers.length} layer(s).`);return this.layers[e]}if(null==t)throw new Ur("Provide either a layer name or layer index");for(const e of this.layers)if(e.name===t)return e;throw new Ur(`No such layer: ${t}`)}calculateLosses(){return e.tidy((()=>{const t=[];for(const e of this.layers)for(let n=0;n<e.inboundNodes.length;++n){const s=Gl.nodeKey(e,n);this.containerNodes.has(s)&&t.push(...e.calculateLosses())}return t}))}getConfig(){const t={name:this.name},e=this.buildNodeConversionMap(this.layers),n=[];for(const t of this.layers){const s=t.getClassName(),i=t.getConfig(),r=[];for(let n=0;n<t.inboundNodes.length;n++){const s=t.inboundNodes[n],i=Gl.nodeKey(t,n);let a={};if(this.containerNodes.has(i)){if(s.callArgs)try{JSON.stringify(s.callArgs),a=s.callArgs}catch(e){console.warn(`Layer ${t.name} was passed non-serializable keyword arguments: ${s.callArgs}. They will not be included in the serialized model (and thus will be missing at deserialization time).`),a={}}if(s.inboundLayers.length>0){const t=[];for(let n=0;n<s.inboundLayers.length;n++){const i=s.inboundLayers[n],r=s.nodeIndices[n],o=s.tensorIndices[n];let l=e[Gl.nodeKey(i,r)];null==l&&(l=0),t.push([i.name,l,o,a])}r.push(t)}}}const a={};a.name=t.name,a.className=s,a.config=i,a.inboundNodes=r,n.push(a)}t.layers=n;const s=[];for(let t=0;t<this.inputLayers.length;t++){const n=this.inputLayers[t],i=this.inputLayersNodeIndices[t],r=Gl.nodeKey(n,i);if(!this.containerNodes.has(r))continue;let a=e[r];null==a&&(a=0);const o=this.inputLayersTensorIndices[t];s.push([n.name,a,o])}t.inputLayers=s;const i=[];for(let t=0;t<this.outputLayers.length;t++){const n=this.outputLayers[t],s=this.outputLayersNodeIndices[t],r=Gl.nodeKey(n,s);if(!this.containerNodes.has(r))continue;let a=e[r];null==a&&(a=0);const o=this.outputLayersTensorIndices[t];i.push([n.name,a,o])}return t.outputLayers=i,t}static fromConfig(t,e,n={},s=!1){const i={},r={};function a(t,e){t.name in r?r[t.name].push(e):r[t.name]=[e]}function o(t,e){const n=[];let s;for(const r of e){const o=r[0],l=r[1],u=r[2];if(s=null==r[3]?{}:r[3],!(o in i))return void a(t,e);const h=i[o];if(h.inboundNodes.length<=l)return void a(t,e);const c=h.inboundNodes[l];n.push(c.outputTensors[u])}n.length>0&&t.apply(Hr(n),s)}function l(t){const n=t.name,r=al(t,null!=e.customObjects?e.customObjects:{});r.setFastWeightInitDuringBuild(s),i[n]=r;t.inboundNodes.forEach((t=>{if(!(t instanceof Array))throw new Ur(`Corrupted configuration, expected array for nodeData: ${t}`);a(r,t)}))}const u=e.name,h=e.layers;for(const t of h)l(t);for(;!ia(r);)for(const t of h){const e=i[t.name];if(e.name in r){const t=r[e.name];delete r[e.name];for(const n of t)o(e,n)}}const c=[],p=[],d=e.inputLayers;for(const t of d){const e=t[0],n=t[1],s=t[2];qr(e in i);const r=i[e].inboundNodes[n].outputTensors;c.push(r[s])}const f=e.outputLayers;for(const t of f){const e=t[0],n=t[1],s=t[2];qr(e in i);const r=i[e].inboundNodes[n].outputTensors;p.push(r[s])}return new t({inputs:c,outputs:p,name:u})}get stateful(){if(this._stateful)throw new Ur("Container instance unexpectedly has _stateful = true. The statefulness of a Container is determined by the Layers it contains. Its _stateful property must remain the default false.");for(const t of this.layers)if(t.stateful)return!0;return!1}resetStates(){e.tidy((()=>{this.layers.forEach((t=>{t.stateful&&t.resetStates()}))}))}}function Hl(t,e){return function(t,e,n){const s=e.length;if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>null));if(1===s)return Array.isArray(t)&&1===t.length?t:"object"==typeof t&&e[0]in t?[t[e[0]]]:[t];if(Array.isArray(t)){if(t.length!==s)throw new Error(`Provided ${n} is an array of ${t.length} element(s), but the model has ${s} outputs. Make sure a set of weights is provided for each model output.`);return t}if("object"==typeof t&&Object.keys(t).length>0&&"object"==typeof t[Object.keys(t)[0]]){const n=[];return e.forEach((e=>{e in t?n.push(t[e]):n.push(null)})),n}throw new Error(`The model has multiple (${s}) outputs, so ${n} must be either an array with ${s} elements or an object with ${e} keys. Provided ${n} not understood: ${JSON.stringify(t)}`)}(t,e,"classWeight")}async function Jl(t,n,s,i){if(null!=n||null!=i)throw new Error("Support sampleWeight is not implemented yet");if(null!=s){const n=e.tidy((()=>{if(1===t.shape.length)return t.clone();if(2===t.shape.length){if(t.shape[1]>1){const e=1;return t.argMax(e)}if(1===t.shape[1])return t.reshape([t.shape[0]]);throw new Error(`Encountered unexpected last-dimension size (${t.shape[1]}) during handling of class weights. The size is expected to be >= 1.`)}throw new Error(`Unexpected rank of target (y) tensor (${t.rank}) during handling of class weights. The rank is expected to be 1 or 2.`)})),i=Array.from(await n.data());e.dispose(n);const r=[];return i.forEach((t=>{if(null==s[t])throw new Error(`classWeight must contain all classes in the training data. The class ${t} exists in the data but not in classWeight`);r.push(s[t])})),e.tensor1d(r,"float32")}return null}function Zl(t,n){return e.mul(t,n)}function Xl(t,n){let s,i;const r=n;s=r.xs,i=r.ys,e.util.assert(null!=s&&null!=i,(()=>`A Dataset iterator for fitDataset() is expected to generate objects of the form \`{xs: xVal, ys: yVal}\`, where the two values may be \`tf.Tensor\`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates ${n}`));const a=Yl("input",t.inputNames,s),o=Yl("output",t.outputNames,i),l=a[0].shape[0];e.util.assert(a.length===t.inputs.length,(()=>`LayersModel has ${t.inputs.length} inputs, but the dataset provides ${a.length} inputs.  (Expected input keys: ${JSON.stringify(t.inputNames)})`)),e.util.assert(o.length===t.outputs.length,(()=>`LayersModel has ${t.outputs.length} outputs, but the dataset provides ${o.length} outputs.  (Expected output keys: ${JSON.stringify(t.outputNames)})`));for(let n=0;n<a.length;n++)e.util.assert(a[n].shape[0]===l,(()=>`Batch size mismatch: input ${t.inputNames[n]} has ${a[n].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`));for(let n=0;n<o.length;n++)e.util.assert(o[n].shape[0]===l,(()=>`Batch size mismatch: output ${t.outputNames[n]} has ${o[n].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`));return{xs:a,ys:o}}function Yl(t,n,s){if(s instanceof e.Tensor)return[s];if(Array.isArray(s))return e.util.assert(s.length===n.length,(()=>`Received an array of ${s.length} Tensors, but expected ${n.length} to match the ${t} keys ${n}.`)),s;{const e=[];for(const i of n){if(null==s[i])throw new Ur(`The feature data generated by the dataset lacks the required ${t} key '${i}'.`);e.push(s[i])}return e}}async function Ql(t,n,s){const i=null!=s.batchesPerEpoch;if(e.util.assert(null!=t.optimizer,(()=>"You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig).")),e.util.assert(null!=s,(()=>"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call.")),e.util.assert(null!=s.epochs&&s.epochs>0&&Number.isInteger(s.epochs),(()=>`For fitDataset(), config.epochs is expected to be a positive integer, but got ${s.epochs}`)),e.util.assert(!i||s.batchesPerEpoch>0&&Number.isInteger(s.batchesPerEpoch),(()=>`For fitDataset(), config.batchesPerEpoch is expected to be a positive integer if specified, but got ${s.batchesPerEpoch}`)),e.util.assert(null==s.validationSplit,(()=>"`validationSplit` is not supported by `fitDataset()`. Use validationData instead.")),t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");t.isTraining=!0;try{const r=null!=s.validationData;let a,o;if(r)if(tu(s.validationData))e.util.assert(null==s.validationBatches||s.validationBatches>0&&Number.isInteger(s.validationBatches),(()=>`For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, but got ${s.validationBatches}`));else{const t=function(t){if(3===t.length)throw new Kr("Validation with sample weights is not implemented yet.");return{xs:t[0],ys:t[1]}}(s.validationData);a=t.xs,o=t.ys}const l=t.makeTrainFunction(),u=t.getDedupedMetricsNames();let h;h=r?u.slice().concat(u.map((t=>"val_"+t))):u.slice();const c=sl(s.callbacks,s.yieldEvery),p=null==s.verbose?1:s.verbose,{callbackList:d,history:f}=rl(c,p,s.epochs,null,null,function(t,e){let n=null;null!=e.batchesPerEpoch?n=e.batchesPerEpoch:Number.isFinite(t.size)&&(n=t.size);return n}(n,s),null,r,h);d.setModel(t),t.history=f,await d.onTrainBegin(),t.stopTraining_=!1;let g=null==s.initialEpoch?0:s.initialEpoch,m=await n.iterator();for(;g<s.epochs;){const h={};await d.onEpochBegin(g);let c=0,p=0;for(i||(m=await n.iterator());!i||c<s.batchesPerEpoch;){const n=await m.next();if(i&&n.done){console.warn(`You provided \`batchesPerEpoch\` as ${s.batchesPerEpoch}, but your dataset iterator ran out of data after ${c} batches; interrupting training. Make sure that your dataset can generate at least \`batchesPerEpoch * epochs\` batches (in this case, `+s.batchesPerEpoch*s.epochs+" batches). You may need to use the repeat() function when building your dataset.");break}if(null!=n.value){const{xs:i,ys:r}=Xl(t,n.value),a={};a.batch=p,a.size=i[0].shape[0],await d.onBatchBegin(p,a);const o=[];if(null!=s.classWeight){const e=Hl(s.classWeight,t.outputNames);for(let t=0;t<e.length;++t)o.push(await Jl(r[t],null,e[t]))}const h=i.concat(r).concat(o),f=l(h);e.dispose(h);for(let t=0;t<u.length;++t){const n=u[t],s=f[t];a[n]=s,e.keep(s)}await d.onBatchEnd(p,a),Zo(a),p++,c++}if(i?c>=s.batchesPerEpoch:n.done){if(r){let e;e=tu(s.validationData)?Jr(await t.evaluateDataset(s.validationData,{batches:s.validationBatches})):Jr(t.evaluate(a,o,{batchSize:null==s.validationBatchSize?32:s.validationBatchSize,verbose:0}));for(let n=0;n<t.metricsNames.length;++n)h[`val_${t.metricsNames[n]}`]=e[n]}break}if(t.stopTraining_)break}if(await d.onEpochEnd(g,h),g++,t.stopTraining_)break}return await d.onTrainEnd(),await t.history.syncData(),t.history}finally{t.isTraining=!1}}function tu(t){return"function"==typeof t.iterator}function eu(t){e.util.assert(t>0&&Number.isInteger(t),(()=>`batchSize is required to be a positive integer, but got ${t}`))}function nu(t,e,n){return null==t?[null]:Array.isArray(t)?t.map((t=>Ka(t,e,n-e))):Ka(t,e,n-e)}function su(t,n){return e.tidy((()=>null==t?null:Array.isArray(t)?t.map((t=>su(t,n))):Xa(t,"int32"===n.dtype?n:n.toInt())))}function iu(t,e){const n=[];let s=0,i=null;for(;s<t;)i=s+e,i>=t&&(i=t),n.push([s,i]),s=i;return n}async function ru(t,n,s,i={}){if(t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");let r,a,o,l,u,h,c;t.isTraining=!0;try{const p=null==i.batchSize?32:i.batchSize;eu(p);const d=!1,f=await t.standardizeUserData(n,s,i.sampleWeight,i.classWeight,d,p);r=f[0],a=f[1],c=f[2];let g,m=!1;if(null!=i.validationData&&i.validationData.length>0){if(m=!0,2!==i.validationData.length)throw 3===i.validationData.length?new Kr("validationData including sample weights is not supported yet."):new Ur(`When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; ${i.validationData} is invalid.`);o=i.validationData[0],l=i.validationData[1];const e=!0,n=await t.standardizeUserData(o,l,null,null,e,p);u=n[0],h=n[1],g=u.concat(h)}else if(null!=i.validationSplit&&i.validationSplit>0&&i.validationSplit<1){m=!0;const t=Math.floor(r[0].shape[0]*(1-i.validationSplit)),e=r[0].shape[0];u=nu(r,t,e),r=nu(r,0,t),h=nu(a,t,e),a=nu(a,0,t),g=u.concat(h)}else null!=i.validationSteps&&(m=!0);const y=r.concat(a).concat(c);t.checkTrainableWeightsConsistency();const b=t.makeTrainFunction(),w=t.getDedupedMetricsNames();let k,x;m?(t.makeTestFunction(),k=t.testFunction,x=w.slice().concat(w.map((t=>"val_"+t)))):(k=null,g=[],x=w.slice());const v=sl(i.callbacks,i.yieldEvery);return await async function(t,n,s,i,r,a,o,l,u,h,c,p,d,f,g){null==r&&(r=32),null==a&&(a=1),null==c&&(c=!0),null==d&&(d=0);let m=!1;if(null!=u&&null!=h&&(m=!0),null!=g&&(m=!0,null==f))throw new Ur("Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.");const y=t.checkNumSamples(s,r,f,"steps_per_epoch");let b;null!=y&&(b=Wa(0,y)),null==o&&(o=1);const{callbackList:w,history:k}=rl(l,o,a,d,y,f,r,m,p);w.setModel(t),t.history=k,await w.onTrainBegin(),t.stopTraining_=!1;for(let o=d;o<a;++o){await w.onEpochBegin(o);const a={};if(null!=f)throw new Kr("stepsPerEpoch mode is not implemented yet.");{if("batch"===c)throw new Kr("batch shuffling is not implemneted yet");c&&e.util.shuffle(b);const o=e.tensor1d(b),l=iu(y,r);for(let c=0;c<l.length;++c){const p={};if(await w.onBatchBegin(c,p),e.tidy((()=>{const d=l[c][0],f=l[c][1],g=Ka(o,d,f-d);p.batch=c,p.size=f-d;const y=su(s,g),b=n(y);for(let t=0;t<i.length;++t){const n=i[t],s=b[t];p[n]=s,e.keep(s)}if(c===l.length-1&&m){const n=t.testLoop(u,h,r);for(let t=0;t<i.length;++t){const s=i[t],r=n[t];e.keep(r),a["val_"+s]=r}}})),await w.onBatchEnd(c,p),Zo(p),t.stopTraining_)break}o.dispose()}if(await w.onEpochEnd(o,a),t.stopTraining_)break}return await w.onTrainEnd(),await t.history.syncData(),t.history}(t,b,y,w,p,i.epochs,i.verbose,v,k,g,i.shuffle,x,i.initialEpoch,null,null)}finally{t.isTraining=!1,ou(r,n),ou(a,s),ou(u,o),ou(h,l),null!=c&&e.dispose(c)}}function au(t){const n=[];t instanceof e.Tensor&&(t=[t]);for(let e=0;e<t.length;++e){const s=t[e];if(1===s.rank)n.push(Ua(s,1));else{if(0===s.rank)throw new Error("Expected tensor to be at least 1D, but received a 0D tensor (scalar).");n.push(s)}}return n}function ou(t,n){if(null==t)return;const s=[];if(n instanceof e.Tensor)s.push(n.id);else if(Array.isArray(n))n.forEach((t=>s.push(t.id)));else if(null!=n)for(const t in n){const e=n[t];s.push(e.id)}const i=[];if(t instanceof e.Tensor)-1===s.indexOf(t.id)&&i.push(t);else if(Array.isArray(t))t.forEach((t=>{-1===s.indexOf(t.id)&&i.push(t)}));else if(null!=t)for(const e in t){const n=t[e];-1===s.indexOf(n.id)&&i.push(n)}i.forEach((t=>{t.isDisposed||t.dispose()}))}function lu(t){return Array.isArray(t)}function uu(t){return!function(t){return t instanceof e.Tensor}(t)&&!lu(t)}function hu(t,e,n,s=!0,i=""){if(null==e||0===e.length){if(null!=t){let e=!1;if(lu(t)&&t.length>0)e=!0;else if(uu(t)){for(const n in t)if(t.hasOwnProperty(n)){e=!0;break}}else e=!0;if(e)throw new Ur(`Error when checking model ${i} expected no data, but got ${t}`)}return[]}if(null==t)return e.map((t=>null));let r;if(uu(t)){t=t,r=[];for(const n of e){if(null==t[n])throw new Ur(`No data provided for "${n}". Need data for each key in: ${e}`);r.push(t[n])}}else if(lu(t)){if((t=t).length!==e.length)throw new Ur(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see ${e.length} Tensor(s), but instead got the following list of Tensor(s): ${t}`);r=t}else{if(t=t,e.length>1)throw new Ur(`The model ${i} expects ${e.length} Tensor(s), but only received one Tensor. Found: Tensor with shape ${t.shape}`);r=[t]}if(r=au(r),null!=n)for(let t=0;t<e.length;++t){if(null==n[t])continue;const a=r[t];if(a.shape.length!==n[t].length)throw new Ur(`Error when checking ${i}: expected ${e[t]} to have ${n[t].length} dimension(s). but got array with shape ${a.shape}`);for(let r=0;r<n[t].length;++r){if(0===r&&!s)continue;const o=a.shape[r],l=n[t][r];if(null!=l&&l>=0&&o!==l)throw new Ur(`Error when checking ${i}: expected ${e[t]} to have shape [${n[t]}], but got array with shape [${a.shape}].`)}}return r}function cu(t,e,n,s=!0,i=""){let r;if(Array.isArray(t)){if(t.length!==e.length)throw new Ur(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see ${e.length} Tensor(s), but instead got ${t.length} Tensors(s).`);r=t}else{if(e.length>1)throw new Ur(`The model expects ${e.length} ${i} Tensors, but only received one Tensor. Found: array with shape ${JSON.stringify(t.shape)}.`);r=[t]}if(null!=n)for(let t=0;t<e.length;++t){if(null==n[t])continue;const a=r[t];if(a.shape.length!==n[t].length)throw new Ur(`Error when checking ${i}: expected ${e[t]} to have ${n[t].length} dimension(s), but got array with shape ${JSON.stringify(a.shape)}`);for(let r=0;r<n[t].length;++r){if(0===r&&!s)continue;const o=a.shape[r],l=n[t][r];if(null!=l&&l!==o)throw new Ur(`Error when checking ${i}: expected ${e[t]} to have shape ${JSON.stringify(n[t])} but got array with shape ${JSON.stringify(a.shape)}.`)}}}class pu extends Gl{constructor(t){super(t),this.isTraining=!1}summary(t,e,n=console.log){if(!this.built)throw new Ur("This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).");El(this,t,e,n)}compile(t){if(null==t.loss&&(t.loss=[]),this.loss=t.loss,"string"==typeof t.optimizer)this.optimizer_=function(t){const n={Adagrad:()=>e.train.adagrad(.01),Adadelta:()=>e.train.adadelta(1,.95,Br()),Adam:()=>e.train.adam(.001,.9,.999,Br()),Adamax:()=>e.train.adamax(.002,.9,.999,Br(),0),RMSProp:()=>e.train.rmsprop(.001,.9,0,Br()),SGD:()=>e.train.sgd(.01)};if(n.adagrad=n.Adagrad,n.adadelta=n.Adadelta,n.adam=n.Adam,n.adamax=n.Adamax,n.rmsprop=n.RMSProp,n.sgd=n.SGD,t in n)return n[t]();throw new Ur(`Unknown Optimizer ${t}`)}(t.optimizer),this.isOptimizerOwned=!0;else{if(!(t.optimizer instanceof e.Optimizer))throw new Ur("User-defined optimizer must be an instance of tf.Optimizer.");this.optimizer_=t.optimizer,this.isOptimizerOwned=!1}let n=[];if(Array.isArray(t.loss)||"string"==typeof t.loss||"function"==typeof t.loss)if(Array.isArray(t.loss)){if(t.loss.length!==this.outputs.length)throw new Ur(`When passing an Array as loss, it should have one entry per model output. The model has ${this.outputs.length} output(s), but you passed loss=${t.loss}.`);const e=t.loss;n=e.map((t=>ml(t)))}else{const e=ml(t.loss);this.outputs.forEach((t=>{n.push(e)}))}else{t.loss=t.loss;for(const e in t.loss)if(-1===this.outputNames.indexOf(e))throw new Ur(`Unknown entry in loss dictionary: "${e}". Only expected the following keys: ${this.outputNames}`);for(const e of this.outputNames)null==t.loss[e]&&console.warn(`Output "${e}" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to ${e} during training`),n.push(ml(t.loss[e]))}this.lossFunctions=n,this.feedOutputNames=[],this.feedOutputShapes=[],this.feedLossFns=[];for(let t=0;t<this.outputs.length;++t){const e=this.internalOutputShapes[t],n=this.outputNames[t];this.feedOutputNames.push(n),this.feedOutputShapes.push(e),this.feedLossFns.push(this.lossFunctions[t])}const s=[];this.metrics=t.metrics,this.metricsNames=["loss"],this.metricsTensors=[],Ta("loss",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==s.indexOf(t))continue;const e=this.lossFunctions[t];this.outputs.length>1&&(this.metricsTensors.push([e,t]),this.metricsNames.push(this.outputNames[t]+"_loss"))}}));const i=function(t,e){if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>[]));let n;if("string"==typeof t||"function"==typeof t)n=[t];else{if(!Array.isArray(t)&&"object"!=typeof t)throw new TypeError(`Type of metrics argument not understood. Expected an string,function, Array, or Object, found: ${t}`);n=t}if(Array.isArray(n))return e.map((t=>n));{const t=[];for(const s of e){let e=n.hasOwnProperty(s)?n[s]:[];Array.isArray(e)||(e=[e]),t.push(e)}return t}}(t.metrics,this.outputNames),r=(t,e,n)=>{this.outputNames.length>1&&(e=this.outputNames[t]+"_"+e),this.metricsNames.push(e),this.metricsTensors.push([n,t])};Ta("metric",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==s.indexOf(t))continue;(e=>{let n,s,i;for(const a of e){if("string"==typeof a&&-1!==["accuracy","acc","crossentropy","ce"].indexOf(a)){const e=this.internalOutputShapes[t];let r;1===e[e.length-1]||this.lossFunctions[t]===dl?-1!==["accuracy","acc"].indexOf(a)?s=yl:-1!==["crossentropy","ce"].indexOf(a)&&(s=vl):this.lossFunctions[t]===pl?-1!==["accuracy","acc"].indexOf(a)?s=Sl:-1!==["crossentropy","ce"].indexOf(a)&&(s=Nl):-1!==["accuracy","acc"].indexOf(a)?s=bl:-1!==["crossentropy","ce"].indexOf(a)&&(s=Il),-1!==["accuracy","acc"].indexOf(a)?r="acc":-1!==["crossentropy","ce"].indexOf(a)&&(r="ce"),i=s,n=""+r}else{const t=Al(a);i=t,n=""+Cl(a)}let e;Ta(n,(()=>{e=i})),r(t,n,e)}})(i[t])}})),this.collectedTrainableWeights=this.trainableWeights}checkTrainableWeightsConsistency(){null!=this.collectedTrainableWeights&&this.trainableWeights.length!==this.collectedTrainableWeights.length&&console.warn("Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?")}evaluate(t,e,n={}){const s=null==n.batchSize?32:n.batchSize;eu(s);const i=this.standardizeUserDataXY(t,e,!0,s);try{const r=i[0].concat(i[1]);this.makeTestFunction();const a=this.testFunction;return Hr(this.testLoop(a,r,s,n.verbose,n.steps))}finally{ou(i[0],t),ou(i[1],e)}}async evaluateDataset(t,n){return this.makeTestFunction(),async function(t,n,s){const i=null!=(s=s||{}).batches,r=t.testFunction;let a=[];if(s.verbose>0)throw new Kr("Verbose mode is not implemented yet.");e.util.assert(!i||s.batches>0&&Number.isInteger(s.batches),(()=>`Test loop expects \`batches\` to be a positive integer, but received ${JSON.stringify(s.batches)}`));const o="function"==typeof n.next?n:await n.iterator();let l=0,u=0;for(;!i||u<s.batches;){const n=await o.next();if(a=e.tidy((()=>{if(n.value){const{xs:s,ys:i}=Xl(t,n.value),o=s.concat(i),h=e.tidy((()=>r(o)));if(e.dispose(o),0===u)for(let t=0;t<h.length;++t)a.push(e.scalar(0));const c=o[0].shape[0];for(let t=0;t<h.length;++t){const n=h[t],s=a[t];a[t]=e.tidy((()=>e.add(a[t],e.mul(c,n)))),u>0&&e.dispose(s)}e.dispose(h),l+=c,++u}return a})),n.done){i&&console.warn(`Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least \`batches\` batches (in this case, ${s.batches} batches). You may need to use the repeat() function when building your dataset.`);break}}for(let t=0;t<a.length;++t){const n=a[t];a[t]=e.div(a[t],l),e.dispose(n)}return Hr(a)}(this,t,n)}checkNumSamples(t,e,n,s="steps"){let i;if(null!=n){if(i=null,null!=e)throw new Ur(`If ${s} is set, batchSize must be null or undefined.Got batchSize = ${e}`)}else{if(null==t)throw new Ur(`Either the input data should have a defined shape, or ${s} shoud be specified.`);i=Array.isArray(t)?t[0].shape[0]:t.shape[0]}return i}execute(t,n){if(Array.isArray(n)&&0===n.length)throw new Ur("`outputs` is an empty Array, which is not allowed.");const s=Array.isArray(n),i=s?n:[n],r=this.retrieveSymbolicTensors(i),a=new Wl;if(t instanceof e.Tensor&&(t=[t]),Array.isArray(t)){if(t.length!==this.inputs.length)throw new Ur(`The number of inputs provided (${t.length}) does not match the number of inputs of this model (${this.inputs.length}).`);for(let e=0;e<this.inputs.length;++e)a.add(this.inputs[e],t[e])}else for(const e of this.inputs){const n=t[e.name];if(null==n)throw new Ur(`No value is provided for the model's input ${e.name}`);a.add(e,n)}const o=Kl(r,a);return s?o:o[0]}retrieveSymbolicTensors(t){const e=jr(null,t.length);let n=t.length;for(const s of this.layers){const i=Array.isArray(s.output)?s.output:[s.output],r=i.map((t=>t.name));for(let s=0;s<t.length;++s){const a=r.indexOf(t[s]);if(-1!==a&&(e[s]=i[a],n--),0===n)break}if(0===n)break}if(n>0){const n=[];throw e.forEach(((e,s)=>{null==e&&n.push(t[s])})),new Ur(`Cannot find SymbolicTensors for output name(s): ${JSON.stringify(n)}`)}return e}predictLoop(t,n=32,s=!1){return e.tidy((()=>{const i=this.checkNumSamples(t);if(s)throw new Kr("Verbose predictLoop() is not implemented yet.");const r=iu(i,n),a=this.outputs.map((t=>[]));for(let n=0;n<r.length;++n){e.tidy((()=>{const e=r[n][0],s=r[n][1],i=nu(t,e,s),a=[];if(Array.isArray(i))for(let t=0;t<i.length;++t)a.push({key:this.inputs[t],value:i[t]});else a.push({key:this.inputs[0],value:i});const o=new Wl(a);return Kl(this.outputs,o)})).forEach(((t,e)=>a[e].push(t)))}return Hr(a.map((t=>e.concat(t,0))))}))}predict(t,e={}){const n=au(t);cu(n,this.inputNames,this.feedInputShapes,!1);try{const s=null==e.batchSize?32:e.batchSize;return eu(s),this.predictLoop(n,s)}finally{ou(n,t)}}predictOnBatch(t){cu(t,this.inputNames,this.feedInputShapes,!0);const e=(Array.isArray(t)?t[0]:t).shape[0];return this.predictLoop(t,e)}standardizeUserDataXY(t,n,s=!0,i){if(null==this.optimizer_)throw new Pr("You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).");const r=[];for(let t=0;t<this.feedOutputShapes.length;++t){const e=this.feedOutputShapes[t];this.feedLossFns[t]===pl?r.push(e.slice(0,e.length-1).concat([1])):r.push(e)}if(function(t,n,s){const i=sa(t.map((t=>t.shape[0])));i.sort();const r=sa(n.map((t=>t.shape[0])));if(r.sort(),i.length>1)throw new Ur(`All input Tensors (x) should have the same number of samples. Got array shapes: ${JSON.stringify(t.map((t=>t.shape)))}`);if(r.length>1)throw new Ur(`All target Tensors (y) should have the same number of samples. Got array shapes: ${JSON.stringify(n.map((t=>t.shape)))}`);if(i.length>0&&r.length>0&&!e.util.arraysEqual(i,r))throw new Ur(`Input Tensors should have the same number of samples as target Tensors. Found ${i[0]} input sample(s) and ${r[0]} target sample(s).`)}(t=hu(t,this.feedInputNames,this.feedInputShapes,!1,"input"),n=hu(n,this.feedOutputNames,r,!1,"target")),function(t,e,n){const s=[ll,dl,cl];for(let i=0;i<t.length;++i){const r=t[i],a=e[i],o=n[i];if(null!=a){if(a===cl&&1===r.shape[r.shape.length-1])throw new Ur(`You are passing a target array of shape ${r.shape} while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].`);if(-1!==s.indexOf(a)){const t=r.shape.slice(1),e=o.slice(1);for(let n=0;n<t.length;++n){const s=t[n],i=e[n];if(null!=i&&s!==i)throw new Ur(`A target Tensor with shape ${r.shape} was passed for an output of shape ${o}, while using a loss function that expects targets to have the same shape as the output.`)}}}}}(n,this.feedLossFns,this.feedOutputShapes),this.stateful&&null!=i&&i>0&&t[0].shape[0]%i!=0)throw new Ur(`In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size ${i}. Found: ${t[0].shape[0]} sample(s).`);return[t,n]}async standardizeUserData(t,e,n,s,i=!0,r){const[a,o]=this.standardizeUserDataXY(t,e,i,r);if(null!=n)throw new Error("sample weight is not supported yet.");let l=null;if(null!=s){const t=Hl(s,this.outputNames);l=[];for(let e=0;e<t.length;++e)l.push(await Jl(o[e],null,t[e]))}return[a,o,l]}testLoop(t,n,s,i=0,r){return e.tidy((()=>{const a=this.checkNumSamples(n,s,r,"steps"),o=[];if(i>0)throw new Kr("Verbose mode is not implemented yet.");if(null!=r)throw new Kr("steps mode in testLoop() is not implemented yet");{const i=iu(a,s),r=e.tensor1d(Wa(0,a));for(let s=0;s<i.length;++s){const a=i[s][0],l=i[s][1],u=Ka(r,a,l-a),h=su(n,u),c=t(h);if(0===s)for(let t=0;t<c.length;++t)o.push(e.scalar(0));for(let t=0;t<c.length;++t){const n=c[t];o[t]=e.add(o[t],e.mul(l-a,n))}}for(let t=0;t<o.length;++t)o[t]=e.div(o[t],a)}return o}))}getDedupedMetricsNames(){const t=this.metricsNames,e=[];for(let n=0;n<t.length;++n){const s=t[n];let i=s;if(Gr(t,s)>1){i+=`_${Gr(t.slice(0,n),s)}`}e.push(i)}return e}makeTrainFunction(){return t=>{const n=[],s=t.slice(0,this.inputs.length),i=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),r=t.slice(this.inputs.length+this.outputs.length,this.inputs.length+2*this.outputs.length),a=[],o=this.collectedTrainableWeights.map((t=>t.read()));return[this.optimizer_.minimize((()=>{const t=[];for(let e=0;e<this.inputs.length;++e)t.push({key:this.inputs[e],value:s[e]});const o=new Wl(t),l=Kl(this.outputs,o,{training:!0});let u;for(let t=0;t<this.lossFunctions.length;++t){let s=(0,this.lossFunctions[t])(i[t],l[t]);null!=r[t]&&(s=Zl(s,r[t]));const a=e.mean(s);n.push(a),u=0===t?s:e.add(u,s)}for(let t=0;t<this.metricsTensors.length;++t){let s;if(this.outputs.length>1&&t<this.outputs.length)s=n[t];else{const n=this.metricsTensors[t][0],r=this.metricsTensors[t][1];s=e.mean(n(i[r],l[r]))}e.keep(s),a.push(s)}return u=e.mean(u),this.calculateLosses().forEach((t=>{u=e.add(u,t)})),u}),!0,o)].concat(a)}}makeTestFunction(){this.testFunction=t=>e.tidy((()=>{const n=[];let s;const i=t.slice(0,this.inputs.length),r=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),a=[];for(let t=0;t<this.inputs.length;++t)a.push({key:this.inputs[t],value:i[t]});const o=new Wl(a),l=Kl(this.outputs,o);for(let t=0;t<this.lossFunctions.length;++t){const i=this.lossFunctions[t],a=e.mean(i(r[t],l[t]));s=0===t?a:e.add(s,a),n.push(s)}for(let t=0;t<this.metricsTensors.length;++t){const s=this.metricsTensors[t][0],i=this.metricsTensors[t][1],a=e.mean(s(r[i],l[i]));n.push(a)}return n}))}async fit(t,e,n={}){return ru(this,t,e,n)}async fitDataset(t,e){return Ql(this,t,e)}async trainOnBatch(t,n){const s=await this.standardizeUserData(t,n),i=s[0],r=s[1],a=this.makeTrainFunction()(i.concat(r)),o=[];for(const t of a){const e=await t.data();o.push(e[0])}return e.dispose(a),Hr(o)}getNamedWeights(t){const e=[],n=null!=t&&t.trainableOnly,s=n?this.trainableWeights:this.weights,i=this.getWeights(n);for(let t=0;t<s.length;++t)n&&!s[t].trainable||e.push({name:s[t].originalName,tensor:i[t]});return e}set stopTraining(t){this.stopTraining_=t}get stopTraining(){return this.stopTraining_}get optimizer(){return this.optimizer_}set optimizer(t){this.optimizer_!==t&&(this.optimizer_=t,this.isOptimizerOwned=!1)}dispose(){const t=super.dispose();if(0===t.refCountAfterDispose&&null!=this.optimizer&&this.isOptimizerOwned){const n=e.memory().numTensors;this.optimizer_.dispose(),t.numDisposedVariables+=n-e.memory().numTensors}return t}getLossIdentifiers(){let t;if("string"==typeof this.loss)t=Zr(this.loss);else if(Array.isArray(this.loss)){for(const t of this.loss)if("string"!=typeof t)throw new Error("Serialization of non-string loss is not supported.");t=this.loss.map((t=>Zr(t)))}else{const e=Object.keys(this.loss);t={};const n=this.loss;for(const s of e){if("string"!=typeof n[s])throw new Error("Serialization of non-string loss is not supported.");t[s]=Zr(n[s])}}return t}getMetricIdentifiers(){if("string"==typeof this.metrics||"function"==typeof this.metrics)return[Zr(Cl(this.metrics))];if(Array.isArray(this.metrics))return this.metrics.map((t=>Zr(Cl(t))));{const t={};for(const e in this.metrics)t[e]=Zr(Cl(this.metrics[e]));return t}}getTrainingConfig(){return{loss:this.getLossIdentifiers(),metrics:this.getMetricIdentifiers(),optimizer_config:{class_name:this.optimizer.getClassName(),config:this.optimizer.getConfig()}}}loadTrainingConfig(t){if(null!=t.weighted_metrics)throw new Error("Loading weight_metrics is not supported yet.");if(null!=t.loss_weights)throw new Error("Loading loss_weights is not supported yet.");if(null!=t.sample_weight_mode)throw new Error("Loading sample_weight_mode is not supported yet.");const e=al(Ml(t.optimizer_config));let n,s;if("string"==typeof t.loss)n=Xr(t.loss);else if(Array.isArray(t.loss))n=t.loss.map((t=>Xr(t)));else if(null!=t.loss){n={};for(const e in t.loss)n[e]=Xr(t.loss[e])}if(Array.isArray(t.metrics))s=t.metrics.map((t=>Xr(t)));else if(null!=t.metrics){s={};for(const e in t.metrics)s[e]=Xr(t.metrics[e])}this.compile({loss:n,metrics:s,optimizer:e})}async save(t,n){if("string"==typeof t){const n=e.io.getSaveHandlers(t);if(0===n.length)throw new Ur(`Cannot find any save handlers for URL '${t}'`);if(n.length>1)throw new Ur(`Found more than one (${n.length}) save handlers for URL '${t}'`);t=n[0]}if(null==t.save)throw new Ur("LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.");const s=await e.io.encodeWeights(this.getNamedWeights(n)),i={modelTopology:this.toJSON(null,!1),format:"layers-model",generatedBy:"TensorFlow.js tfjs-layers v3.6.0",convertedBy:null};if(null!=n&&n.includeOptimizer&&null!=this.optimizer){i.trainingConfig=this.getTrainingConfig();const t="optimizer",{data:n,specs:r}=await e.io.encodeWeights(await this.optimizer.getWeights(),t);s.specs.push(...r),s.data=e.io.concatenateArrayBuffers([s.data,n])}if(null!=this.userDefinedMetadata){const t=!0;Dl(this.userDefinedMetadata,this.name,t),i.userDefinedMetadata=this.userDefinedMetadata}return i.weightData=s.data,i.weightSpecs=s.specs,t.save(i)}setUserDefinedMetadata(t){Dl(t,this.name),this.userDefinedMetadata=t}getUserDefinedMetadata(){return this.userDefinedMetadata}}pu.className="Model",e.serialization.registerClass(pu);class du extends pu{}async function fu(t,n){if(null==n&&(n={}),"string"==typeof t){const s=e.io.getLoadHandlers(t,n);if(0===s.length)s.push(e.io.browserHTTPRequest(t,n));else if(s.length>1)throw new Ur(`Found more than one (${s.length}) load handlers for URL '${t}'`);t=s[0]}return async function(t,n,s){null==s&&(s={});if(null==t.load)throw new Ur("Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.");const i=await t.load();let r=i.modelTopology;null!=r.model_config&&(r=r.model_config);const a=null==s.strict||s.strict,o=null!=i.weightData&&null!=i.weightSpecs&&a,l=al(Ml(r),n,o),u=i.trainingConfig;null!=u&&l.loadTrainingConfig(u);null!=i.userDefinedMetadata&&l.setUserDefinedMetadata(i.userDefinedMetadata);if(null!=i.weightData){if(null==i.weightSpecs)throw new Ur("LayersModel artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.");const{modelWeights:t,optimizerWeights:n}=function(t,n){const s=e.io.decodeWeights(t,n),i={},r=[];return n.forEach((t=>{"optimizer"===t.group?r.push({name:t.name,tensor:s[t.name]}):i[t.name]=s[t.name]})),{modelWeights:i,optimizerWeights:r}}(i.weightData,i.weightSpecs);l.loadWeights(t,a),null!=l.optimizer&&n.length>0&&await l.optimizer.setWeights(n),e.dispose(t),e.dispose(n.map((t=>t.tensor)))}return l}(t,void 0,n)}du.className="Functional",e.serialization.registerClass(du);class gu extends pu{constructor(t){if(super({inputs:[],outputs:[]}),t=t||{},this.trainable=!0,this.built=!1,this.name=null!=t.name?t.name:Do("sequential_"),null!=t.layers)for(const e of t.layers)this.add(e)}checkShape(t){if(t.inboundNodes[0].outputTensors[0].shape.some((t=>t<0)))throw new Ur(`Negative dimension size caused by adding layer ${t.name} with input shape [${t.inboundNodes[0].inputTensors[0].shape}]`)}add(t){const e=t instanceof gu||t instanceof pu;let n;if(e){if(n=t,1!==n.outputs.length)throw new Ur("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");if(1!==n.inputs.length)throw new Ur("All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.")}if(0===this.outputs.length){if(0===t.inboundNodes.length){if(null==t.batchInputShape)throw new Ur("The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.");const e=Ho({batchShape:t.batchInputShape,dtype:t.dtype,name:t.name+"_input"});t.apply(e)}if(e)this.outputs=n.outputs,this.inputs=n.inputs;else{if(1!==t.inboundNodes.length)throw new Ur(`A layer added to a Sequential model must not already be connected somewhere else. LayersModel received layer ${t.name} which has ${t.inboundNodes.length} pre-existing inbound connections.`);if(1!==t.inboundNodes[0].outputTensors.length)throw new Ur("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[t.inboundNodes[0].outputTensors[0]],this.inputs=qo(this.outputs[0])}this.inboundNodes=[],new Ko({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:jr(null,this.inputs.length),outputMasks:[null],inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs[0].shape})}else{const e=t.apply(this.outputs[0]);if(Array.isArray(e))throw new TypeError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[e],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}this.layers.push(t),this.built=!1}pop(){if(0===this.layers.length)throw new TypeError("There are no layers in the model.");if(this.layers.pop(),0===this.layers.length)this.outputs=[],this.inboundNodes=[],this.outboundNodes=[];else{const t=this.layers.length-1;this.layers[t].outboundNodes=[],this.outputs=[this.layers[t].output],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}}call(t,e){return null==this.model&&this.build(),this.model.call(t,e)}build(t){if(Lo(t),0===this.inputs.length||0===this.outputs.length)throw new TypeError("Sequential model cannot be built: model is empty. Add some layers first.");this.model=new pu({inputs:this.inputs,outputs:this.outputs[0],name:this.name+"_model"}),this.model.trainable=this.trainable,this.supportsMasking=this.model.supportsMasking,this.inputLayers=this.model.inputLayers,this.inputLayersNodeIndices=this.model.inputLayersNodeIndices,this.inputLayersTensorIndices=this.model.inputLayersTensorIndices,this.outputLayers=this.model.outputLayers,this.outputLayersNodeIndices=this.model.outputLayersNodeIndices,this.outputLayersTensorIndices=this.model.outputLayersTensorIndices,this.nodesByDepth=this.model.nodesByDepth,this.containerNodes=this.model.containerNodes,this.outputNames=this.model.outputNames,this.inputNames=this.model.inputNames,this.built=!0}countParams(){return this.built||this.build(),super.countParams()}summary(t,e,n=console.log){this.built||this.build(),super.summary(t,e,n)}setWeights(t){null==this.model&&this.build(),this.model.setWeights(t)}evaluate(t,e,n={}){if(!this.built)throw new Pr("The model needs to be compiled before being used.");return this.model.evaluate(t,e,n)}async evaluateDataset(t,e){if(!this.built)throw new Pr("The model needs to be compiled before being used.");return this.model.evaluateDataset(t,e)}predict(t,e={}){return null==this.model&&this.build(),this.model.predict(t,e)}predictOnBatch(t){return null==this.model&&this.build(),this.model.predictOnBatch(t)}compile(t){this.build(),this.model.compile(t),this.optimizer_=this.model.optimizer,this.isOptimizerOwned=this.model.isOptimizerOwned,this.loss=this.model.loss,this.metrics=this.model.metrics,this.metricsTensors=this.model.metricsTensors,this.metricsNames=this.model.metricsNames}get optimizer(){return null==this.model?void 0:this.model.optimizer}set optimizer(t){this.model.optimizer=t}async fit(t,e,n={}){if(!this.built)throw new Pr("The model needs to be compiled before being used.");return this.model.fit(t,e,n)}async fitDataset(t,e){if(!this.built)throw new Pr("The model needs to be compiled before being used.");return this.model.fitDataset(t,e)}async trainOnBatch(t,e){return this.model.trainOnBatch(t,e)}static fromConfig(t,n,s={},i=!1){let r,a={};if(n instanceof Array){if(null==n[0].className||"Merge"===n[0].className)throw new Ur("Legacy serialization format not supported yet.");r=n}else e.util.assert(null!=n.layers,(()=>"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field.")),r=n.layers,delete n.layers,a=n;const o=new t(a);if(!(o instanceof gu))throw new Kr(`Sequential.fromConfig called on non-Sequential input: ${o}`);for(const t of r){const e=al(t,void 0,i);i&&e.setFastWeightInitDuringBuild(!0),o.add(e)}return o}set stopTraining(t){if(null==this.model)throw new Ur("Cannot set the stopTraining property of a sequential model before it is compiled.");this.model.stopTraining=t}get stopTraining(){if(null==this.model)throw new Ur("Cannot get the stopTraining property of a sequential model before it is compiled.");return this.model.stopTraining}getConfig(){const t=[];for(const e of this.layers){const n={};n.className=e.getClassName(),n.config=e.getConfig(),t.push(n)}return{name:this.name,layers:t}}}function mu(t){return Ho(t)}gu.className="Sequential",e.serialization.registerClass(gu);class yu extends e.serialization.Serializable{getConfig(){return{}}}class bu extends yu{apply(t,n=1){return function(t,n=1){if(1!==n)throw new Kr(`Support for alpha values other than 1 (${n}) is not implemented yet.`);return e.elu(t)}(t,n)}}bu.className="elu",e.serialization.registerClass(bu);class wu extends yu{apply(t){return e.selu(t)}}wu.className="selu",e.serialization.registerClass(wu);class ku extends yu{apply(t){return e.relu(t)}}ku.className="relu",e.serialization.registerClass(ku);class xu extends yu{apply(t){return e.tidy((()=>e.minimum(6,e.relu(t))))}}xu.className="relu6",e.serialization.registerClass(xu);class vu extends yu{apply(t){return t}}vu.className="linear",e.serialization.registerClass(vu);class Su extends yu{apply(t){return e.sigmoid(t)}}Su.className="sigmoid",e.serialization.registerClass(Su);class Iu extends yu{apply(t){return function(t){return e.tidy((()=>{const n=e.add(.5,e.mul(.2,t));return e.clipByValue(n,0,1)}))}(t)}}Iu.className="hardSigmoid",e.serialization.registerClass(Iu);class Nu extends yu{apply(t){return e.softplus(t)}}Nu.className="softplus",e.serialization.registerClass(Nu);class zu extends yu{apply(t){return function(t){return e.tidy((()=>e.div(t,e.abs(t).add(1))))}(t)}}zu.className="softsign",e.serialization.registerClass(zu);class Au extends yu{apply(t){return e.tanh(t)}}Au.className="tanh",e.serialization.registerClass(Au);class Cu extends yu{apply(t,n=-1){return e.softmax(t,n)}}Cu.className="softmax",e.serialization.registerClass(Cu);class $u extends yu{apply(t,n=-1){return e.logSoftmax(t,n)}}$u.className="logSoftmax",e.serialization.registerClass($u);class Du extends yu{apply(t,n=1){return e.tidy((()=>e.sigmoid(t.mul(n)).mul(t)))}}Du.className="swish",e.serialization.registerClass(Du);class Tu extends yu{apply(t){return e.tidy((()=>e.mul(t,e.tanh(e.softplus(t)))))}}function Eu(t){return t.getClassName()}function Fu(t,n={}){return ea(t,e.serialization.SerializationMap.getMap().classNameMap,n,"activation")}function Lu(t){if(null==t){const t={className:"linear",config:{}};return Fu(t)}if("string"==typeof t){const e={};return e.className=t,e.config={},Fu(e)}return t instanceof yu?t:Fu(t)}function _u(t){if(null!=t&&"object"!=typeof t)throw new Error(`Argument to L1L2 regularizer's constructor is expected to be an object, but received: ${t}`)}Tu.className="mish",e.serialization.registerClass(Tu);class Ru extends e.serialization.Serializable{}class Mu extends Ru{constructor(t){super(),_u(t),this.l1=null==t||null==t.l1?.01:t.l1,this.l2=null==t||null==t.l2?.01:t.l2,this.hasL1=0!==this.l1,this.hasL2=0!==this.l2}apply(t){return e.tidy((()=>{let n=e.zeros([1]);return this.hasL1&&(n=e.add(n,e.sum(e.mul(this.l1,e.abs(t))))),this.hasL2&&(n=e.add(n,e.sum(e.mul(this.l2,Ya(t))))),n.asScalar()}))}getConfig(){return{l1:this.l1,l2:this.l2}}static fromConfig(t,e){return new t({l1:e.l1,l2:e.l2})}}Mu.className="L1L2",e.serialization.registerClass(Mu);const Ou={l1l2:"L1L2"};function Bu(t){return Qr(t)}function Wu(t,n={}){return ea(t,e.serialization.SerializationMap.getMap().classNameMap,n,"regularizer")}function Pu(t){if(null==t)return null;if("string"==typeof t){return Wu({className:t in Ou?Ou[t]:t,config:{}})}return t instanceof Ru?t:Wu(t)}class Uu extends jo{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,null!=t&&(this.maxValue=t.maxValue)}call(t,n){t=Fo(t);let s=e.relu(t);return null!=this.maxValue&&(s=e.clipByValue(s,0,this.maxValue)),s}computeOutputShape(t){return t}getConfig(){const t={maxValue:this.maxValue},e=super.getConfig();return Object.assign(t,e),t}}Uu.className="ReLU",e.serialization.registerClass(Uu);class Ku extends jo{constructor(t){super(null==t?{}:t),this.DEFAULT_ALPHA=.3,null==t&&(t={}),this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,n){const s=Fo(t);return e.leakyRelu(s,this.alpha)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}Ku.className="LeakyReLU",e.serialization.registerClass(Ku);class Vu extends jo{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA_INITIALIZER="zeros",null==t&&(t={}),this.supportsMasking=!0,this.alphaInitializer=No(t.alphaInitializer||this.DEFAULT_ALPHA_INITIALIZER),this.alphaRegularizer=Pu(t.alphaRegularizer),this.alphaConstraint=wa(t.alphaConstraint),null==t.sharedAxes)this.sharedAxes=null;else if(Array.isArray(t.sharedAxes))this.sharedAxes=t.sharedAxes;else{if("number"!=typeof t.sharedAxes)throw new Ur(`Expected sharedAxes to be a number or an array of numbers, but got ${t.sharedAxes}`);this.sharedAxes=[t.sharedAxes]}}build(t){const e=(t=Lo(t)).slice(1);if(null!=this.sharedAxes)for(const t of this.sharedAxes)e[t-1]=1;this.alpha=this.addWeight("alpha",e,"float32",this.alphaInitializer,this.alphaRegularizer,!0,this.alphaConstraint);const n={};if(null!=this.sharedAxes)for(let e=1;e<t.length;++e)n[e]=t[e];this.inputSpec=[new Wo({ndim:t.length,axes:n})],this.built=!0}call(t,n){return t=Fo(t),e.prelu(t,this.alpha.read())}getConfig(){const t={alphaInitializer:Io(this.alphaInitializer),alphaRegularizer:Bu(this.alphaRegularizer),alphaConstraint:ya(this.alphaConstraint),sharedAxes:this.sharedAxes},e=super.getConfig();return Object.assign(t,e),t}}Vu.className="PReLU",e.serialization.registerClass(Vu);class ju extends jo{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA=1,null==t&&(t={}),null!=t.alpha&&t.alpha!==this.DEFAULT_ALPHA)throw new Kr(`Non-default alpha value (${t.alpha}) is not supported by the ELU layer yet.`);this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,n){const s=Fo(t);return e.elu(s)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}ju.className="ELU",e.serialization.registerClass(ju);class qu extends jo{constructor(t){super(null==t?{}:t),this.DEFAULT_THETA=1,null==t&&(t={}),this.theta=null==t.theta?this.DEFAULT_THETA:t.theta}call(t,e){const n=Fo(t);return n.mul(Pa(n.greater(this.theta),"float32"))}computeOutputShape(t){return t}getConfig(){const t={theta:this.theta},e=super.getConfig();return Object.assign(t,e),t}}qu.className="ThresholdedReLU",e.serialization.registerClass(qu);class Gu extends jo{constructor(t){super(null==t?{}:t),this.DEFAULT_AXIS=1,null==t&&(t={}),this.softmax=(new Cu).apply,this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis}call(t,e){const n=Fo(t);return this.softmax(n,this.axis)}computeOutputShape(t){return t}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function Hu(t,e,n){if("number"==typeof t)return jr(t,e);if(t.length!==e)throw new Ur(`The ${n} argument must be an integer or tuple of ${e} integers. Received: ${t.length} elements.`);for(let i=0;i<e;++i){const r=t[i];if((s=r)!==parseInt(s.toString(),10))throw new Ur(`The ${n} argument must be an integer or tuple of ${e} integers. Received: ${JSON.stringify(t)} including a non-integer number ${r}`)}return t;var s}function Ju(t,e,n,s,i=1){if(null==t)return t;let r;return r="same"===n?t:t-(e+(e-1)*(i-1))+1,Math.floor((r+s-1)/s)}function Zu(t,e,n,s){if(null==t)return null;if("valid"===s)t=t*e+Ba([n-e,0]);else{if("same"!==s)throw new Ur(`Unsupport padding mode: ${s}.`);t*=e}return t}function Xu(t,n){return e.tidy((()=>(Aa(n),"channelsFirst"===n?e.transpose(t,[0,2,3,1]):t)))}function Yu(t,n){return e.tidy((()=>(Aa(n),"channelsFirst"===n?e.transpose(t,[0,2,3,4,1]):t)))}function Qu(t,n,s,i=[1,1],r="valid",a,o,l=null){return e.tidy((()=>{if(null==a&&(a="channelsLast"),Aa(a),3!==t.rank&&4!==t.rank)throw new Ur(`conv2dWithBiasActivation expects input to be of rank 3 or 4, but received ${t.rank}.`);if(3!==n.rank&&4!==n.rank)throw new Ur(`conv2dWithBiasActivation expects kernel to be of rank 3 or 4, but received ${t.rank}.`);let u=Xu(t,a);if("causal"===r)throw new Kr("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");return u=e.fused.conv2d({x:u,filter:n,strides:i,pad:"same"===r?"same":"valid",dilations:o,dataFormat:"NHWC",bias:s,activation:l}),"channelsFirst"===a&&(u=e.transpose(u,[0,3,1,2])),u}))}Gu.className="Softmax",e.serialization.registerClass(Gu);class th extends jo{constructor(t,e){if(super(e),this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",th.verifyArgs(e),this.rank=t,oa(this.rank,"rank"),1!==this.rank&&2!==this.rank&&3!==this.rank)throw new Kr(`Convolution layer for rank other than 1, 2, or 3 (${this.rank}) is not implemented yet.`);if(this.kernelSize=Hu(e.kernelSize,t,"kernelSize"),this.strides=Hu(null==e.strides?1:e.strides,t,"strides"),this.padding=null==e.padding?"valid":e.padding,Ca(this.padding),this.dataFormat=null==e.dataFormat?"channelsLast":e.dataFormat,Aa(this.dataFormat),this.activation=Lu(e.activation),this.useBias=null==e.useBias||e.useBias,this.biasInitializer=No(e.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.biasConstraint=wa(e.biasConstraint),this.biasRegularizer=Pu(e.biasRegularizer),this.activityRegularizer=Pu(e.activityRegularizer),this.dilationRate=Hu(null==e.dilationRate?1:e.dilationRate,t,"dilationRate"),1===this.rank&&Array.isArray(this.dilationRate)&&1!==this.dilationRate.length)throw new Ur(`dilationRate must be a number or an array of a single number for 1D convolution, but received ${JSON.stringify(this.dilationRate)}`);if(2===this.rank){if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate];else if(2!==this.dilationRate.length)throw new Ur(`dilationRate must be a number or array of two numbers for 2D convolution, but received ${JSON.stringify(this.dilationRate)}`)}else if(3===this.rank)if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate,this.dilationRate];else if(3!==this.dilationRate.length)throw new Ur(`dilationRate must be a number or array of three numbers for 3D convolution, but received ${JSON.stringify(this.dilationRate)}`)}static verifyArgs(t){if(qr("kernelSize"in t,"required key 'kernelSize' not in config"),"number"!=typeof t.kernelSize&&!aa(t.kernelSize,"number",1,3))throw new Ur(`BaseConv expects config.kernelSize to be number or number[] with length 1, 2, or 3, but received ${JSON.stringify(t.kernelSize)}.`)}getConfig(){const t={kernelSize:this.kernelSize,strides:this.strides,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,activation:Eu(this.activation),useBias:this.useBias,biasInitializer:Io(this.biasInitializer),biasRegularizer:Bu(this.biasRegularizer),activityRegularizer:Bu(this.activityRegularizer),biasConstraint:ya(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}class eh extends th{constructor(t,e){super(t,e),this.kernel=null,eh.verifyArgs(e),this.filters=e.filters,oa(this.filters,"filters"),this.kernelInitializer=No(e.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.kernelConstraint=wa(e.kernelConstraint),this.kernelRegularizer=Pu(e.kernelRegularizer)}build(t){t=Lo(t);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new Ur(`The channel dimension of the input should be defined. Found ${t[e]}`);const n=t[e],s=this.kernelSize.concat([n,this.filters]);this.kernel=this.addWeight("kernel",s,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[{ndim:this.rank+2,axes:{[e]:n}}],this.built=!0}call(t,n){return e.tidy((()=>{let n;t=Fo(t);const s=null==this.bias?null:this.bias.read(),i=ua(this.activation.getClassName());if(null!=i&&2===this.rank)n=Qu(t,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate,i);else{if(1===this.rank)n=function(t,n,s,i=1,r="valid",a,o=1){return e.tidy((()=>{if(null==a&&(a="channelsLast"),Aa(a),3!==t.shape.length)throw new Ur(`The input of a conv1dWithBias operation should be 3, but is ${t.shape.length} instead.`);if(3!==n.shape.length)throw new Ur(`The kernel for a conv1dWithBias operation should be 3, but is ${n.shape.length} instead`);if(null!=s&&1!==s.shape.length)throw new Ur(`The bias for a conv1dWithBias operation should be 1, but is ${n.shape.length} instead`);if("channelsFirst"===a&&(t=e.transpose(t,[0,2,1])),"causal"===r)throw new Kr("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");let l=e.conv1d(t,n,i,"same"===r?"same":"valid","NWC",o);return null!=s&&(l=to(l,s)),l}))}(t,this.kernel.read(),s,this.strides[0],this.padding,this.dataFormat,this.dilationRate[0]);else if(2===this.rank)n=Qu(t,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate);else{if(3!==this.rank)throw new Kr("convolutions greater than 3D are not implemented yet.");n=function(t,n,s,i=[1,1,1],r="valid",a,o){return e.tidy((()=>{if(null==a&&(a="channelsLast"),Aa(a),4!==t.rank&&5!==t.rank)throw new Ur(`conv3dWithBias expects input to be of rank 4 or 5, but received ${t.rank}.`);if(4!==n.rank&&5!==n.rank)throw new Ur(`conv3dWithBias expects kernel to be of rank 4 or 5, but received ${t.rank}.`);let l=Yu(t,a);if("causal"===r)throw new Kr("The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.");return l=e.conv3d(l,n,i,"same"===r?"same":"valid","NDHWC",o),null!=s&&(l=to(l,s)),"channelsFirst"===a&&(l=e.transpose(l,[0,4,1,2,3])),l}))}(t,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate)}null!=this.activation&&(n=this.activation.apply(n))}return n}))}computeOutputShape(t){t=Lo(t);const e=[],n="channelsLast"===this.dataFormat?t.slice(1,t.length-1):t.slice(2);for(let t=0;t<n.length;++t){const s=Ju(n[t],this.kernelSize[t],this.padding,this.strides[t],"number"==typeof this.dilationRate?this.dilationRate:this.dilationRate[t]);e.push(s)}let s=[t[0]];return"channelsLast"===this.dataFormat?(s=s.concat(e),s.push(this.filters)):(s.push(this.filters),s=s.concat(e)),s}getConfig(){const t={filters:this.filters,kernelInitializer:Io(this.kernelInitializer),kernelRegularizer:Bu(this.kernelRegularizer),kernelConstraint:ya(this.kernelConstraint)},e=super.getConfig();return Object.assign(t,e),t}static verifyArgs(t){if(!("filters"in t)||"number"!=typeof t.filters||t.filters<1)throw new Ur(`Convolution layer expected config.filters to be a 'number' > 0 but got ${JSON.stringify(t.filters)}`)}}class nh extends eh{constructor(t){super(2,t),nh.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!aa(t.kernelSize,"number",1,2))throw new Ur(`Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received ${JSON.stringify(t.kernelSize)}.`)}}nh.className="Conv2D",e.serialization.registerClass(nh);class sh extends eh{constructor(t){super(3,t),sh.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&(!Array.isArray(t.kernelSize)||1!==t.kernelSize.length&&3!==t.kernelSize.length))throw new Ur(`Conv3D expects config.kernelSize to be number or [number, number, number], but received ${JSON.stringify(t.kernelSize)}.`)}}sh.className="Conv3D",e.serialization.registerClass(sh);class ih extends nh{constructor(t){if(super(t),this.inputSpec=[new Wo({ndim:4})],"same"!==this.padding&&"valid"!==this.padding)throw new Ur(`Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(4!==(t=Lo(t)).length)throw new Ur("Input should have rank 4; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new Ur("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new Wo({ndim:4,axes:{[e]:n}})],this.built=!0}call(t,n){return e.tidy((()=>{let n=Fo(t);if(4!==n.shape.length)throw new Ur(`Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${n.shape.length}`);const s=n.shape,i=s[0];let r,a;"channelsFirst"===this.dataFormat?(r=2,a=3):(r=1,a=2);const o=s[r],l=s[a],u=this.kernelSize[0],h=this.kernelSize[1],c=this.strides[0],p=this.strides[1],d=[i,Zu(o,c,u,this.padding),Zu(l,p,h,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(n=e.transpose(n,[0,2,3,1]));let f=e.conv2dTranspose(n,this.kernel.read(),d,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(f=e.transpose(f,[0,3,1,2])),null!=this.bias&&(f=to(f,this.bias.read(),this.dataFormat)),null!=this.activation&&(f=this.activation.apply(f)),f}))}computeOutputShape(t){const e=(t=Lo(t)).slice();let n,s,i;"channelsFirst"===this.dataFormat?(n=1,s=2,i=3):(n=3,s=1,i=2);const r=this.kernelSize[0],a=this.kernelSize[1],o=this.strides[0],l=this.strides[1];return e[n]=this.filters,e[s]=Zu(e[s],o,r,this.padding),e[i]=Zu(e[i],l,a,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}ih.className="Conv2DTranspose",e.serialization.registerClass(ih);class rh extends sh{constructor(t){if(super(t),this.inputSpec=[new Wo({ndim:5})],"same"!==this.padding&&"valid"!==this.padding)throw new Ur(`Conv3DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(5!==(t=Lo(t)).length)throw new Ur("Input should have rank 5; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new Ur("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new Wo({ndim:5,axes:{[e]:n}})],this.built=!0}call(t,n){return e.tidy((()=>{let n=Fo(t);if(5!==n.shape.length)throw new Ur(`Conv3DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${n.shape.length}`);const s=n.shape,i=s[0];let r,a,o;"channelsFirst"===this.dataFormat?(o=2,r=3,a=4):(o=1,r=2,a=3);const l=s[o],u=s[r],h=s[a],c=this.kernelSize[0],p=this.kernelSize[1],d=this.kernelSize[2],f=this.strides[0],g=this.strides[1],m=this.strides[2],y=[i,Zu(l,f,c,this.padding),Zu(u,g,p,this.padding),Zu(h,m,d,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(n=e.transpose(n,[0,2,3,4,1]));let b=e.conv3dTranspose(n,this.kernel.read(),y,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(b=e.transpose(b,[0,4,1,2,3])),null!==this.bias&&(b=to(b,this.bias.read(),this.dataFormat)),null!==this.activation&&(b=this.activation.apply(b)),b}))}computeOutputShape(t){const e=(t=Lo(t)).slice();let n,s,i,r;"channelsFirst"===this.dataFormat?(n=1,s=2,i=3,r=4):(n=4,s=1,i=2,r=3);const a=this.kernelSize[0],o=this.kernelSize[1],l=this.kernelSize[2],u=this.strides[0],h=this.strides[1],c=this.strides[2];return e[n]=this.filters,e[s]=Zu(e[s],u,a,this.padding),e[i]=Zu(e[i],h,o,this.padding),e[r]=Zu(e[r],c,l,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}rh.className="Conv3DTranspose",e.serialization.registerClass(rh);class ah extends eh{constructor(t,e){if(super(t,e),this.DEFAULT_DEPTHWISE_INITIALIZER="glorotUniform",this.DEFAULT_POINTWISE_INITIALIZER="glorotUniform",this.depthwiseKernel=null,this.pointwiseKernel=null,null==e.filters)throw new Ur("The `filters` configuration field is required by SeparableConv, but is unspecified.");if(null!=e.kernelInitializer||null!=e.kernelRegularizer||null!=e.kernelConstraint)throw new Ur("Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.");if(null!=e.padding&&"same"!==e.padding&&"valid"!==e.padding)throw new Ur(`SeparableConv${this.rank}D supports only padding modes: 'same' and 'valid', but received ${JSON.stringify(e.padding)}`);this.depthMultiplier=null==e.depthMultiplier?1:e.depthMultiplier,this.depthwiseInitializer=No(e.depthwiseInitializer||this.DEFAULT_DEPTHWISE_INITIALIZER),this.depthwiseRegularizer=Pu(e.depthwiseRegularizer),this.depthwiseConstraint=wa(e.depthwiseConstraint),this.pointwiseInitializer=No(e.depthwiseInitializer||this.DEFAULT_POINTWISE_INITIALIZER),this.pointwiseRegularizer=Pu(e.pointwiseRegularizer),this.pointwiseConstraint=wa(e.pointwiseConstraint)}build(t){if((t=Lo(t)).length<this.rank+2)throw new Ur(`Inputs to SeparableConv${this.rank}D should have rank ${this.rank+2}, but received input shape: ${JSON.stringify(t)}`);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e]||t[e]<0)throw new Ur(`The channel dimension of the inputs should be defined, but found ${JSON.stringify(t[e])}`);const n=t[e],s=this.kernelSize.concat([n,this.depthMultiplier]),i=[];for(let t=0;t<this.rank;++t)i.push(1);i.push(n*this.depthMultiplier,this.filters);const r=!0;this.depthwiseKernel=this.addWeight("depthwise_kernel",s,"float32",this.depthwiseInitializer,this.depthwiseRegularizer,r,this.depthwiseConstraint),this.pointwiseKernel=this.addWeight("pointwise_kernel",i,"float32",this.pointwiseInitializer,this.pointwiseRegularizer,r,this.pointwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,r,this.biasConstraint):this.bias=null,this.inputSpec=[new Wo({ndim:this.rank+2,axes:{[e]:n}})],this.built=!0}call(t,n){return e.tidy((()=>{let n;if(t=Fo(t),1===this.rank)throw new Kr("1D separable convolution is not implemented yet.");return 2===this.rank&&("channelsFirst"===this.dataFormat&&(t=e.transpose(t,[0,2,3,1])),n=e.separableConv2d(t,this.depthwiseKernel.read(),this.pointwiseKernel.read(),this.strides,this.padding,this.dilationRate,"NHWC")),this.useBias&&(n=to(n,this.bias.read(),this.dataFormat)),null!=this.activation&&(n=this.activation.apply(n)),"channelsFirst"===this.dataFormat&&(n=e.transpose(n,[0,3,1,2])),n}))}getConfig(){const t=super.getConfig();return delete t.rank,delete t.kernelInitializer,delete t.kernelRegularizer,delete t.kernelConstraint,t.depthwiseInitializer=Io(this.depthwiseInitializer),t.pointwiseInitializer=Io(this.pointwiseInitializer),t.depthwiseRegularizer=Bu(this.depthwiseRegularizer),t.pointwiseRegularizer=Bu(this.pointwiseRegularizer),t.depthwiseConstraint=ya(this.depthwiseConstraint),t.pointwiseConstraint=ya(this.pointwiseConstraint),t}}ah.className="SeparableConv";class oh extends ah{constructor(t){super(2,t)}}oh.className="SeparableConv2D",e.serialization.registerClass(oh);class lh extends eh{constructor(t){super(1,t),lh.verifyArgs(t),this.inputSpec=[{ndim:3}]}getConfig(){const t=super.getConfig();return delete t.rank,delete t.dataFormat,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!aa(t.kernelSize,"number",1,1))throw new Ur(`Conv1D expects config.kernelSize to be number or number[] with length 1, but received ${JSON.stringify(t.kernelSize)}.`)}}lh.className="Conv1D",e.serialization.registerClass(lh);class uh extends jo{constructor(t){super(t),"number"==typeof t.cropping?this.cropping=[[t.cropping,t.cropping],[t.cropping,t.cropping]]:"number"==typeof t.cropping[0]?this.cropping=[[t.cropping[0],t.cropping[0]],[t.cropping[1],t.cropping[1]]]:this.cropping=t.cropping,this.dataFormat=void 0===t.dataFormat?"channelsLast":t.dataFormat,this.inputSpec=[{ndim:4}]}computeOutputShape(t){return"channelsFirst"===this.dataFormat?[t[0],t[1],t[2]-this.cropping[0][0]-this.cropping[0][1],t[3]-this.cropping[1][0]-this.cropping[1][1]]:[t[0],t[1]-this.cropping[0][0]-this.cropping[0][1],t[2]-this.cropping[1][0]-this.cropping[1][1],t[3]]}call(t,n){return e.tidy((()=>{if(t=Fo(t),"channelsLast"===this.dataFormat){const e=ja(t,this.cropping[0][0],t.shape[1]-this.cropping[0][0]-this.cropping[0][1],2);return ja(e,this.cropping[1][0],t.shape[2]-this.cropping[1][1]-this.cropping[1][0],3)}{const e=ja(t,this.cropping[0][0],t.shape[2]-this.cropping[0][0]-this.cropping[0][1],3);return ja(e,this.cropping[1][0],t.shape[3]-this.cropping[1][1]-this.cropping[1][0],4)}}))}getConfig(){const t={cropping:this.cropping,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}uh.className="Cropping2D",e.serialization.registerClass(uh);class hh extends jo{constructor(t){var e;super(t),this.DEFAULT_SIZE=[2,2],this.inputSpec=[{ndim:4}],this.size=null==t.size?this.DEFAULT_SIZE:t.size,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,Aa(this.dataFormat),this.interpolation=null==t.interpolation?"nearest":t.interpolation,e=this.interpolation,ra(va,"InterpolationFormat",e)}computeOutputShape(t){if("channelsFirst"===this.dataFormat){const e=null==t[2]?null:this.size[0]*t[2],n=null==t[3]?null:this.size[1]*t[3];return[t[0],t[1],e,n]}{const e=null==t[1]?null:this.size[0]*t[1],n=null==t[2]?null:this.size[1]*t[2];return[t[0],e,n,t[3]]}}call(t,n){return e.tidy((()=>{let n=Fo(t);const s=n.shape;if("channelsFirst"===this.dataFormat){n=e.transpose(n,[0,2,3,1]);const t=this.size[0]*s[2],i=this.size[1]*s[3],r="nearest"===this.interpolation?n.resizeNearestNeighbor([t,i]):n.resizeBilinear([t,i]);return e.transpose(r,[0,3,1,2])}{const t=this.size[0]*s[1],e=this.size[1]*s[2];return"nearest"===this.interpolation?n.resizeNearestNeighbor([t,e]):n.resizeBilinear([t,e])}}))}getConfig(){const t={size:this.size,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}hh.className="UpSampling2D",e.serialization.registerClass(hh);class ch extends th{constructor(t){super(2,t),this.depthwiseKernel=null,this.depthMultiplier=null==t.depthMultiplier?1:t.depthMultiplier,this.depthwiseInitializer=No(t.depthwiseInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.depthwiseConstraint=wa(t.depthwiseConstraint),this.depthwiseRegularizer=Pu(t.depthwiseRegularizer)}build(t){if((t=Lo(t)).length<4)throw new Ur(`Inputs to DepthwiseConv2D should have rank 4. Received input shape: ${JSON.stringify(t)}.`);const e="channelsFirst"===this.dataFormat?1:3;if(null==t[e]||t[e]<0)throw new Ur(`The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (${t[e]}).`);const n=t[e],s=[this.kernelSize[0],this.kernelSize[1],n,this.depthMultiplier];this.depthwiseKernel=this.addWeight("depthwise_kernel",s,null,this.depthwiseInitializer,this.depthwiseRegularizer,!0,this.depthwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[n*this.depthMultiplier],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,n){return e.tidy((()=>{let n=function(t,n,s=[1,1],i="valid",r,a){return e.tidy((()=>{null==r&&(r="channelsLast"),Aa(r);let o=Xu(t,r);if(4!==t.rank)throw new Ur(`Input for depthwiseConv2d is required to be 4-D, but is instead ${t.rank}-D`);if(4!==n.rank)throw new Ur(`depthwiseKernel is required to be 4-D, but is instead ${n.rank}-D`);return o=e.depthwiseConv2d(o,n,s,"same"===i?"same":"valid","NHWC",a),"channelsFirst"===r&&(o=e.transpose(o,[0,3,1,2])),o}))}(t=Fo(t),this.depthwiseKernel.read(),this.strides,this.padding,this.dataFormat,null);return this.useBias&&(n=to(n,this.bias.read(),this.dataFormat)),null!=this.activation&&(n=this.activation.apply(n)),n}))}computeOutputShape(t){t=Lo(t);const e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[1]*this.depthMultiplier:t[3]*this.depthMultiplier,i=Ju(e,this.kernelSize[0],this.padding,this.strides[0]),r=Ju(n,this.kernelSize[1],this.padding,this.strides[1]);return"channelsFirst"===this.dataFormat?[t[0],s,i,r]:[t[0],i,r,s]}getConfig(){const t=super.getConfig();return t.depthMultiplier=this.depthMultiplier,t.depthwiseInitializer=Io(this.depthwiseInitializer),t.depthwiseRegularizer=Bu(this.depthwiseRegularizer),t.depthwiseConstraint=ya(this.depthwiseRegularizer),t}}function ph(t,e,n,s){if(Array.isArray(t)){if(null!=e||null!=n)throw new Ur("When inputs is an array, neither initialState or constants should be provided");null!=s&&(n=t.slice(t.length-s,t.length),t=t.slice(0,t.length-s)),t.length>1&&(e=t.slice(1,t.length)),t=t[0]}function i(t){return null==t||Array.isArray(t)?t:[t]}return{inputs:t,initialState:e=i(e),constants:n=i(n)}}function dh(t,n,s,i=!1,r,a,o=!1,l=!1){return e.tidy((()=>{const u=n.shape.length;if(u<3)throw new Ur(`Input should be at least 3D, but is ${u}D.`);const h=[1,0].concat(Wa(2,u));if(n=e.transpose(n,h),null!=a)throw new Kr("The rnn() functoin of the deeplearn.js backend does not support constants yet.");o&&console.warn("Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend."),null!=r&&((r=r.asType("bool").asType("float32")).rank===u-1&&(r=e.expandDims(r,-1)),r=e.transpose(r,h)),i&&(n=e.reverse(n,0),null!=r&&(r=e.reverse(r,0)));const c=[];let p,d=s;const f=n.shape[0],g=e.unstack(n);let m,y;null!=r&&(m=e.unstack(r));for(let n=0;n<f;++n){const s=g[n],i=e.tidy((()=>t(s,d)));if(null==r)p=i[0],d=i[1];else{const t=e.tidy((()=>{const t=m[n],s=e.onesLike(t).sub(t);return{output:i[0].mul(t).add(d[0].mul(s)),newStates:d.map(((e,n)=>i[1][n].mul(t).add(e.mul(s))))}}));p=t.output,d=t.newStates}l&&c.push(p)}if(l){const t=1;y=e.stack(c,t)}return[p,y,d]}))}ch.className="DepthwiseConv2D",e.serialization.registerClass(ch);class fh extends jo{constructor(t){let e;if(super(t),null==t.cell)throw new Ur("cell property is missing for the constructor of RNN.");if(e=Array.isArray(t.cell)?new vh({cells:t.cell}):t.cell,null==e.stateSize)throw new Ur("The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).");this.cell=e,this.returnSequences=null!=t.returnSequences&&t.returnSequences,this.returnState=null!=t.returnState&&t.returnState,this.goBackwards=null!=t.goBackwards&&t.goBackwards,this._stateful=null!=t.stateful&&t.stateful,this.unroll=null!=t.unroll&&t.unroll,this.supportsMasking=!0,this.inputSpec=[new Wo({ndim:3})],this.stateSpec=null,this.states_=null,this.numConstants=null,this.keptStates=[]}getStates(){if(null==this.states_){return Wa(0,Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1).map((t=>null))}return this.states_}setStates(t){this.states_=t}computeOutputShape(t){To(t)&&(t=t[0]),t=t;let e=this.cell.stateSize;Array.isArray(e)||(e=[e]);const n=e[0];let s;if(s=this.returnSequences?[t[0],t[1],n]:[t[0],n],this.returnState){const n=[];for(const s of e)n.push([t[0],s]);return[s].concat(n)}return s}computeMask(t,n){return e.tidy((()=>{Array.isArray(n)&&(n=n[0]);const t=this.returnSequences?n:null;if(this.returnState){const e=this.states.map((t=>null));return[t].concat(e)}return t}))}get states(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1,e=[];for(let n=0;n<t;++n)e.push(null);return e}return this.states_}set states(t){this.states_=t}build(t){if(null!=this.numConstants)throw new Kr("Constants support is not implemented in RNN yet.");To(t)&&(t=t[0]),t=t;const n=this.stateful?t[0]:null,s=t.slice(2);this.inputSpec[0]=new Wo({shape:[n,null,...s]});const i=[t[0]].concat(t.slice(2));let r;if(this.cell.build(i),r=Array.isArray(this.cell.stateSize)?this.cell.stateSize:[this.cell.stateSize],null!=this.stateSpec){if(!e.util.arraysEqual(this.stateSpec.map((t=>t.shape[t.shape.length-1])),r))throw new Ur(`An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=${this.stateSpec}; However cell.stateSize is ${this.cell.stateSize}`)}else this.stateSpec=r.map((t=>new Wo({shape:[null,t]})));this.stateful&&this.resetStates()}resetStates(t,n=!1){e.tidy((()=>{if(!this.stateful)throw new Wr("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape[0];if(null==s)throw new Ur("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.states_)Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>e.zeros([s,t]))):this.states_=[e.zeros([s,this.cell.stateSize])];else if(null==t)e.dispose(this.states_),null!=this.keptStates&&(e.dispose(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>e.zeros([s,t]))):this.states_[0]=e.zeros([s,this.cell.stateSize]);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new Ur(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);!0===n?this.keptStates.push(this.states_.slice()):e.dispose(this.states_);for(let n=0;n<this.states_.length;++n){const i=t[n],r=Array.isArray(this.cell.stateSize)?this.cell.stateSize[n]:this.cell.stateSize,a=[s,r];if(!e.util.arraysEqual(i.shape,a))throw new Ur(`State ${n} is incompatible with layer ${this.name}: expected shape=${a}, received shape=${i.shape}`);this.states_[n]=i}}this.states_=this.states_.map((t=>e.keep(t.clone())))}))}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=ph(t,n,s,this.numConstants);t=i.inputs,n=i.initialState,s=i.constants;let r=[],a=[];if(null!=n){e.initialState=n,r=r.concat(n),this.stateSpec=[];for(const t of n)this.stateSpec.push(new Wo({shape:t.shape}));a=a.concat(this.stateSpec)}null!=s&&(e.constants=s,r=r.concat(s),this.numConstants=s.length);if(r[0]instanceof Po){const n=[t].concat(r),s=this.inputSpec.concat(a),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,n){return e.tidy((()=>{const e=null==n?null:n.mask,s=null==n?null:n.training;let i=null==n?null:n.initialState;t=Fo(t),null==i&&(i=this.stateful?this.states_:this.getInitialState(t));const r=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;if(i.length!==r)throw new Ur(`RNN Layer has ${r} state(s) but was passed ${i.length} initial state(s).`);this.unroll&&console.warn("Ignoring unroll = true for RNN layer, due to imperative backend.");const a={training:s},o=dh(((t,e)=>{const n=this.cell.call([t].concat(e),a);return[n[0],n.slice(1)]}),t,i,this.goBackwards,e,null,this.unroll,this.returnSequences),l=o[0],u=o[1],h=o[2];this.stateful&&this.resetStates(h,s);const c=this.returnSequences?u:l;return this.returnState?[c].concat(h):c}))}getInitialState(t){return e.tidy((()=>{let n=e.zeros(t.shape);return n=e.sum(n,[1,2]),n=Ua(n),Array.isArray(this.cell.stateSize)?this.cell.stateSize.map((t=>t>1?Ha(n,[1,t]):n)):this.cell.stateSize>1?[Ha(n,[1,this.cell.stateSize])]:[n]}))}get trainableWeights(){return this.trainable?this.cell.trainableWeights:[]}get nonTrainableWeights(){return this.trainable?this.cell.nonTrainableWeights:this.cell.weights}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.cell&&this.cell.setFastWeightInitDuringBuild(t)}getConfig(){const t=super.getConfig(),e={returnSequences:this.returnSequences,returnState:this.returnState,goBackwards:this.goBackwards,stateful:this.stateful,unroll:this.unroll};null!=this.numConstants&&(e.numConstants=this.numConstants);const n=this.cell.getConfig();return this.getClassName()===fh.className&&(e.cell={className:this.cell.getClassName(),config:n}),Object.assign({},n,t,e)}static fromConfig(t,e,n={}){const s=al(e.cell,n);return new t(Object.assign(e,{cell:s}))}}fh.className="RNN",e.serialization.registerClass(fh);class gh extends jo{}class mh extends gh{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,oa(this.units,"units"),this.activation=Lu(null==t.activation?this.DEFAULT_ACTIVATION:t.activation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=No(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=No(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=No(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=Pu(t.kernelRegularizer),this.recurrentRegularizer=Pu(t.recurrentRegularizer),this.biasRegularizer=Pu(t.biasRegularizer),this.kernelConstraint=wa(t.kernelConstraint),this.recurrentConstraint=wa(t.recurrentConstraint),this.biasConstraint=wa(t.biasConstraint),this.dropout=Oa([1,Ba([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=Oa([1,Ba([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=Lo(t),this.kernel=this.addWeight("kernel",[t[t.length-1],this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,n){return e.tidy((()=>{if(2!==(t=t).length)throw new Ur(`SimpleRNNCell expects 2 input Tensors, got ${t.length}.`);let s=t[1];t=t[0];const i=null!=n.training&&n.training;let r;0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Sh({ones:()=>e.onesLike(t),rate:this.dropout,training:i})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Sh({ones:()=>e.onesLike(s),rate:this.recurrentDropout,training:i}));const a=this.dropoutMask,o=this.recurrentDropoutMask;r=Za(null!=a?e.mul(t,a):t,this.kernel.read()),null!=this.bias&&(r=to(r,this.bias.read())),null!=o&&(s=e.mul(s,o));let l=e.add(r,Za(s,this.recurrentKernel.read()));return null!=this.activation&&(l=this.activation.apply(l)),[l,l]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:Eu(this.activation),useBias:this.useBias,kernelInitializer:Io(this.kernelInitializer),recurrentInitializer:Io(this.recurrentInitializer),biasInitializer:Io(this.biasInitializer),kernelRegularizer:Bu(this.kernelRegularizer),recurrentRegularizer:Bu(this.recurrentRegularizer),biasRegularizer:Bu(this.biasRegularizer),activityRegularizer:Bu(this.activityRegularizer),kernelConstraint:ya(this.kernelConstraint),recurrentConstraint:ya(this.recurrentConstraint),biasConstraint:ya(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout};return Object.assign({},t,e)}}mh.className="SimpleRNNCell",e.serialization.registerClass(mh);class yh extends fh{constructor(t){t.cell=new mh(t),super(t)}call(t,n){return e.tidy((()=>{null!=this.cell.dropoutMask&&(e.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(e.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const s=null==n?null:n.mask,i=null==n?null:n.training,r=null==n?null:n.initialState;return super.call(t,{mask:s,training:i,initialState:r})}))}static fromConfig(t,e){return new t(e)}}yh.className="SimpleRNN",e.serialization.registerClass(yh);class bh extends gh{constructor(t){if(super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",t.resetAfter)throw new Ur("GRUCell does not support reset_after parameter set to true.");this.units=t.units,oa(this.units,"units"),this.activation=Lu(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=Lu(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=No(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=No(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=No(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=Pu(t.kernelRegularizer),this.recurrentRegularizer=Pu(t.recurrentRegularizer),this.biasRegularizer=Pu(t.biasRegularizer),this.kernelConstraint=wa(t.kernelConstraint),this.recurrentConstraint=wa(t.recurrentConstraint),this.biasConstraint=wa(t.biasConstraint),this.dropout=Oa([1,Ba([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=Oa([1,Ba([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){const e=(t=Lo(t))[t.length-1];this.kernel=this.addWeight("kernel",[e,3*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,3*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[3*this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,n){return e.tidy((()=>{if(2!==(t=t).length)throw new Ur(`GRUCell expects 2 input Tensors (inputs, h, c), got ${t.length}.`);const s=null!=n.training&&n.training;let i=t[1];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Sh({ones:()=>e.onesLike(t),rate:this.dropout,training:s,count:3})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Sh({ones:()=>e.onesLike(i),rate:this.recurrentDropout,training:s,count:3}));const r=this.dropoutMask,a=this.recurrentDropoutMask;let o,l,u;0<this.dropout&&this.dropout<1&&(t=e.mul(t,r[0]));let h=Za(t,this.kernel.read());this.useBias&&(h=to(h,this.bias.read())),0<this.recurrentDropout&&this.recurrentDropout<1&&(i=e.mul(i,a[0]));const c=this.recurrentKernel.read(),[p,d]=e.split(c,[2*this.units,this.units],c.rank-1),f=Za(i,p),[g,m,y]=e.split(h,3,h.rank-1),[b,w]=e.split(f,2,f.rank-1);o=this.recurrentActivation.apply(e.add(g,b)),l=this.recurrentActivation.apply(e.add(m,w));const k=Za(e.mul(l,i),d);u=this.activation.apply(e.add(y,k));const x=e.add(e.mul(o,i),e.mul(e.add(1,e.neg(o)),u));return[x,x]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:Eu(this.activation),recurrentActivation:Eu(this.recurrentActivation),useBias:this.useBias,kernelInitializer:Io(this.kernelInitializer),recurrentInitializer:Io(this.recurrentInitializer),biasInitializer:Io(this.biasInitializer),kernelRegularizer:Bu(this.kernelRegularizer),recurrentRegularizer:Bu(this.recurrentRegularizer),biasRegularizer:Bu(this.biasRegularizer),activityRegularizer:Bu(this.activityRegularizer),kernelConstraint:ya(this.kernelConstraint),recurrentConstraint:ya(this.recurrentConstraint),biasConstraint:ya(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation,resetAfter:!1};return Object.assign({},t,e)}}bh.className="GRUCell",e.serialization.registerClass(bh);class wh extends fh{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new bh(t),super(t)}call(t,n){return e.tidy((()=>{null!=this.cell.dropoutMask&&(e.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(e.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const s=null==n?null:n.mask,i=null==n?null:n.training,r=null==n?null:n.initialState;return super.call(t,{mask:s,training:i,initialState:r})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}wh.className="GRU",e.serialization.registerClass(wh);class kh extends gh{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,oa(this.units,"units"),this.activation=Lu(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=Lu(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=No(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=No(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=No(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.unitForgetBias=t.unitForgetBias,this.kernelRegularizer=Pu(t.kernelRegularizer),this.recurrentRegularizer=Pu(t.recurrentRegularizer),this.biasRegularizer=Pu(t.biasRegularizer),this.kernelConstraint=wa(t.kernelConstraint),this.recurrentConstraint=wa(t.recurrentConstraint),this.biasConstraint=wa(t.biasConstraint),this.dropout=Oa([1,Ba([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=Oa([1,Ba([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=[this.units,this.units],this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){var e;const n=(t=Lo(t))[t.length-1];let s;if(this.kernel=this.addWeight("kernel",[n,4*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,4*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){if(this.unitForgetBias){const t=this.biasInitializer,n=this.units;s=new((e=class extends ro{apply(e,s){const i=t.apply([n]),r=(new oo).apply([n]),a=t.apply([2*n]);return Ga(Ga(i,r),a)}}).className="CustomInit",e)}else s=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.units],null,s,this.biasRegularizer,!0,this.biasConstraint)}else this.bias=null;this.built=!0}call(t,n){return e.tidy((()=>{const s=null!=n.training&&n.training;if(3!==(t=t).length)throw new Ur(`LSTMCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);let i=t[1];const r=t[2];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Sh({ones:()=>e.onesLike(t),rate:this.dropout,training:s,count:4})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Sh({ones:()=>e.onesLike(i),rate:this.recurrentDropout,training:s,count:4}));const a=this.dropoutMask,o=this.recurrentDropoutMask;let l,u,h,c;0<this.dropout&&this.dropout<1&&(t=e.mul(t,a[0]));let p=Za(t,this.kernel.read());0<this.recurrentDropout&&this.recurrentDropout<1&&(i=e.mul(i,o[0])),p=e.add(p,Za(i,this.recurrentKernel.read())),this.useBias&&(p=to(p,this.bias.read()));const[d,f,g,m]=e.split(p,4,p.rank-1);l=this.recurrentActivation.apply(d),u=this.recurrentActivation.apply(f),h=e.add(e.mul(u,r),e.mul(l,this.activation.apply(g))),c=this.recurrentActivation.apply(m);const y=e.mul(c,this.activation.apply(h));return[y,y,h]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:Eu(this.activation),recurrentActivation:Eu(this.recurrentActivation),useBias:this.useBias,kernelInitializer:Io(this.kernelInitializer),recurrentInitializer:Io(this.recurrentInitializer),biasInitializer:Io(this.biasInitializer),unitForgetBias:this.unitForgetBias,kernelRegularizer:Bu(this.kernelRegularizer),recurrentRegularizer:Bu(this.recurrentRegularizer),biasRegularizer:Bu(this.biasRegularizer),activityRegularizer:Bu(this.activityRegularizer),kernelConstraint:ya(this.kernelConstraint),recurrentConstraint:ya(this.recurrentConstraint),biasConstraint:ya(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation};return Object.assign({},t,e)}}kh.className="LSTMCell",e.serialization.registerClass(kh);class xh extends fh{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new kh(t),super(t)}call(t,n){return e.tidy((()=>{null!=this.cell.dropoutMask&&(e.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(e.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const s=null==n?null:n.mask,i=null==n?null:n.training,r=null==n?null:n.initialState;return super.call(t,{mask:s,training:i,initialState:r})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}xh.className="LSTM",e.serialization.registerClass(xh);class vh extends gh{constructor(t){super(t),this.cells=t.cells}get stateSize(){const t=[];for(const e of this.cells.slice().reverse())Array.isArray(e.stateSize)?t.push(...e.stateSize):t.push(e.stateSize);return t}call(t,n){return e.tidy((()=>{let e=(t=t).slice(1);const s=[];for(const t of this.cells.slice().reverse())Array.isArray(t.stateSize)?s.push(e.splice(0,t.stateSize.length)):s.push(e.splice(0,1));s.reverse();const i=[];let r;for(let a=0;a<this.cells.length;++a){const o=this.cells[a];e=s[a],r=0===a?[t[0]].concat(e):[r[0]].concat(e),r=o.call(r,n),i.push(r.slice(1))}e=[];for(const t of i.slice().reverse())e.push(...t);return[r[0]].concat(e)}))}build(t){let e;To(t)&&(t=t[0]),t=t,this.cells.forEach(((n,s)=>{Ta(`RNNCell_${s}`,(()=>{n.build(t),e=Array.isArray(n.stateSize)?n.stateSize[0]:n.stateSize,t=[t[0],e]}))})),this.built=!0}getConfig(){const t=super.getConfig(),e={cells:this.cells.map((t=>({className:t.getClassName(),config:t.getConfig()})))};return Object.assign({},t,e)}static fromConfig(t,e,n={}){const s=[];for(const t of e.cells)s.push(al(t,n));return new t({cells:s})}get trainableWeights(){if(!this.trainable)return[];const t=[];for(const e of this.cells)t.push(...e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.cells)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.cells)e.push(...t.trainableWeights);return e.concat(t)}return t}getWeights(){const t=[];for(const e of this.cells)t.push(...e.weights);return Oo(t)}setWeights(t){const e=[];for(const n of this.cells){const s=n.weights.length,i=t.splice(s);for(let t=0;t<n.weights.length;++t)e.push([n.weights[t],i[t]])}Bo(e)}}function Sh(t){const{ones:n,rate:s,training:i=!1,count:r=1}=t,a=()=>eo(n(),s),o=()=>no(a,n,i);if(!r||r<=1)return e.keep(o().clone());return Array(r).fill(void 0).map(o).map((t=>e.keep(t.clone())))}vh.className="StackedRNNCells",e.serialization.registerClass(vh);class Ih extends fh{constructor(t){if(t.unroll)throw new Kr("Unrolling is not possible with convolutional RNNs.");if(Array.isArray(t.cell))throw new Kr("It is not possible at the moment to stack convolutional cells.");super(t),this.inputSpec=[new Wo({ndim:5})]}call(t,n){return e.tidy((()=>{if(null!=this.cell.dropoutMask&&(e.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(e.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),n&&n.constants)throw new Ur("ConvRNN2D cell does not support constants");const s=null==n?null:n.mask,i=null==n?null:n.training,r=null==n?null:n.initialState;return super.call(t,{mask:s,training:i,initialState:r})}))}computeOutputShape(t){let e=this.computeSingleOutputShape(t);return this.returnSequences||(e=[e[0],...e.slice(2)]),this.returnState&&(e=[e,...Array(2).fill([t[0],...e.slice(-3)])]),e}getInitialState(t){return e.tidy((()=>{const{stateSize:n}=this.cell,s=t.shape,i=this.computeSingleOutputShape(s),r=[i[0],...i.slice(2)],a=e.zeros(r);return Array.isArray(n)?Array(n.length).fill(a):[a]}))}resetStates(t,n=!1){e.tidy((()=>{if(!this.stateful)throw new Wr("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape,i=this.computeSingleOutputShape(s),r=[i[0],...i.slice(2)];if(null==s[0])throw new Ur("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.getStates())Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>e.zeros(r))):this.states_=[e.zeros(r)];else if(null==t)e.dispose(this.states_),null!=this.keptStates&&(e.dispose(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>e.zeros(r))):this.states_[0]=e.zeros(r);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new Ur(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);n?this.keptStates.push(this.states_.slice()):e.dispose(this.states_);for(let n=0;n<this.states_.length;++n){const s=t[n],i=r;if(!e.util.arraysEqual(s.shape,i))throw new Ur(`State ${n} is incompatible with layer ${this.name}: expected shape=${i}, received shape=${s.shape}`);this.states_[n]=s}}this.states_=this.states_.map((t=>e.keep(t.clone())))}))}computeSingleOutputShape(t){const{dataFormat:e,filters:n,kernelSize:s,padding:i,strides:r,dilationRate:a}=this.cell,o="channelsFirst"===e,l=t[o?3:2],u=t[o?4:3],h=Ju(l,s[0],i,r[0],a[0]),c=Ju(u,s[1],i,r[1],a[1]);return[...t.slice(0,2),...o?[n,h,c]:[h,c,n]]}}Ih.className="ConvRNN2D";class Nh extends kh{constructor(t){const{filters:e,kernelSize:n,strides:s,padding:i,dataFormat:r,dilationRate:a}=t;super(Object.assign({},t,{units:e})),this.filters=e,oa(this.filters,"filters"),this.kernelSize=Hu(n,2,"kernelSize"),this.kernelSize.forEach((t=>oa(t,"kernelSize"))),this.strides=Hu(s||1,2,"strides"),this.strides.forEach((t=>oa(t,"strides"))),this.padding=i||"valid",Ca(this.padding),this.dataFormat=r||"channelsLast",Aa(this.dataFormat),this.dilationRate=Hu(a||1,2,"dilationRate"),this.dilationRate.forEach((t=>oa(t,"dilationRate")))}build(t){var n;t=Lo(t);const s="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[s])throw new Ur(`The channel dimension of the input should be defined. Found ${t[s]}`);const i=t[s],r=this.kernelSize.concat([i,4*this.filters]);this.kernel=this.addWeight("kernel",r,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint);const a=this.kernelSize.concat([this.filters,4*this.filters]);if(this.recurrentKernel=this.addWeight("recurrent_kernel",a,null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){let t;if(this.unitForgetBias){const s=this.biasInitializer,i=this.filters;t=new((n=class extends ro{apply(t,n){return qa([s.apply([i]),e.ones([i]),s.apply([2*i])])}}).className="CustomInit",n)}else t=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.filters],null,t,this.biasRegularizer,!0,this.biasConstraint)}this.built=!0}call(t,n){return e.tidy((()=>{if(3!==t.length)throw new Ur(`ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);const s=n.training||!1,i=t[0],r=t[1],a=t[2];0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Sh({ones:()=>e.onesLike(i),rate:this.dropout,training:s,count:4}));const o=this.dropoutMask,l=(t,n,s)=>n&&n[s]?e.mul(n[s],t):t;let u=l(i,o,0),h=l(i,o,1),c=l(i,o,2),p=l(i,o,3);0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Sh({ones:()=>e.onesLike(r),rate:this.recurrentDropout,training:s,count:4}));const d=this.recurrentDropoutMask;let f=l(r,d,0),g=l(r,d,1),m=l(r,d,2),y=l(r,d,3);const[b,w,k,x]=e.split(this.kernel.read(),4,3),[v,S,I,N]=this.useBias?e.split(this.bias.read(),4):[null,null,null,null];u=this.inputConv(u,b,v,this.padding),h=this.inputConv(h,w,S,this.padding),c=this.inputConv(c,k,I,this.padding),p=this.inputConv(p,x,N,this.padding);const[z,A,C,$]=e.split(this.recurrentKernel.read(),4,3);f=this.recurrentConv(f,z),g=this.recurrentConv(g,A),m=this.recurrentConv(m,C),y=this.recurrentConv(y,$);const D=this.recurrentActivation.apply(e.add(u,f)),T=this.recurrentActivation.apply(e.add(h,g)),E=e.add(e.mul(T,a),e.mul(D,this.activation.apply(e.add(c,m)))),F=e.mul(this.recurrentActivation.apply(e.add(p,y)),this.activation.apply(E));return[F,F,E]}))}getConfig(){const t=function(t,e){var n={};for(var s in t)Object.prototype.hasOwnProperty.call(t,s)&&e.indexOf(s)<0&&(n[s]=t[s]);if(null!=t&&"function"==typeof Object.getOwnPropertySymbols){var i=0;for(s=Object.getOwnPropertySymbols(t);i<s.length;i++)e.indexOf(s[i])<0&&Object.prototype.propertyIsEnumerable.call(t,s[i])&&(n[s[i]]=t[s[i]])}return n}(super.getConfig(),["units"]),e={filters:this.filters,kernelSize:this.kernelSize,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,strides:this.strides};return Object.assign({},t,e)}inputConv(t,n,s,i){const r=e.conv2d(t,n,this.strides,i||"valid","channelsFirst"===this.dataFormat?"NCHW":"NHWC",this.dilationRate);return s?to(r,s,this.dataFormat):r}recurrentConv(t,n){return e.conv2d(t,n,1,"same","channelsFirst"===this.dataFormat?"NCHW":"NHWC")}}Nh.className="ConvLSTM2DCell",e.serialization.registerClass(Nh);class zh extends Ih{constructor(t){const e=new Nh(t);super(Object.assign({},t,{cell:e}))}static fromConfig(t,e){return new t(e)}}zh.className="ConvLSTM2D",e.serialization.registerClass(zh);class Ah extends jo{constructor(t){super(t),this.rate=Math.max(Math.min(t.rate,1),0),this.noiseShape=t.noiseShape,this.seed=t.seed,this.supportsMasking=!0}getNoiseShape(t){if(null==this.noiseShape)return this.noiseShape;const e=t.shape,n=[];for(let t=0;t<this.noiseShape.length;++t)n.push(null==this.noiseShape[t]?e[t]:this.noiseShape[t]);return n}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const e=Fo(t);if(0<this.rate&&this.rate<1){const t=null!=n.training&&n.training,s=this.getNoiseShape(e);return no((()=>eo(e,this.rate,s,this.seed)),(()=>e),t)}return t}))}getConfig(){const t={rate:this.rate,noiseShape:this.noiseShape,seed:this.seed},e=super.getConfig();return Object.assign(t,e),t}dispose(){return super.dispose()}}Ah.className="Dropout",e.serialization.registerClass(Ah);class Ch extends Ah{constructor(t){super(t),this.inputSpec=[{ndim:3}]}getNoiseShape(t){const e=t.shape;return[e[0],1,e[2]]}}Ch.className="SpatialDropout1D",e.serialization.registerClass(Ch);class $h extends jo{constructor(t){if(super(t),this.activation=null,this.useBias=!0,this.kernel=null,this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",null==t.batchInputShape&&null==t.inputShape&&null!=t.inputDim){let e=null;null!=t.batchSize&&(e=t.batchSize),this.batchInputShape=[e,t.inputDim]}this.units=t.units,oa(this.units,"units"),this.activation=Lu(t.activation),null!=t.useBias&&(this.useBias=t.useBias),this.kernelInitializer=No(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.biasInitializer=No(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelConstraint=wa(t.kernelConstraint),this.biasConstraint=wa(t.biasConstraint),this.kernelRegularizer=Pu(t.kernelRegularizer),this.biasRegularizer=Pu(t.biasRegularizer),this.activityRegularizer=Pu(t.activityRegularizer),this.supportsMasking=!0,this.inputSpec=[{minNDim:2}]}build(t){const e=(t=Lo(t))[t.length-1];null==this.kernel&&(this.kernel=this.addWeight("kernel",[e,this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint))),this.inputSpec=[{minNDim:2,axes:{[-1]:e}}],this.built=!0}computeOutputShape(t){const e=(t=Lo(t)).slice();return e[e.length-1]=this.units,e}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const e=Fo(t),s=ua(this.activation.getClassName());let i;return null!=s?i=Za(e,this.kernel.read(),s,this.bias?this.bias.read():null):(i=Za(e,this.kernel.read()),null!=this.bias&&(i=to(i,this.bias.read())),null!=this.activation&&(i=this.activation.apply(i))),i}))}getConfig(){const t={units:this.units,activation:Eu(this.activation),useBias:this.useBias,kernelInitializer:Io(this.kernelInitializer),biasInitializer:Io(this.biasInitializer),kernelRegularizer:Bu(this.kernelRegularizer),biasRegularizer:Bu(this.biasRegularizer),activityRegularizer:Bu(this.activityRegularizer),kernelConstraint:ya(this.kernelConstraint),biasConstraint:ya(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}$h.className="Dense",e.serialization.registerClass($h);class Dh extends jo{constructor(t){super(t=t||{}),this.inputSpec=[{minNDim:3}],this.dataFormat=t.dataFormat}computeOutputShape(t){t=Lo(t);for(const e of t.slice(1))if(null==e)throw new Ur(`The shape of the input to "Flatten" is not fully defined (got ${t.slice(1)}). Make sure to pass a complete "input_shape" or "batch_input_shape" argument to the first layer in your model.`);return[t[0],Ra(t,1)]}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);let e=Fo(t);if("channelsFirst"===this.dataFormat&&e.rank>1){const t=[0];for(let n=2;n<e.rank;++n)t.push(n);t.push(1),e=e.transpose(t)}return function(t){if(t.rank<=1)throw new Ur(`batchFlatten requires a minimum rank of 2. Got rank: ${t.rank}.`);const e=[t.shape[0],Ra(t.shape,1)];return t.reshape(e)}(e)}))}getConfig(){const t={};null!=this.dataFormat&&(t.dataFormat=this.dataFormat);const e=super.getConfig();return Object.assign(t,e),t}}Dh.className="Flatten",e.serialization.registerClass(Dh);class Th extends jo{constructor(t){super(t),this.supportsMasking=!0,this.activation=Lu(t.activation)}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const e=Fo(t);return this.activation.apply(e)}))}getConfig(){const t={activation:Eu(this.activation)},e=super.getConfig();return Object.assign(t,e),t}}Th.className="Activation",e.serialization.registerClass(Th);class Eh extends jo{constructor(t){super(t),this.n=t.n,this.inputSpec=[{ndim:2}]}computeOutputShape(t){return[t[0],this.n,t[1]]}call(t,n){return e.tidy((()=>{return t=Fo(t),n=t,s=this.n,e.tidy((()=>{if(2!==n.shape.length)throw new Ur(`repeat() expects a rank-2 tensor, but received a rank-${n.shape.length} tensor.`);return Ha(Ua(n,1),[1,s,1])}));var n,s}))}getConfig(){const t={n:this.n},e=super.getConfig();return Object.assign(t,e),t}}Eh.className="RepeatVector",e.serialization.registerClass(Eh);class Fh extends jo{constructor(t){super(t),this.targetShape=t.targetShape;for(let t=0;t<this.targetShape.length;++t)this.isUnknown(this.targetShape[t])&&(this.targetShape[t]=null)}isUnknown(t){return t<0||null==t}fixUnknownDimension(t,e){const n="Total size of new array must be unchanged.",s=e.slice();let i=1,r=null;for(let t=0;t<s.length;++t){const e=s[t];if(this.isUnknown(e)){if(null!==r)throw new Ur("Can only specifiy one unknown dimension.");r=t}else i*=e}const a=Ra(t);if(null!==r){if(0===i||a%i!=0)throw new Ur(n);s[r]=a/i}else if(a!==i)throw new Ur(n);return s}computeOutputShape(t){let e=!1;for(let n=0;n<t.length;++n)if(this.isUnknown(t[n])){e=!0;break}return e?t.slice(0,1).concat(this.targetShape):t.slice(0,1).concat(this.fixUnknownDimension(t.slice(1),this.targetShape))}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const e=Fo(t),s=e.shape,i=s.slice(0,1).concat(this.fixUnknownDimension(s.slice(1),this.targetShape));return e.reshape(i)}))}getConfig(){const t={targetShape:this.targetShape},e=super.getConfig();return Object.assign(t,e),t}}Fh.className="Reshape",e.serialization.registerClass(Fh);class Lh extends jo{constructor(t){if(super(t),null==t.dims)throw new Error("Required configuration field `dims` is missing during Permute constructor call.");if(!Array.isArray(t.dims))throw new Error(`Permute constructor requires \`dims\` to be an Array, but received ${t.dims} instead.`);const n=Wa(1,t.dims.length+1);if(!e.util.arraysEqual(t.dims.slice().sort(),n))throw new Error("Invalid permutation `dims`: "+JSON.stringify(t.dims)+" `dims` must contain consecutive integers starting from 1.");this.dims=t.dims,this.dimsIncludingBatch=[0].concat(this.dims),this.inputSpec=[new Wo({ndim:this.dims.length+1})]}computeOutputShape(t){const e=(t=Lo(t)).slice();return this.dims.forEach(((n,s)=>{e[s+1]=t[n]})),e}call(t,n){return e.transpose(Fo(t),this.dimsIncludingBatch)}getConfig(){const t={dims:this.dims},e=super.getConfig();return Object.assign(t,e),t}}Lh.className="Permute",e.serialization.registerClass(Lh);class _h extends jo{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,this.maskValue=null!=t?null==t.maskValue?0:t.maskValue:0}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={maskValue:this.maskValue};return Object.assign(e,t),e}computeMask(t,n){const s=Fo(t);return e.any(e.notEqual(s,this.maskValue),-1)}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const s=Fo(t),i=e.any(e.notEqual(s,this.maskValue),-1,!0);return s.mul(i.asType(s.dtype))}))}}_h.className="Masking",e.serialization.registerClass(_h);class Rh extends jo{constructor(t){if(super(t),this.embeddings=null,this.DEFAULT_EMBEDDINGS_INITIALIZER="randomUniform",null==t.batchInputShape&&null==t.inputShape){let e=null;null!=t.batchSize&&(e=t.batchSize),null==t.inputLength?this.batchInputShape=[e,null]:this.batchInputShape=[e].concat(Jr(t.inputLength))}this.inputDim=t.inputDim,oa(this.inputDim,"inputDim"),this.outputDim=t.outputDim,oa(this.outputDim,"outputDim"),this.embeddingsInitializer=No(t.embeddingsInitializer||this.DEFAULT_EMBEDDINGS_INITIALIZER),this.embeddingsRegularizer=Pu(t.embeddingsRegularizer),this.activityRegularizer=Pu(t.activityRegularizer),this.embeddingsConstraint=wa(t.embeddingsConstraint),this.maskZero=t.maskZero,this.supportsMasking=t.maskZero,this.inputLength=t.inputLength}build(t){this.embeddings=this.addWeight("embeddings",[this.inputDim,this.outputDim],this.dtype,this.embeddingsInitializer,this.embeddingsRegularizer,!0,this.embeddingsConstraint),this.built=!0}warnOnIncompatibleInputShape(t){}computeMask(t,n){return e.tidy((()=>this.maskZero?(t=Fo(t),e.notEqual(t,e.zerosLike(t))):null))}computeOutputShape(t){if(t=Lo(t),null==this.inputLength)return[...t,this.outputDim];const e=Jr(this.inputLength);if(e.length!==t.length-1)throw new Ur(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);{let n=0;for(let s=0;s<e.length;++s){const i=e[s],r=t[s+1];if(null!=i&&null!=r&&i!==r)throw new Ur(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);null==i&&(e[n]=r),n++}}return[t[0],...e,this.outputDim]}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);let e=Fo(t);"int32"!==e.dtype&&(e=Pa(e,"int32"));return Xa(this.embeddings.read(),e.as1D()).reshape(Lo(this.computeOutputShape(e.shape)))}))}getConfig(){const t={inputDim:this.inputDim,outputDim:this.outputDim,embeddingsInitializer:Io(this.embeddingsInitializer),embeddingsRegularizer:Bu(this.embeddingsRegularizer),activityRegularizer:Bu(this.activityRegularizer),embeddingsConstraint:ya(this.embeddingsConstraint),maskZero:this.maskZero,inputLength:this.inputLength},e=super.getConfig();return Object.assign(t,e),t}}Rh.className="Embedding",e.serialization.registerClass(Rh);class Mh extends jo{constructor(t){super(t||{}),this.supportsMasking=!0}mergeFunction(t){throw new Kr}computeElementwiseOpOutputShape(t,e){if(null==t||null==e)return null;if(t.length<e.length)return this.computeElementwiseOpOutputShape(e,t);if(0===e.length)return t;const n=t.slice(0,t.length-e.length);for(let s=0;s<e.length;++s){const i=t[t.length-e.length+s],r=e[s];if(null==i||null==r||i<0||r<0)n.push(null);else if(1===i)n.push(r);else if(1===r)n.push(i);else{if(i!==r)throw new Ur("Operands could not be broadcast together with shapes "+JSON.stringify(t)+" "+JSON.stringify(e));n.push(i)}}return n}build(t){if(Array.isArray(t)&&!Array.isArray(t[0])&&(t=[Lo(t)]),(t=t).length<2)throw new Ur(`A merge layer should be called on an Array of at least 2 inputs. Got ${t.length} input(s).`);let e=[];for(const n of t)null!=n&&null!==n[0]&&e.push(n[0]);if(e=sa(e),e.length>1)throw new Ur(`Can not merge tensors with different batch sizes. Got tensors with shapes: ${JSON.stringify(t)}.`);let n=null==t[0]?null:t[0].slice(1);for(let e=1;e<t.length;++e){const s=null==t[e]?null:t[e].slice(1);n=this.computeElementwiseOpOutputShape(n,s)}const s=t.map((t=>t.length));-1===t.indexOf(null)&&1===sa(s).length?this.reshapeRequired=!1:this.reshapeRequired=!0}call(t,n){return e.tidy((()=>{if(t=t,this.reshapeRequired){const n=[],s=t.map((t=>t.rank));if(-1===s.indexOf(null)){const e=Ba(s);for(let s of t){const t=s.rank;for(let n=0;n<e-t;++n)s=Ua(s,1);n.push(s)}return this.mergeFunction(n)}{let s=!1;for(const i of t){const t=i.rank;if(null==t){const t=i.shape,r=t[0],a=t.slice(1).concat([r]);let o=i.reshape([r].concat(Ra(t.slice(1))));o=e.transpose(o,[1,0]),o=o.reshape(a),n.push(o),s=!0}else if(t>1){const r=Wa(1,t).concat([0]);n.push(e.transpose(i,r)),s=!0}else n.push(i)}let i=this.mergeFunction(n);const r=i.rank;if(s)if(null==r){const t=i.shape,n=t[t.length-1],s=[n].concat(t.slice(0,t.length-1));i=e.transpose(i.reshape([-1,n]),[1,0]).reshape(s)}else if(r>1){const t=[r-1].concat(Wa(0,r-1));i=e.transpose(i,t)}return i}}return this.mergeFunction(t)}))}computeOutputShape(t){let e;e=null==(t=t)[0]?null:t[0].slice(1);for(let n=1;n<t.length;++n){const s=null==t[n]?null:t[n].slice(1);e=this.computeElementwiseOpOutputShape(e,s)}let n=[];for(const e of t)null!=e&&null!==e[0]&&n.push(e[0]);return n=sa(n),e=1===n.length?n.concat(e):[null].concat(e),e}computeMask(t,n){return e.tidy((()=>{if(null==n)return null;if(!Array.isArray(n))throw new Ur("`mask` should be an Array");if(!Array.isArray(t))throw new Ur("`inputs` should be an Array");if(n.length!==t.length)throw new Ur(`The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (${t.length} vs ${n.length})`);if(n.every((t=>null==t)))return null;let s=(n=n.map((t=>null==t?t:e.expandDims(t,0))))[0];for(let t=1;t<n.length-1;++t)s=e.logicalAnd(s,n[t]);return s}))}}class Oh extends Mh{constructor(t){super(t)}mergeFunction(t){return e.tidy((()=>{let n=t[0].clone();for(let s=1;s<t.length;++s)n=e.add(n,t[s]);return n}))}}Oh.className="Add",e.serialization.registerClass(Oh);class Bh extends Mh{constructor(t){super(t)}mergeFunction(t){return e.tidy((()=>{let n=t[0].clone();for(let s=1;s<t.length;++s)n=e.mul(n,t[s]);return n}))}}Bh.className="Multiply",e.serialization.registerClass(Bh);class Wh extends Mh{constructor(t){super(t)}mergeFunction(t){return e.tidy((()=>{let n=t[0].clone();for(let s=1;s<t.length;++s)n=e.add(n,t[s]);return e.mul(1/t.length,n)}))}}Wh.className="Average",e.serialization.registerClass(Wh);class Ph extends Mh{constructor(t){super(t)}mergeFunction(t){return e.tidy((()=>{let n=t[0];for(let s=1;s<t.length;++s)n=e.maximum(n,t[s]);return n}))}}Ph.className="Maximum",e.serialization.registerClass(Ph);class Uh extends Mh{constructor(t){super(t)}mergeFunction(t){return e.tidy((()=>{let n=t[0];for(let s=1;s<t.length;++s)n=e.minimum(n,t[s]);return n}))}}Uh.className="Minimum",e.serialization.registerClass(Uh);class Kh extends Mh{constructor(t){super(t),this.DEFAULT_AXIS=-1,null==t&&(t={}),this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){if(!Array.isArray(t)||!Array.isArray(t[0])||1===t.length)throw new Ur("A `Concatenate` layer should be called on a list of at least 2 inputs");t=t;let n=!0;for(const e of t)if(null!=e){n=!1;break}if(n)return;const s=[];for(let n=0;n<t.length;++n){const i=t[n].slice();i.splice(this.axis,1);let r=!1;for(const t of s)if(e.util.arraysEqual(t,i)){r=!0;break}r||s.push(i)}if(s.length>1)throw new Ur("A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: "+JSON.stringify(t))}mergeFunction(t){return e.tidy((()=>qa(t,this.axis)))}computeOutputShape(t){if(!Array.isArray(t)||!Array.isArray(t[0]))throw new Ur("A `Concatenate` layer should be called on a list of inputs.");const e=t,n=e[0].slice(),s=this.axis<0?n.length+this.axis:this.axis;for(const t of e.slice(1)){if(null==n[s]||null==t[s]){n[s]=null;break}n[s]+=t[s]}return n}computeMask(t,n){if(null==n)return null;if(!Array.isArray(n))throw new Ur("`mask` should be an array for Concatenate");if(!Array.isArray(t))throw new Ur("`inputs` should be an array for Concatenate");if(n.length!==t.length)throw new Ur(`Mismatch in the length of mask (${n.length}) and the legnth of inputs (${t.length})`);return e.tidy((()=>{let s=!0;if(n.forEach((t=>{null==t||(s=!1)})),s)return null;const i=[];for(let s=0;s<t.length;++s)null==n[s]?i.push(e.onesLike(t[s]).asType("bool")):n[s].rank<t[s].rank?i.push(e.expandDims(n[s],-1)):i.push(n[s]);const r=e.concat(i,this.axis);return e.all(r,-1,!1)}))}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function Vh(t,e){for(;t<0;)t+=e;return t}Kh.className="Concatenate",e.serialization.registerClass(Kh);class jh extends Mh{constructor(t){super(t),this.axes=t.axes,this.normalize=null!=t.normalize&&t.normalize,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){e.util.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const n=t[0],s=t[1];if(n.length>3||s.length>3)throw new Kr("Dot layer does not support tensors of 4D or higher rank yet.");const i=this.interpretAxes(n,s);if(n[i[0]]!==s[i[1]])throw new Ur(`Dimension incompatibility: ${n[i[0]]} !== ${s[i[1]]}`)}mergeFunction(t){if(2!==t.length)throw new Ur(`A \`Dot\` layer must be called on exactly 2 inputs, but received ${t.length} input(s).`);let n,s=t[0],i=t[1];return n=Array.isArray(this.axes)?this.axes.map(((e,n)=>Vh(e,t[n].shape.length))):[Vh(this.axes,s.shape.length),Vh(this.axes,i.shape.length)],this.normalize&&(s=ol(s,n[0]),i=ol(i,n[1])),function(t,n,s){if(t.shape.length>3||n.shape.length>3)throw new Kr("batchDot is not implemented for tensors of 4D or higher rank yet");if(e.util.assert(t.shape.length>=2,(()=>`batchDot requires the rank of x to be >= 2, but got ${t.shape.length}`)),e.util.assert(t.shape.length>=2,(()=>`batchDot requires the rank of y to be >= 2, but got ${n.shape.length}`)),"number"==typeof s&&(s=[s,s]),"complex64"===t.dtype||"complex64"===n.dtype)throw new Kr("batchDot is not implemented for complex64-type Tensors yet.");const i=t.shape.length,r=n.shape.length;null==s&&(s=[i-1,r-2]);const a=s;return e.tidy((()=>{let e,s;if(i>r){e=i-r;const t=[];for(let n=0;n<e;++n)t.push(1);n=n.reshape(n.shape.concat(t))}else if(r>i){e=r-i;const n=[];for(let t=0;t<e;++t)n.push(1);t=t.reshape(t.shape.concat(n))}else e=0;if(2===t.shape.length&&2===n.shape.length)s=a[0]===a[1]?t.mul(n).sum(a[0]):t.transpose([1,0]).mul(n).sum(a[1]);else{const e=a[0]!==t.shape.length-1,i=a[1]===n.shape.length-1;s=t.matMul(n,e,i)}if(e>0){let t;t=i>r?i+r-3:i-1;const n=[];for(let s=t;s<t+e;++s)n.push(s);s=s.squeeze(n)}return 1===s.shape.length&&(s=s.expandDims(1)),s}))}(s,i,n)}interpretAxes(t,e){let n;return n=Array.isArray(this.axes)?this.axes:[Vh(this.axes,t.length),Vh(this.axes,e.length)],n}computeOutputShape(t){e.util.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const n=t[0].slice(),s=t[1].slice();if(n.length>3||s.length>3)throw new Kr("Dot layer does not support tensors of 4D or higher rank yet.");const i=this.interpretAxes(n,s);n.splice(i[0],1),s.splice(i[1],1),s.splice(0,1);const r=n.concat(s);return 1===r.length&&r.push(1),r}computeMask(t,e){return null}getConfig(){const t={axes:this.axes,normalize:this.normalize},e=super.getConfig();return Object.assign(t,e),t}}jh.className="Dot",e.serialization.registerClass(jh);class qh extends jo{constructor(t){super(t),this.supportsMasking=!0,this.stddev=t.stddev}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={stddev:this.stddev};return Object.assign(e,t),e}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const e=Fo(t);return no((()=>Ja(e.shape,0,this.stddev).add(e)),(()=>e),n.training||!1)}))}}qh.className="GaussianNoise",e.serialization.registerClass(qh);class Gh extends jo{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const e=Fo(t);if(this.rate>0&&this.rate<1){return no((()=>{const t=Math.sqrt(this.rate/(1-this.rate));return e.mul(Ja(e.shape,1,t))}),(()=>e),n.training||!1)}return e}))}}Gh.className="GaussianDropout",e.serialization.registerClass(Gh);class Hh extends jo{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate,this.noiseShape=t.noiseShape}_getNoiseShape(t){return this.noiseShape||Fo(t).shape}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,n){return e.tidy((()=>{if(this.rate<1&&this.rate>0){const s=this._getNoiseShape(t);return no((()=>{const n=Fo(t),i=-1.7580993408473766;let r=e.greaterEqual(e.randomUniform(s),this.rate);r=Pa(r,"float32");const a=((1-this.rate)*(1+this.rate*i**2))**-.5,o=-a*i*this.rate;return n.mul(r).add(r.add(-1).mul(i)).mul(a).add(o)}),(()=>Fo(t)),n.training||!1)}return t}))}}function Jh(t,n,s,i,r,a=.001){let o;if(2===t.rank)o=e.batchNorm2d(t,n,s,i,r,a);else if(3===t.rank)o=e.batchNorm3d(t,n,s,i,r,a);else{if(4!==t.rank)throw new Kr(`batchNormalization is not implemented for array of rank ${t.rank} yet`);o=e.batchNorm4d(t,n,s,i,r,a)}return o}function Zh(t,n,s,i,r=.001){return e.util.arraysEqual(i.slice().sort(),Wa(0,t.rank-1))?function(t,n,s,i,r=.001){return e.tidy((()=>{const a=e.moments(t,i),o=a.mean,l=a.variance;return[Jh(t,o,l,s,n,r),o,l]}))}(t,n,s,i,r):function(t,n,s,i,r=.001){return e.tidy((()=>{const a=e.moments(t,i),o=a.mean,l=a.variance,u=[];for(const e of Wa(0,t.rank))-1!==i.indexOf(e)?u.push(1):u.push(t.shape[e]);const h=o.reshape(u),c=l.reshape(u),p=null==n?null:n.reshape(u),d=null==s?null:s.reshape(u);return[Jh(t,h,c,d,p,r),o,l]}))}(t,n,s,i,r)}Hh.className="AlphaDropout",e.serialization.registerClass(Hh);class Xh extends jo{constructor(t){null==t&&(t={}),super(t),this.supportsMasking=!0,this.axis=null==t.axis?-1:t.axis,this.momentum=null==t.momentum?.99:t.momentum,this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=No(t.betaInitializer||"zeros"),this.gammaInitializer=No(t.gammaInitializer||"ones"),this.movingMeanInitializer=No(t.movingMeanInitializer||"zeros"),this.movingVarianceInitializer=No(t.movingVarianceInitializer||"ones"),this.betaConstraint=wa(t.betaConstraint),this.gammaConstraint=wa(t.gammaConstraint),this.betaRegularizer=Pu(t.betaRegularizer),this.gammaRegularizer=Pu(t.gammaRegularizer)}build(t){t=Lo(t);const e=this.axis>=0?this.axis:this.axis+t.length,n=t[e];if(null==n)throw new Ur(`Axis ${e} of input tensor should have a defined dimension but the layer received an input with shape ${JSON.stringify(t)}.`);this.inputSpec=[new Wo({ndim:t.length,axes:{[e]:n}})];const s=[n];this.scale&&(this.gamma=this.addWeight("gamma",s,null,this.gammaInitializer,this.gammaRegularizer,!0,this.gammaConstraint)),this.center&&(this.beta=this.addWeight("beta",s,null,this.betaInitializer,this.betaRegularizer,!0,this.betaConstraint)),this.movingMean=this.addWeight("moving_mean",s,null,this.movingMeanInitializer,null,!1),this.movingVariance=this.addWeight("moving_variance",s,null,this.movingVarianceInitializer,null,!1),this.built=!0}call(t,n){return e.tidy((()=>{const s=null!=n.training&&n.training,i=Fo(t),r=i.shape,a=r.length,o=Wa(0,a),l=this.axis>=0?this.axis:this.axis+a;o.splice(l,1);const u=jr(1,a);u[l]=r[l];const h=o.slice();h.sort();const c=!e.util.arraysEqual(h,Wa(0,a).slice(0,a-1));if(!s)return(()=>{if(c){const t=this.movingMean.read().reshape(u),e=this.movingVariance.read().reshape(u),n=this.center?this.beta.read().reshape(u):null,s=this.scale?this.gamma.read().reshape(u):null;return Jh(i,t,e,n,s,this.epsilon)}return Jh(i,this.movingMean.read(),this.movingVariance.read(),null==this.beta?null:this.beta.read(),null==this.gamma?null:this.gamma.read(),this.epsilon)})();const[p,d,f]=Zh(i,this.gamma.read(),this.beta.read(),o,this.epsilon),g=(t,n,s)=>{e.tidy((()=>{const e=1-s,i=t.read(),r=i.sub(n).mul(e);t.write(i.sub(r))}))};return(()=>{g(this.movingMean,d,this.momentum),g(this.movingVariance,f,this.momentum)})(),p}))}getConfig(){const t={axis:this.axis,momentum:this.momentum,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:Io(this.betaInitializer),gammaInitializer:Io(this.gammaInitializer),movingMeanInitializer:Io(this.movingMeanInitializer),movingVarianceInitializer:Io(this.movingVarianceInitializer),betaRegularizer:Bu(this.betaRegularizer),gammaRegularizer:Bu(this.gammaRegularizer),betaConstraint:ya(this.betaConstraint),gammaConstraint:ya(this.gammaConstraint)},e=super.getConfig();return Object.assign(t,e),t}}Xh.className="BatchNormalization",e.serialization.registerClass(Xh);class Yh extends jo{constructor(t){if(null==t&&(t={}),super(t),this.axis=null==t.axis?-1:t.axis,"number"==typeof this.axis){if(!Number.isInteger(this.axis))throw new Error(`Expected axis to be an integer, but received ${this.axis}`)}else{if(!Array.isArray(this.axis))throw new Error(`Expected axis to be an integer or an array of integers, but received ${JSON.stringify(this.axis)}`);for(const t of this.axis)if(!Number.isInteger(t))throw new Error(`Expected axis to be an array of integers, but received ${JSON.stringify(this.axis)}`)}this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=No(t.betaInitializer||"zeros"),this.gammaInitializer=No(t.gammaInitializer||"ones"),this.betaRegularizer=Pu(t.betaRegularizer),this.gammaRegularizer=Pu(t.gammaRegularizer),this.supportsMasking=!0}build(t){const e=(t=Lo(t)).length;"number"==typeof this.axis&&(this.axis=[this.axis]);for(let t=0;t<this.axis.length;++t)this.axis[t]<0&&(this.axis[t]+=e);for(const t of this.axis)if(t<0||t>=e)throw new Error(`Invalid axis: ${t}`);if(this.axis.length!==sa(this.axis).length)throw new Error(`Found duplicate axes in: ${this.axis}`);const n=this.axis.map((e=>t[e]));this.scale?this.gamma=this.addWeight("gamma",n,"float32",this.gammaInitializer,this.gammaRegularizer,true):this.gamma=null,this.center?this.beta=this.addWeight("beta",n,"float32",this.betaInitializer,this.betaRegularizer,true):this.beta=null,this.built=!0}call(t,n){const s=Fo(t),i=s.shape,r=i.length;return e.tidy((()=>{let{mean:t,variance:n}=e.moments(s,this.axis,!0);const a=jr(1,r);for(const t of this.axis)a[t]=i[t];const o=t=>null!=t&&t.shape.length!==r&&this.axis!==[r-1]?t.reshape(a):t;let l=o(this.gamma.read()),u=o(this.beta.read());const h=[],c=[];for(let t=0;t<r;++t)-1!==this.axis.indexOf(t)?(h.push(i[t]),c.push(1)):(h.push(1),c.push(i[t]));return t=t.tile(h),n=n.tile(h),l=l.tile(c),u=u.tile(c),Jh(s,t,n,u,l,this.epsilon)}))}getConfig(){const t={axis:this.axis,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:Io(this.betaInitializer),gammaInitializer:Io(this.gammaInitializer),betaRegularizer:Bu(this.betaRegularizer),gammaRegularizer:Bu(this.gammaRegularizer)},e=super.getConfig();return Object.assign(t,e),t}}Yh.className="LayerNormalization",e.serialization.registerClass(Yh);class Qh extends jo{constructor(t){if(null==t&&(t={}),super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,null==t.padding)this.padding=[[1,1],[1,1]];else if("number"==typeof t.padding)this.padding=[[t.padding,t.padding],[t.padding,t.padding]];else{if(t.padding=t.padding,2!==t.padding.length)throw new Ur(`ZeroPadding2D expects padding to be a length-2 array, but received a length-${t.padding.length} array.`);let e,n;if("number"==typeof t.padding[0])e=[t.padding[0],t.padding[0]],n=[t.padding[1],t.padding[1]];else{if(t.padding=t.padding,2!==t.padding[0].length)throw new Ur(`ZeroPadding2D expects height padding to be a length-2 array, but received a length-${t.padding[0].length} array.`);if(e=t.padding[0],2!==t.padding[1].length)throw new Ur(`ZeroPadding2D expects width padding to be a length-2 array, but received a length-${t.padding[1].length} array.`);n=t.padding[1]}this.padding=[e,n]}this.inputSpec=[new Wo({ndim:4})]}computeOutputShape(t){let e,n;return t=Lo(t),"channelsFirst"===this.dataFormat?(e=null!=t[2]&&t[2]>=0?t[2]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[3]&&t[3]>=0?t[3]+this.padding[1][0]+this.padding[1][1]:null,[t[0],t[1],e,n]):(e=null!=t[1]&&t[1]>=0?t[1]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[2]&&t[2]>=0?t[2]+this.padding[1][0]+this.padding[1][1]:null,[t[0],e,n,t[3]])}call(t,n){return e.tidy((()=>{return n=Fo(t),s=this.padding,i=this.dataFormat,e.tidy((()=>{if(4!==n.rank)throw new Ur(`temporalPadding expects input tensor to be 4-D, but received a ${n.rank}-D tensor.`);if(null==s&&(s=[[1,1],[1,1]]),2!==s.length||2!==s[0].length||2!==s[1].length)throw new Ur("spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.");if(null==i&&(i="channelsLast"),"channelsLast"!==i&&"channelsFirst"!==i)throw new Ur(`Unknown data format: ${i}. Supported data formats are 'channelsLast' and 'channelsFirst.`);let t;return t="channelsFirst"===i?[[0,0],[0,0],s[0],s[1]]:[[0,0],s[0],s[1],[0,0]],e.pad(n,t)}));var n,s,i}))}getConfig(){const t={padding:this.padding,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}function tc(t,n,s,i,r,a){return e.tidy((()=>{let o;Aa(r),$a(a),Ca(i),null==s&&(s=[1,1]),null==i&&(i="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),t=Xu(t,r);const l="same"===i?"same":"valid";return o="max"===a?e.maxPool(t,n,s,l):e.avgPool(t,n,s,l),"channelsFirst"===r&&(o=e.transpose(o,[0,3,1,2])),o}))}function ec(t,n,s,i,r,a){return e.tidy((()=>{let o;Aa(r),$a(a),Ca(i),null==s&&(s=[1,1,1]),null==i&&(i="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),t=Yu(t,r);const l="same"===i?"same":"valid";return o="max"===a?e.maxPool3d(t,n,s,l):e.avgPool3d(t,n,s,l),"channelsFirst"===r&&(o=e.transpose(o,[0,4,1,2,3])),o}))}Qh.className="ZeroPadding2D",e.serialization.registerClass(Qh);class nc extends jo{constructor(t){if(null==t.poolSize&&(t.poolSize=2),super(t),"number"==typeof t.poolSize)this.poolSize=[t.poolSize];else{if(!Array.isArray(t.poolSize)||1!==t.poolSize.length||"number"!=typeof t.poolSize[0])throw new Ur(`poolSize for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(t.poolSize)}`);this.poolSize=t.poolSize}if(oa(this.poolSize,"poolSize"),null==t.strides)this.strides=this.poolSize;else if("number"==typeof t.strides)this.strides=[t.strides];else{if(!Array.isArray(t.strides)||1!==t.strides.length||"number"!=typeof t.strides[0])throw new Ur(`strides for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(t.strides)}`);this.strides=t.strides}oa(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,Ca(this.padding),this.inputSpec=[new Wo({ndim:3})]}computeOutputShape(t){const e=Ju((t=Lo(t))[1],this.poolSize[0],this.padding,this.strides[0]);return[t[0],e,t[2]]}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n),t=Ua(Fo(t),2);const s=this.poolingFunction(Fo(t),[this.poolSize[0],1],[this.strides[0],1],this.padding,"channelsLast");return e.squeeze(s,[2])}))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides},e=super.getConfig();return Object.assign(t,e),t}}class sc extends nc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Aa(i),Ca(s),tc(t,e,n,s,i,"max")}}sc.className="MaxPooling1D",e.serialization.registerClass(sc);class ic extends nc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Aa(i),Ca(s),tc(t,e,n,s,i,"avg")}}ic.className="AveragePooling1D",e.serialization.registerClass(ic);class rc extends jo{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(2!==t.strides.length)throw new Ur(`If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length ${t.strides.length}.`);this.strides=t.strides}else this.strides=[t.strides,t.strides];oa(this.poolSize,"poolSize"),oa(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,Aa(this.dataFormat),Ca(this.padding),this.inputSpec=[new Wo({ndim:4})]}computeOutputShape(t){t=Lo(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2];return e=Ju(e,this.poolSize[0],this.padding,this.strides[0]),n=Ju(n,this.poolSize[1],this.padding,this.strides[1]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n]:[t[0],e,n,t[3]]}call(t,n){return e.tidy((()=>(this.invokeCallHook(t,n),this.poolingFunction(Fo(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class ac extends rc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Aa(i),Ca(s),tc(t,e,n,s,i,"max")}}ac.className="MaxPooling2D",e.serialization.registerClass(ac);class oc extends rc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Aa(i),Ca(s),tc(t,e,n,s,i,"avg")}}oc.className="AveragePooling2D",e.serialization.registerClass(oc);class lc extends jo{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(3!==t.strides.length)throw new Ur(`If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length ${t.strides.length}.`);this.strides=t.strides}else this.strides=[t.strides,t.strides,t.strides];oa(this.poolSize,"poolSize"),oa(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,Aa(this.dataFormat),Ca(this.padding),this.inputSpec=[new Wo({ndim:5})]}computeOutputShape(t){t=Lo(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[4]:t[3];return e=Ju(e,this.poolSize[0],this.padding,this.strides[0]),n=Ju(n,this.poolSize[1],this.padding,this.strides[1]),s=Ju(s,this.poolSize[2],this.padding,this.strides[2]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n,s]:[t[0],e,n,s,t[4]]}call(t,n){return e.tidy((()=>(this.invokeCallHook(t,n),this.poolingFunction(Fo(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class uc extends lc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Aa(i),Ca(s),ec(t,e,n,s,i,"max")}}uc.className="MaxPooling3D",e.serialization.registerClass(uc);class hc extends lc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Aa(i),Ca(s),ec(t,e,n,s,i,"avg")}}hc.className="AveragePooling3D",e.serialization.registerClass(hc);class cc extends jo{constructor(t){super(t),this.inputSpec=[new Wo({ndim:3})]}computeOutputShape(t){return[t[0],t[2]]}call(t,e){throw new Kr}}class pc extends cc{constructor(t){super(t||{})}call(t,n){return e.tidy((()=>{const n=Fo(t);return e.mean(n,1)}))}}pc.className="GlobalAveragePooling1D",e.serialization.registerClass(pc);class dc extends cc{constructor(t){super(t||{})}call(t,n){return e.tidy((()=>{const n=Fo(t);return e.max(n,1)}))}}dc.className="GlobalMaxPooling1D",e.serialization.registerClass(dc);class fc extends jo{constructor(t){super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,Aa(this.dataFormat),this.inputSpec=[new Wo({ndim:4})]}computeOutputShape(t){return t=t,"channelsLast"===this.dataFormat?[t[0],t[3]]:[t[0],t[1]]}call(t,e){throw new Kr}getConfig(){const t={dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class gc extends fc{call(t,n){return e.tidy((()=>{const n=Fo(t);return"channelsLast"===this.dataFormat?e.mean(n,[1,2]):e.mean(n,[2,3])}))}}gc.className="GlobalAveragePooling2D",e.serialization.registerClass(gc);class mc extends fc{call(t,n){return e.tidy((()=>{const n=Fo(t);return"channelsLast"===this.dataFormat?e.max(n,[1,2]):e.max(n,[2,3])}))}}mc.className="GlobalMaxPooling2D",e.serialization.registerClass(mc);class yc extends jo{constructor(t){super(t),this.layer=t.layer}build(t){this.built=!0}get trainable(){return null!=this.layer&&this.layer.trainable}set trainable(t){null!=this.layer&&(this.layer.trainable=t)}get trainableWeights(){return this.layer.trainableWeights}get nonTrainableWeights(){return this.layer.nonTrainableWeights}get updates(){return this.layer._updates}get losses(){return this.layer.losses}getWeights(){return this.layer.getWeights()}setWeights(t){this.layer.setWeights(t)}getConfig(){const t={layer:{className:this.layer.getClassName(),config:this.layer.getConfig()}},e=super.getConfig();return Object.assign(t,e),t}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.layer&&this.layer.setFastWeightInitDuringBuild(t)}static fromConfig(t,e,n={}){const s=al(e.layer,n);delete e.layer;const i={layer:s};return Object.assign(i,e),new t(i)}}class bc extends yc{constructor(t){super(t),this.supportsMasking=!0}build(t){if((t=Lo(t)).length<3)throw new Ur(`TimeDistributed layer expects an input shape >= 3D, but received input shape ${JSON.stringify(t)}`);this.inputSpec=[{shape:t}];const e=[t[0]].concat(t.slice(2));this.layer.built||(this.layer.build(e),this.layer.built=!0),super.build(t)}computeOutputShape(t){const e=[(t=Lo(t))[0]].concat(t.slice(2)),n=this.layer.computeOutputShape(e),s=t[1];return[n[0],s].concat(n.slice(1))}call(t,n){return e.tidy((()=>dh(((t,e)=>[Fo(this.layer.call(t,n)),[]]),t=Fo(t),[],!1,null,null,!1,!0)[1]))}}bc.className="TimeDistributed",e.serialization.registerClass(bc);class wc extends yc{constructor(t){super(t);const e=t.layer.getConfig(),n={};n.className=t.layer.getClassName(),n.config=e,this.forwardLayer=al(n),e.goBackwards=!0!==e.goBackwards;const s={};var i;if(s.className=t.layer.getClassName(),s.config=e,this.backwardLayer=al(s),this.forwardLayer.name="forward_"+this.forwardLayer.name,this.backwardLayer.name="backward_"+this.backwardLayer.name,this.mergeMode=void 0===t.mergeMode?"concat":t.mergeMode,i=this.mergeMode,ra(Na,"BidirectionalMergeMode",i),t.weights)throw new Kr("weights support is not implemented for Bidirectional layer yet.");this._stateful=t.layer.stateful,this.returnSequences=t.layer.returnSequences,this.returnState=t.layer.returnState,this.supportsMasking=!0,this._trainable=!0,this.inputSpec=t.layer.inputSpec,this.numConstants=null}get trainable(){return this._trainable}set trainable(t){this._trainable=t,null!=this.forwardLayer&&(this.forwardLayer.trainable=t),null!=this.backwardLayer&&(this.backwardLayer.trainable=t)}getWeights(){return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights())}setWeights(t){const e=t.length,n=Math.floor(e/2);this.forwardLayer.setWeights(t.slice(0,n)),this.backwardLayer.setWeights(t.slice(n))}computeOutputShape(t){let e,n,s,i=this.forwardLayer.computeOutputShape(t);return Array.isArray(i)&&Array.isArray(i[0])||(i=[i]),i=i,this.returnState?(s=i.slice(1),e=i[0]):e=i[0],e=e,"concat"===this.mergeMode?(e[e.length-1]*=2,n=[e]):n=null==this.mergeMode?[e,e.slice()]:[e],this.returnState?null==this.mergeMode?n.concat(s).concat(s.slice()):[e].concat(s).concat(s.slice()):Hr(n)}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=ph(t,n,s,this.numConstants);if(t=i.inputs,n=i.initialState,s=i.constants,Array.isArray(t)&&(n=t.slice(1),t=t[0]),(null==n||0===n.length)&&null==s)return super.apply(t,e);const r=[],a=[];if(null!=n){const t=n.length;if(t%2>0)throw new Ur("When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.");e.initialState=n,r.push(...n);const s=n.map((t=>new Wo({shape:t.shape})));this.forwardLayer.stateSpec=s.slice(0,t/2),this.backwardLayer.stateSpec=s.slice(t/2),a.push(...s)}if(null!=s)throw new Kr("Support for constants in Bidirectional layers is not implemented yet.");const o=r[0]instanceof Po;for(const t of r)if(t instanceof Po!==o)throw new Ur("The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors");if(o){const n=[t].concat(r),s=this.inputSpec.concat(a),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,n){return e.tidy((()=>{const s=n.initialState;let i,r,a,o;if(null==s)i=this.forwardLayer.call(t,n),r=this.backwardLayer.call(t,n);else{const e=s.slice(0,s.length/2),a=s.slice(s.length/2);i=this.forwardLayer.call(t,Object.assign(n,{initialState:e})),r=this.backwardLayer.call(t,Object.assign(n,{initialState:a}))}return this.returnState&&(Array.isArray(i)&&(a=i.slice(1).concat(r.slice(1))),i=i[0],r=r[0]),this.returnSequences&&(r=e.reverse(r,1)),"concat"===this.mergeMode?o=qa([i,r]):"sum"===this.mergeMode?o=e.add(i,r):"ave"===this.mergeMode?o=e.mul(.5,e.add(i,r)):"mul"===this.mergeMode?o=e.mul(i,r):null==this.mergeMode&&(o=[i,r]),this.returnState?null==this.mergeMode?o.concat(a):[o].concat(a):o}))}resetStates(t){this.forwardLayer.resetStates(),this.backwardLayer.resetStates()}build(t){Ta(this.forwardLayer.name,(()=>{this.forwardLayer.build(t)})),Ta(this.backwardLayer.name,(()=>{this.backwardLayer.build(t)})),this.built=!0}computeMask(t,e){let n;if(Array.isArray(e)&&(e=e[0]),n=this.returnSequences?null==this.mergeMode?[e,e]:e:null==this.mergeMode?[null,null]:null,this.returnState){const t=this.forwardLayer.states.map((t=>null));return Array.isArray(n)?n.concat(t).concat(t):[n].concat(t).concat(t)}return n}get trainableWeights(){return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights)}get nonTrainableWeights(){return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights)}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.forwardLayer&&this.forwardLayer.setFastWeightInitDuringBuild(t),null!=this.backwardLayer&&this.backwardLayer.setFastWeightInitDuringBuild(t)}getConfig(){const t={mergeMode:this.mergeMode},e=super.getConfig();return Object.assign(t,e),t}static fromConfig(t,e){const n=al(e.layer);if(delete e.layer,null!=e.numConstants)throw new Kr("Deserialization of a Bidirectional layer with numConstants present is not supported yet.");const s=e;return s.layer=n,new t(s)}}function kc(t){return new ic(t)}function xc(t){return new oc(t)}function vc(t){return new hc(t)}function Sc(t){return new dc(t)}function Ic(t){return new mc(t)}function Nc(t){return new sc(t)}function zc(t){return new ac(t)}wc.className="Bidirectional",e.serialization.registerClass(wc);const Ac=Sc,Cc=Ic,$c=Nc,Dc=zc;var Tc=Object.freeze({__proto__:null,inputLayer:function(t){return new Go(t)},elu:function(t){return new ju(t)},reLU:function(t){return new Uu(t)},leakyReLU:function(t){return new Ku(t)},prelu:function(t){return new Vu(t)},softmax:function(t){return new Gu(t)},thresholdedReLU:function(t){return new qu(t)},conv1d:function(t){return new lh(t)},conv2d:function(t){return new nh(t)},conv2dTranspose:function(t){return new ih(t)},conv3d:function(t){return new sh(t)},conv3dTranspose:function(t){return new rh(t)},separableConv2d:function(t){return new oh(t)},cropping2D:function(t){return new uh(t)},upSampling2d:function(t){return new hh(t)},depthwiseConv2d:function(t){return new ch(t)},activation:function(t){return new Th(t)},dense:function(t){return new $h(t)},dropout:function(t){return new Ah(t)},spatialDropout1d:function(t){return new Ch(t)},flatten:function(t){return new Dh(t)},repeatVector:function(t){return new Eh(t)},reshape:function(t){return new Fh(t)},permute:function(t){return new Lh(t)},embedding:function(t){return new Rh(t)},add:function(t){return new Oh(t)},average:function(t){return new Wh(t)},concatenate:function(t){return new Kh(t)},maximum:function(t){return new Ph(t)},minimum:function(t){return new Uh(t)},multiply:function(t){return new Bh(t)},dot:function(t){return new jh(t)},batchNormalization:function(t){return new Xh(t)},layerNormalization:function(t){return new Yh(t)},zeroPadding2d:function(t){return new Qh(t)},averagePooling1d:kc,avgPool1d:function(t){return kc(t)},avgPooling1d:function(t){return kc(t)},averagePooling2d:xc,avgPool2d:function(t){return xc(t)},avgPooling2d:function(t){return xc(t)},averagePooling3d:vc,avgPool3d:function(t){return vc(t)},avgPooling3d:function(t){return vc(t)},globalAveragePooling1d:function(t){return new pc(t)},globalAveragePooling2d:function(t){return new gc(t)},globalMaxPooling1d:Sc,globalMaxPooling2d:Ic,maxPooling1d:Nc,maxPooling2d:zc,maxPooling3d:function(t){return new uc(t)},gru:function(t){return new wh(t)},gruCell:function(t){return new bh(t)},lstm:function(t){return new xh(t)},lstmCell:function(t){return new kh(t)},simpleRNN:function(t){return new yh(t)},simpleRNNCell:function(t){return new mh(t)},convLstm2d:function(t){return new zh(t)},convLstm2dCell:function(t){return new Nh(t)},rnn:function(t){return new fh(t)},stackedRNNCells:function(t){return new vh(t)},bidirectional:function(t){return new wc(t)},timeDistributed:function(t){return new bc(t)},globalMaxPool1d:Ac,globalMaxPool2d:Cc,maxPool1d:$c,maxPool2d:Dc,Layer:jo,RNN:fh,RNNCell:gh,input:mu,gaussianNoise:function(t){return new qh(t)},gaussianDropout:function(t){return new Gh(t)},alphaDropout:function(t){return new Hh(t)},masking:function(t){return new _h(t)}});var Ec=Object.freeze({__proto__:null,binaryAccuracy:function(t,e){return yl(t,e)},binaryCrossentropy:function(t,e){return vl(t,e)},sparseCategoricalAccuracy:function(t,e){return Sl(t,e)},categoricalAccuracy:function(t,e){return bl(t,e)},categoricalCrossentropy:function(t,e){return Il(t,e)},precision:function(t,e){return kl(t,e)},recall:function(t,e){return xl(t,e)},cosineProximity:function(t,e){return fl(t,e)},meanAbsoluteError:function(t,e){return ul(t,e)},meanAbsolutePercentageError:function(t,e){return hl(t,e)},MAPE:function(t,e){return hl(t,e)},mape:function(t,e){return hl(t,e)},meanSquaredError:function(t,e){return ll(t,e)},MSE:function(t,e){return ll(t,e)},mse:function(t,e){return ll(t,e)}}),Fc=Object.freeze({__proto__:null,modelFromJSON:async function(t,n){"modelTopology"in t||(t={modelTopology:t});let s=(t=t).modelTopology;null!=s.model_config&&(s=s.model_config);const i=al(Ml(s),n);if(null!=t.weightsManifest){const n=await e.io.loadWeights(t.weightsManifest,t.pathPrefix,i.weights.map((t=>t.originalName))),s={};for(const t of i.weights)s[t.originalName]=n[t.originalName];i.loadWeights(s),e.dispose(n)}return i}});var Lc=Object.freeze({__proto__:null,l1l2:function(t){return new Mu(t)},l1:function(t){return _u(e=t),new Mu({l1:null!=e?e.l1:null,l2:0});var e},l2:function(t){return _u(e=t),new Mu({l2:null!=e?e.l2:null,l1:0});var e}});class _c extends Yo{constructor(){super(...arguments),this.model=null}setModel(t){if(!(t instanceof pu))throw new Error("model must be a LayersModel, not some other Container");this.model=t}}function Rc(t,e){return t<e}function Mc(t,e){return t>e}class Oc extends _c{constructor(t){if(super(),null==t&&(t={}),t.restoreBestWeights)throw new Kr("restoreBestWeights = True is not implemented in EarlyStopping yet.");this.monitor=t.monitor||"val_loss",this.minDelta=Math.abs(t.minDelta||0),this.patience=t.patience||0,this.verbose=t.verbose||0,this.mode=t.mode||"auto",this.baseline=t.baseline,-1===["auto","min","max"].indexOf(this.mode)&&(console.warn(`EarlyStopping mode '${this.mode}' is invalid. Falling back to mode 'auto'.`),this.mode="auto"),"min"===this.mode?this.monitorFunc=Rc:"max"===this.mode||-1!==this.monitor.indexOf("acc")?this.monitorFunc=Mc:this.monitorFunc=Rc,this.monitorFunc===Rc&&(this.minDelta*=-1)}async onTrainBegin(t){this.wait=0,this.stoppedEpoch=0,null!=this.baseline?this.best=this.baseline:this.best=this.monitorFunc===Rc?1/0:-1/0}async onEpochEnd(t,e){await Jo(e);const n=this.getMonitorValue(e);null!=n&&(this.monitorFunc(n-this.minDelta,this.best)?(this.best=n,this.wait=0):(this.wait++,this.wait>=this.patience&&(this.stoppedEpoch=t,this.model.stopTraining=!0)))}async onTrainEnd(t){this.stoppedEpoch>0&&this.verbose&&console.log(`Epoch ${this.stoppedEpoch}: early stopping.`)}getMonitorValue(t){null==t&&(t={});const e=t[this.monitor];return null==e&&console.warn(`Metric for EarlyStopping ${this.monitor} is not available. Available metrics are: ${Object.keys(t)}`),e}}const Bc={earlyStopping:function(t){return new Oc(t)}};t.Callback=_c,t.CallbackList=Qo,t.CustomCallback=nl,t.EarlyStopping=Oc,t.History=el,t.InputSpec=Wo,t.LayerVariable=Mo,t.LayersModel=pu,t.RNN=fh,t.Sequential=gu,t.SymbolicTensor=Po,t.callbacks=Bc,t.constraints=ka,t.initializers=zo,t.input=mu,t.layers=Tc,t.loadLayersModel=function(t,e){return null==e&&(e={}),fu(t,e)},t.metrics=Ec,t.model=function(t){return new pu(t)},t.models=Fc,t.registerCallbackConstructor=function(t,e){il.registerCallbackConstructor(t,e)},t.regularizers=Lc,t.sequential=function(t){return new gu(t)},t.version_layers=Bl,Object.defineProperty(t,"__esModule",{value:!0})}));
//# sourceMappingURL=tf-layers.es2017.min.js.map
