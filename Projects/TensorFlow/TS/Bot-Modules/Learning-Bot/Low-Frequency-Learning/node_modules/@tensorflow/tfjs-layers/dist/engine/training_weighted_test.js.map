{"version":3,"file":"training_weighted_test.js","sourceRoot":"","sources":["../../src/engine/training_weighted_test.ts"],"names":[],"mappings":"AAAA;;;;;;;;GAQG;AAEH,4DAA4D;AAE5D,OAAO,EAAC,MAAM,EAAE,IAAI,EAAU,QAAQ,EAAE,KAAK,EAAE,KAAK,EAAC,MAAM,uBAAuB,CAAC;AAEnF,OAAO,KAAK,GAAG,MAAM,UAAU,CAAC;AAEhC,OAAO,EAAC,qBAAqB,EAAE,kBAAkB,EAAC,MAAM,qBAAqB,CAAC;AAC9E,OAAO,EAAC,kBAAkB,EAAC,MAAM,iBAAiB,CAAC;AAGnD,qBAAqB,CAAC,oCAAoC,EAAE,GAAG,EAAE;IAC/D,yBAAyB;IACzB,QAAQ;IACR,qBAAqB;IACrB,0BAA0B;IAC1B,EAAE;IACF,gCAAgC;IAChC,mCAAmC;IACnC,eAAe;IACf,uBAAuB;IACvB,kCAAkC;IAClC,6BAA6B;IAC7B,iDAAiD;IACjD,iCAAiC;IACjC,wDAAwD;IACxD,kBAAkB;IAClB,EAAE;IACF,wEAAwE;IACxE,kCAAkC;IAClC,4BAA4B;IAC5B,4BAA4B;IAC5B,4BAA4B;IAC5B,4BAA4B;IAC5B,4BAA4B;IAC5B,+CAA+C;IAC/C,EAAE;IACF,gBAAgB;IAChB,gBAAgB;IAChB,sBAAsB;IACtB,4BAA4B;IAC5B,oBAAoB;IACpB,qBAAqB;IACrB,mBAAmB;IACnB,gBAAgB;IAChB,gCAAgC;IAChC,MAAM;IACN,EAAE,CAAC,2CAA2C,EAAE,KAAK,IAAI,EAAE;QACzD,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,EAAE,CAAC;QAC/B,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CAAC;YACzB,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC,CAAC,CAAC;QACJ,KAAK,CAAC,OAAO,CAAC;YACZ,IAAI,EAAE,yBAAyB;YAC/B,OAAO,EAAE,CAAC,KAAK,CAAC;YAChB,SAAS,EAAE,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC;SACxB,CAAC,CAAC;QAEH,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC5E,MAAM,EAAE,GAAG,QAAQ,CACf,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACxE,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,OAAO,GAAG,MAAM,KAAK,CAAC,GAAG,CAC3B,EAAE,EAAE,EAAE,EAAE,EAAC,MAAM,EAAE,CAAC,EAAE,WAAW,EAAE,CAAC,EAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAC,CAAC,EAAC,CAAC,CAAC;QAC7D,MAAM,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC,CAAE,yBAAyB;QAC5E,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC/C,gEAAgE;QAChE,kCAAkC;QAClC,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC9C,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACnD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;IACrD,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,0DAA0D,EAAE,KAAK,IAAI,EAAE;QACxE,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,EAAE,CAAC;QAC/B,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CAAC;YACzB,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC,CAAC,CAAC;QACJ,KAAK,CAAC,OAAO,CAAC;YACZ,IAAI,EAAE,yBAAyB;YAC/B,OAAO,EAAE,CAAC,KAAK,CAAC;YAChB,SAAS,EAAE,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC;SACxB,CAAC,CAAC;QAEH,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC5E,MAAM,EAAE,GAAG,QAAQ,CACf,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACxE,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,OAAO,GAAG,MAAM,KAAK,CAAC,GAAG,CAAC,EAAE,EAAE,EAAE,EAAE;YACtC,MAAM,EAAE,CAAC;YACT,SAAS,EAAE,CAAC;YACZ,OAAO,EAAE,KAAK;YACd,WAAW,EAAE,CAAC,EAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAC,CAAC;SACnC,CAAC,CAAC;QACH,MAAM,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC,CAAE,yBAAyB;QAC5E,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC/C,gEAAgE;QAChE,kCAAkC;QAClC,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,OAAO,CAAC,CAAC;QACrD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,OAAO,CAAC,CAAC;QACrD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC9C,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACnD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;IACrD,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,2DAA2D,EAAE,KAAK,IAAI,EAAE;QACzE,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,EAAE,CAAC;QAC/B,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CAAC;YACzB,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC,CAAC,CAAC;QACJ,KAAK,CAAC,OAAO,CAAC;YACZ,IAAI,EAAE,yBAAyB;YAC/B,OAAO,EAAE,CAAC,KAAK,CAAC;YAChB,SAAS,EAAE,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC;SACxB,CAAC,CAAC;QAEH,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC5E,MAAM,EAAE,GAAG,QAAQ,CACf,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACxE,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,OAAO,GAAG,MAAM,KAAK,CAAC,GAAG,CAAC,EAAE,EAAE,EAAE,EAAE;YACtC,MAAM,EAAE,CAAC;YACT,WAAW,EAAE,CAAC,EAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAC,CAAC;YAClC,cAAc,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC;SACzB,CAAC,CAAC;QACH,MAAM,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC,CAAE,yBAAyB;QAC5E,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC/C,gEAAgE;QAChE,kCAAkC;QAClC,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC9C,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACnD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QAEnD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QACnD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACxD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACxD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAClD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACvD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;IACzD,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,4DAA4D,EAAE,KAAK,IAAI,EAAE;QAC1E,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,EAAE,CAAC;QAC/B,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CAAC;YACzB,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC,CAAC,CAAC;QACJ,KAAK,CAAC,OAAO,CAAC;YACZ,IAAI,EAAE,yBAAyB;YAC/B,OAAO,EAAE,CAAC,KAAK,CAAC;YAChB,SAAS,EAAE,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC;SACxB,CAAC,CAAC;QAEH,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC5E,MAAM,EAAE,GAAG,QAAQ,CACf,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACxE,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,OAAO,GAAG,MAAM,KAAK,CAAC,GAAG,CAC3B,EAAE,EAAE,EAAE,EACN,EAAC,MAAM,EAAE,CAAC,EAAE,WAAW,EAAE,CAAC,EAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAC,CAAC,EAAE,eAAe,EAAE,GAAG,EAAC,CAAC,CAAC;QAC3E,MAAM,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC,CAAE,yBAAyB;QAC5E,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC/C,gEAAgE;QAChE,kCAAkC;QAClC,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,OAAO,CAAC,CAAC;QACrD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC9C,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACnD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QAEnD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QACnD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,UAAU,CAAC,CAAC;QAC5D,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,UAAU,CAAC,CAAC;QAC5D,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAClD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;QAClD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;IACpD,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,0CAA0C,EAAE,KAAK,IAAI,EAAE;QACxD,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,EAAE,CAAC;QAC/B,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CAAC;YACzB,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC,CAAC,CAAC;QACJ,KAAK,CAAC,OAAO,CACT,EAAC,IAAI,EAAE,+BAA+B,EAAE,SAAS,EAAE,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC;QAEtE,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC5E,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QACpD,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,OAAO,GACT,MAAM,KAAK,CAAC,GAAG,CAAC,EAAE,EAAE,EAAE,EAAE,EAAC,MAAM,EAAE,CAAC,EAAE,WAAW,EAAE,EAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAC,EAAC,CAAC,CAAC;QAC3E,MAAM,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC,CAAE,yBAAyB;QAC5E,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC/C,gEAAgE;QAChE,kCAAkC;QAClC,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;IACtD,CAAC,CAAC,CAAC;IAEH,yBAAyB;IACzB,QAAQ;IACR,qBAAqB;IACrB,0BAA0B;IAC1B,EAAE;IACF,gCAAgC;IAChC,mCAAmC;IACnC,eAAe;IACf,uBAAuB;IACvB,kCAAkC;IAClC,6BAA6B;IAC7B,4CAA4C;IAC5C,wDAAwD;IACxD,kBAAkB;IAClB,EAAE;IACF,oDAAoD;IACpD,kCAAkC;IAClC,wDAAwD;IACxD,EAAE;IACF,gCAAgC;IAChC,gBAAgB;IAChB,gBAAgB;IAChB,sBAAsB;IACtB,4BAA4B;IAC5B,wBAAwB;IACxB,uBAAuB;IACvB,gBAAgB;IAChB,gCAAgC;IAChC,MAAM;IACN,EAAE,CAAC,6CAA6C,EAAE,KAAK,IAAI,EAAE;QAC3D,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,EAAE,CAAC;QAC/B,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CAAC;YACzB,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC,CAAC,CAAC;QACJ,KAAK,CAAC,OAAO,CAAC,EAAC,IAAI,EAAE,oBAAoB,EAAE,SAAS,EAAE,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC;QAErE,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC;QACxD,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAC1C,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,OAAO,GACT,MAAM,KAAK,CAAC,GAAG,CAAC,EAAE,EAAE,EAAE,EAAE,EAAC,MAAM,EAAE,CAAC,EAAE,WAAW,EAAE,CAAC,EAAC,CAAC,EAAE,GAAG,EAAE,CAAC,EAAE,GAAG,EAAC,CAAC,EAAC,CAAC,CAAC;QAC1E,MAAM,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC,CAAE,yBAAyB;QAC5E,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC/C,gEAAgE;QAChE,kCAAkC;QAClC,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;IACtD,CAAC,CAAC,CAAC;IAEH,yBAAyB;IACzB,QAAQ;IACR,qBAAqB;IACrB,0BAA0B;IAC1B,EAAE;IACF,kCAAkC;IAClC,8BAA8B;IAC9B,eAAe;IACf,uBAAuB;IACvB,kCAAkC;IAClC,iCAAiC;IACjC,8BAA8B;IAC9B,eAAe;IACf,uBAAuB;IACvB,kCAAkC;IAClC,iCAAiC;IACjC,wCAAwC;IACxC,iBAAiB;IACjB,uEAAuE;IACvE,8CAA8C;IAC9C,kBAAkB;IAClB,EAAE;IACF,iBAAiB;IACjB,4DAA4D;IAC5D,wBAAwB;IACxB,kBAAkB;IAClB,wDAAwD;IACxD,kBAAkB;IAClB,wDAAwD;IACxD,EAAE;IACF,gCAAgC;IAChC,gBAAgB;IAChB,wBAAwB;IACxB,sBAAsB;IACtB,4BAA4B;IAC5B,wBAAwB;IACxB,wBAAwB;IACxB,uBAAuB;IACvB,iBAAiB;IACjB,wBAAwB;IACxB,uBAAuB;IACvB,gBAAgB;IAChB,MAAM;IACN,EAAE,CAAC,mCAAmC,EAAE,KAAK,IAAI,EAAE;QACjD,MAAM,GAAG,GAAG,GAAG,CAAC,KAAK,CAAC,EAAC,KAAK,EAAE,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC;QACpC,MAAM,EAAE,GAAG,GAAG,CAAC,MAAM;aACL,KAAK,CAAC;YACL,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC;aACD,KAAK,CAAC,GAAG,CAAuB,CAAC;QACjD,MAAM,EAAE,GAAG,GAAG,CAAC,MAAM;aACL,KAAK,CAAC;YACL,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC;aACD,KAAK,CAAC,GAAG,CAAuB,CAAC;QACjD,MAAM,KAAK,GAAG,GAAG,CAAC,KAAK,CAAC,EAAC,MAAM,EAAE,GAAG,EAAE,OAAO,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,EAAC,CAAC,CAAC;QAC1D,KAAK,CAAC,OAAO,CAAC;YACZ,IAAI,EAAE,CAAC,+BAA+B,EAAE,oBAAoB,CAAC;YAC7D,SAAS,EAAE,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC;SACxB,CAAC,CAAC;QAEH,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC;QAC1E,MAAM,GAAG,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QACrD,MAAM,GAAG,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAErD,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,OAAO,GAAG,MAAM,KAAK,CAAC,GAAG,CAC3B,EAAE,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,EACd,EAAC,MAAM,EAAE,CAAC,EAAE,WAAW,EAAE,CAAC,EAAC,CAAC,EAAE,GAAG,EAAE,CAAC,EAAE,GAAG,EAAE,CAAC,EAAE,GAAG,EAAC,EAAE,EAAC,CAAC,EAAE,GAAG,EAAE,CAAC,EAAE,GAAG,EAAC,CAAC,EAAC,CAAC,CAAC;QAC5E,MAAM,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC,CAAE,yBAAyB;QAC5E,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC/C,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,QAAQ,GAAG,GAAG,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,OAAO,CAAC;QAChD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,QAAQ,GAAG,GAAG,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,OAAO,CAAC;QAChD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;IAC3D,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,mDAAmD,EAAE,KAAK,IAAI,EAAE;QACjE,MAAM,GAAG,GAAG,GAAG,CAAC,KAAK,CAAC,EAAC,KAAK,EAAE,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC;QACpC,MAAM,EAAE,GAAG,GAAG,CAAC,MAAM;aACL,KAAK,CAAC;YACL,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC;aACD,KAAK,CAAC,GAAG,CAAuB,CAAC;QACjD,MAAM,EAAE,GAAG,GAAG,CAAC,MAAM;aACL,KAAK,CAAC;YACL,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC;aACD,KAAK,CAAC,GAAG,CAAuB,CAAC;QACjD,MAAM,KAAK,GAAG,GAAG,CAAC,KAAK,CAAC,EAAC,MAAM,EAAE,GAAG,EAAE,OAAO,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,EAAC,CAAC,CAAC;QAC1D,KAAK,CAAC,OAAO,CAAC;YACZ,IAAI,EAAE,CAAC,+BAA+B,EAAE,oBAAoB,CAAC;YAC7D,SAAS,EAAE,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC;SACxB,CAAC,CAAC;QAEH,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC;QAC1E,MAAM,GAAG,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QACrD,MAAM,GAAG,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAErD,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,OAAO,GAAG,MAAM,KAAK,CAAC,GAAG,CAC3B,EAAE,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,EACd,EAAC,MAAM,EAAE,CAAC,EAAE,WAAW,EAAE,CAAC,IAAI,EAAE,EAAC,CAAC,EAAE,GAAG,EAAE,CAAC,EAAE,GAAG,EAAC,CAAC,EAAE,OAAO,EAAE,KAAK,EAAC,CAAC,CAAC;QACxE,MAAM,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC,CAAE,yBAAyB;QAC5E,kEAAkE;QAClE,gEAAgE;QAChE,6DAA6D;QAC7D,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC/C,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,QAAQ,GAAG,GAAG,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,OAAO,CAAC;QAChD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,QAAQ,GAAG,GAAG,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,OAAO,CAAC;QAChD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,kBAAkB,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,EAAE,QAAQ,CAAC;YAC9B,CAAC,CAAC,SAAS,EAAE,CAAC,UAAU,EAAE,SAAS,CAAC;YACpC,CAAC,CAAC,SAAS,EAAE,SAAS,EAAE,UAAU,CAAC;SACpC,CAAC,CAAC,CAAC;IACzB,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,0DAA0D,EAAE,KAAK,IAAI,EAAE;QACxE,MAAM,GAAG,GAAG,GAAG,CAAC,KAAK,CAAC,EAAC,KAAK,EAAE,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC;QACpC,MAAM,EAAE,GAAG,GAAG,CAAC,MAAM;aACL,KAAK,CAAC;YACL,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC;aACD,KAAK,CAAC,GAAG,CAAuB,CAAC;QACjD,MAAM,EAAE,GAAG,GAAG,CAAC,MAAM;aACL,KAAK,CAAC;YACL,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC;aACD,KAAK,CAAC,GAAG,CAAuB,CAAC;QACjD,MAAM,KAAK,GAAG,GAAG,CAAC,KAAK,CAAC,EAAC,MAAM,EAAE,GAAG,EAAE,OAAO,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,EAAC,CAAC,CAAC;QAC1D,KAAK,CAAC,OAAO,CAAC;YACZ,IAAI,EAAE,CAAC,+BAA+B,EAAE,oBAAoB,CAAC;YAC7D,SAAS,EAAE,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC;SACxB,CAAC,CAAC;QAEH,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC;QAC1E,MAAM,GAAG,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QACrD,MAAM,GAAG,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAErD,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,OAAO,GAAG,MAAM,KAAK,CAAC,GAAG,CAAC,EAAE,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE;YAC9C,MAAM,EAAE,CAAC;YACT,WAAW,EAAE;gBACX,CAAC,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC,EAAE,EAAC,CAAC,EAAE,GAAG,EAAE,CAAC,EAAE,GAAG,EAAC;aACzC;YACD,OAAO,EAAE,KAAK;SACf,CAAC,CAAC;QACH,MAAM,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC,CAAE,yBAAyB;QAC5E,kEAAkE;QAClE,gEAAgE;QAChE,0CAA0C;QAC1C,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC/C,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,QAAQ,GAAG,GAAG,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,OAAO,CAAC;QAChD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,QAAQ,GAAG,GAAG,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,OAAO,CAAC;QAChD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,kBAAkB,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,EAAE,QAAQ,CAAC;YAC9B,CAAC,CAAC,SAAS,EAAE,CAAC,UAAU,EAAE,SAAS,CAAC;YACpC,CAAC,CAAC,SAAS,EAAE,SAAS,EAAE,UAAU,CAAC;SACpC,CAAC,CAAC,CAAC;IACzB,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,iCAAiC,EAAE,KAAK,IAAI,EAAE;QAC/C,MAAM,GAAG,GAAG,GAAG,CAAC,KAAK,CAAC,EAAC,KAAK,EAAE,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC;QACpC,MAAM,EAAE,GAAG,GAAG,CAAC,MAAM;aACL,KAAK,CAAC;YACL,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC;aACD,KAAK,CAAC,GAAG,CAAuB,CAAC;QACjD,MAAM,EAAE,GAAG,GAAG,CAAC,MAAM;aACL,KAAK,CAAC;YACL,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC;aACD,KAAK,CAAC,GAAG,CAAuB,CAAC;QACjD,MAAM,KAAK,GAAG,GAAG,CAAC,KAAK,CAAC,EAAC,MAAM,EAAE,GAAG,EAAE,OAAO,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,EAAC,CAAC,CAAC;QAC1D,KAAK,CAAC,OAAO,CAAC;YACZ,IAAI,EAAE,CAAC,+BAA+B,EAAE,oBAAoB,CAAC;YAC7D,SAAS,EAAE,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC;SACxB,CAAC,CAAC;QAEH,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC;QAC1E,MAAM,GAAG,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QACrD,MAAM,GAAG,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAErD,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,OAAO,GAAG,MAAM,KAAK,CAAC,GAAG,CAAC,EAAE,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE;YAC9C,MAAM,EAAE,CAAC;YACT,WAAW,EAAE;gBACX,CAAC,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC,EAAE,EAAC,CAAC,EAAE,GAAG,EAAE,CAAC,EAAE,GAAG,EAAE,CAAC,EAAE,GAAG,EAAC;gBAChD,CAAC,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC,EAAE,EAAC,CAAC,EAAE,GAAG,EAAE,CAAC,EAAE,GAAG,EAAC;aACzC;SACF,CAAC,CAAC;QACH,MAAM,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC,CAAE,yBAAyB;QAC5E,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC/C,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,QAAQ,GAAG,GAAG,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,OAAO,CAAC;QAChD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,QAAQ,GAAG,GAAG,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,OAAO,CAAC;QAChD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;IAC3D,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC;AAEH,qBAAqB,CAAC,2CAA2C,EAAE,GAAG,EAAE;IACtE,yBAAyB;IACzB,QAAQ;IACR,qBAAqB;IACrB,0BAA0B;IAC1B,EAAE;IACF,iBAAiB;IACjB,kBAAkB;IAClB,aAAa;IACb,EAAE;IACF,uDAAuD;IACvD,wBAAwB;IACxB,iEAAiE;IACjE,iEAAiE;IACjE,cAAc;IACd,EAAE;IACF,gDAAgD;IAChD,kCAAkC;IAClC,EAAE;IACF,gCAAgC;IAChC,mCAAmC;IACnC,SAAS;IACT,uBAAuB;IACvB,4BAA4B;IAC5B,mCAAmC;IACnC,4CAA4C;IAC5C,iCAAiC;IACjC,iCAAiC;IACjC,EAAE;IACF,uBAAuB;IACvB,eAAe;IACf,mCAAmC;IACnC,qBAAqB;IACrB,mCAAmC;IACnC,yBAAyB;IACzB,MAAM;IACN,EAAE,CAAC,6CAA6C,EAAE,KAAK,IAAI,EAAE;QAC3D,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,EAAE,CAAC;QAC/B,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CAAC;YACzB,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC,CAAC,CAAC;QACJ,KAAK,CAAC,OAAO,CACT,EAAC,IAAI,EAAE,oBAAoB,EAAE,OAAO,EAAE,CAAC,KAAK,CAAC,EAAE,SAAS,EAAE,KAAK,EAAC,CAAC,CAAC;QAEtE,MAAM,SAAS,GAAG,CAAC,CAAC;QACpB,MAAM,eAAe,GAAG,CAAC,CAAC;QAC1B,MAAM,MAAM,GAAG,CAAC,CAAC;QACjB,MAAM,YAAY,GAAG,GAAG,EAAE,CACtB,CAAC,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC;YAChE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACvE,MAAM,YAAY,GAAG,GAAG,EAAE,CACtB,CAAC,KAAK,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC;YACnE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACvE,MAAM,OAAO,GAAG,IAAI,kBAAkB,CAAC;YACrC,MAAM,EAAE,CAAC,CAAC,CAAC;YACX,MAAM,EAAE,CAAC,CAAC,CAAC;YACX,SAAS;YACT,UAAU,EAAE,eAAe,GAAG,MAAM;YACpC,YAAY;YACZ,YAAY;SACb,CAAC,CAAC;QAEH,MAAM,WAAW,GAAgB,EAAC,CAAC,EAAE,GAAG,EAAE,CAAC,EAAE,CAAC,EAAC,CAAC;QAEhD,yEAAyE;QACzE,gCAAgC;QAChC,MAAM,KAAK,CAAC,UAAU,CAAC,OAAO,EAAE,EAAC,eAAe,EAAE,MAAM,EAAE,CAAC,EAAE,WAAW,EAAC,CAAC,CAAC;QAC3E,KAAK,CAAC,UAAU,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAE9C,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,OAAO,GACT,MAAM,KAAK,CAAC,UAAU,CAAC,OAAO,EAAE,EAAC,eAAe,EAAE,MAAM,EAAE,WAAW,EAAC,CAAC,CAAC;QAC5E,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,CAAC,WAAW,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC;QAEzC,mEAAmE;QACnE,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC/C,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QAEpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC9C,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;QAC9C,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;IACrD,CAAC,CAAC,CAAC;IAEH,yBAAyB;IACzB,QAAQ;IACR,qBAAqB;IACrB,0BAA0B;IAC1B,EAAE;IACF,iBAAiB;IACjB,kBAAkB;IAClB,aAAa;IACb,EAAE;IACF,uDAAuD;IACvD,wBAAwB;IACxB,iEAAiE;IACjE,iEAAiE;IACjE,cAAc;IACd,EAAE;IACF,gDAAgD;IAChD,kCAAkC;IAClC,EAAE;IACF,gCAAgC;IAChC,mCAAmC;IACnC,SAAS;IACT,uBAAuB;IACvB,4BAA4B;IAC5B,mCAAmC;IACnC,4CAA4C;IAC5C,iCAAiC;IACjC,iCAAiC;IACjC,EAAE;IACF,uBAAuB;IACvB,eAAe;IACf,mCAAmC;IACnC,qBAAqB;IACrB,mCAAmC;IACnC,yBAAyB;IACzB,MAAM;IACN,EAAE,CAAC,4CAA4C,EAAE,KAAK,IAAI,EAAE;QAC1D,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,EAAE,CAAC;QAC/B,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CAAC;YACzB,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC,CAAC,CAAC;QACJ,KAAK,CAAC,OAAO,CACT,EAAC,IAAI,EAAE,oBAAoB,EAAE,OAAO,EAAE,CAAC,KAAK,CAAC,EAAE,SAAS,EAAE,KAAK,EAAC,CAAC,CAAC;QAEtE,MAAM,SAAS,GAAG,CAAC,CAAC;QACpB,MAAM,eAAe,GAAG,CAAC,CAAC;QAC1B,MAAM,MAAM,GAAG,CAAC,CAAC;QACjB,MAAM,YAAY,GAAG,GAAG,EAAE,CACtB,CAAC,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC;YAChE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACvE,MAAM,YAAY,GAAG,GAAG,EAAE,CACtB,CAAC,KAAK,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC;YACnE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACvE,MAAM,OAAO,GAAG,IAAI,kBAAkB,CAAC;YACrC,MAAM,EAAE,CAAC,CAAC,CAAC;YACX,MAAM,EAAE,CAAC,CAAC,CAAC;YACX,SAAS;YACT,UAAU,EAAE,eAAe,GAAG,MAAM;YACpC,YAAY;YACZ,YAAY;SACb,CAAC,CAAC;QAEH,MAAM,WAAW,GAAgB,EAAC,CAAC,EAAE,GAAG,EAAE,CAAC,EAAE,CAAC,EAAC,CAAC;QAEhD,yEAAyE;QACzE,gCAAgC;QAChC,MAAM,KAAK,CAAC,UAAU,CAClB,OAAO,EACP,EAAC,eAAe,EAAE,MAAM,EAAE,CAAC,EAAE,WAAW,EAAE,cAAc,EAAE,OAAO,EAAC,CAAC,CAAC;QACxE,KAAK,CAAC,UAAU,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAE9C,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,OAAO,GAAG,MAAM,KAAK,CAAC,UAAU,CAClC,OAAO,EACP,EAAC,eAAe,EAAE,MAAM,EAAE,WAAW,EAAE,cAAc,EAAE,OAAO,EAAC,CAAC,CAAC;QACrE,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,CAAC,WAAW,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC;QAEzC,mEAAmE;QACnE,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC/C,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QAEpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC9C,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;QAC9C,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QAEnD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QACnD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,QAAQ,CAAC,CAAC;QAC1D,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,QAAQ,CAAC,CAAC;QAE1D,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAClD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,GAAG,CAAC,CAAC;QACpD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,GAAG,CAAC,CAAC;IACtD,CAAC,CAAC,CAAC;IAEH,yBAAyB;IACzB,QAAQ;IACR,qBAAqB;IACrB,0BAA0B;IAC1B,EAAE;IACF,iBAAiB;IACjB,kBAAkB;IAClB,aAAa;IACb,EAAE;IACF,uDAAuD;IACvD,yBAAyB;IACzB,iEAAiE;IACjE,iEAAiE;IACjE,cAAc;IACd,yBAAyB;IACzB,gEAAgE;IAChE,kEAAkE;IAClE,cAAc;IACd,EAAE;IACF,gDAAgD;IAChD,0DAA0D;IAC1D,EAAE;IACF,kCAAkC;IAClC,gCAAgC;IAChC,SAAS;IACT,uBAAuB;IACvB,4BAA4B;IAC5B,kCAAkC;IAClC,wBAAwB;IACxB,gCAAgC;IAChC,SAAS;IACT,uBAAuB;IACvB,4BAA4B;IAC5B,kCAAkC;IAClC,wBAAwB;IACxB,EAAE;IACF,4CAA4C;IAC5C,iBAAiB;IACjB,2DAA2D;IAC3D,uBAAuB;IACvB,EAAE;IACF,uBAAuB;IACvB,eAAe;IACf,mCAAmC;IACnC,qBAAqB;IACrB,qDAAqD;IACrD,yBAAyB;IACzB,MAAM;IACN,EAAE,CAAC,8CAA8C,EAAE,KAAK,IAAI,EAAE;QAC5D,MAAM,GAAG,GAAG,GAAG,CAAC,KAAK,CAAC,EAAC,KAAK,EAAE,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC;QACpC,MAAM,IAAI,GAAG,GAAG,CAAC,MAAM;aACL,KAAK,CAAC;YACL,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC;aACD,KAAK,CAAC,GAAG,CAAuB,CAAC;QACnD,MAAM,IAAI,GAAG,GAAG,CAAC,MAAM;aACL,KAAK,CAAC;YACL,KAAK,EAAE,CAAC;YACR,UAAU,EAAE,CAAC,CAAC,CAAC;YACf,iBAAiB,EAAE,OAAO;YAC1B,UAAU,EAAE,SAAS;SACtB,CAAC;aACD,KAAK,CAAC,GAAG,CAAuB,CAAC;QACnD,MAAM,KAAK,GAAG,GAAG,CAAC,KAAK,CAAC,EAAC,MAAM,EAAE,GAAG,EAAE,OAAO,EAAE,CAAC,IAAI,EAAE,IAAI,CAAC,EAAC,CAAC,CAAC;QAC9D,KAAK,CAAC,OAAO,CACT,EAAC,IAAI,EAAE,CAAC,oBAAoB,EAAE,oBAAoB,CAAC,EAAE,SAAS,EAAE,KAAK,EAAC,CAAC,CAAC;QAE5E,MAAM,SAAS,GAAG,CAAC,CAAC;QACpB,MAAM,eAAe,GAAG,CAAC,CAAC;QAC1B,MAAM,MAAM,GAAG,CAAC,CAAC;QACjB,MAAM,YAAY,GAAG,GAAG,EAAE,CACtB,CAAC,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC;YAChE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACvE,MAAM,YAAY,GAAG,GAAG,EAAE;YACxB,MAAM,MAAM,GAA+B,EAAE,CAAC;YAC9C,MAAM,CAAC,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC,GAAG;gBAC7B,KAAK,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC;gBACnE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC;aACjE,CAAC;YACF,MAAM,CAAC,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC,GAAG;gBAC7B,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC;gBAChE,KAAK,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC;aACpE,CAAC;YACF,OAAO,MAAM,CAAC;QAChB,CAAC,CAAC;QACF,MAAM,OAAO,GAAG,IAAI,kBAAkB,CAAC;YACrC,MAAM,EAAE,CAAC,CAAC,CAAC;YACX,MAAM,EAAE,EAAC,CAAC,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAC;YAClE,SAAS;YACT,UAAU,EAAE,eAAe,GAAG,MAAM;YACpC,YAAY;YACZ,YAAY;SACb,CAAC,CAAC;QAEH,MAAM,WAAW,GAAmB;YAClC,CAAC,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC,EAAE,EAAC,CAAC,EAAE,GAAG,EAAE,CAAC,EAAE,CAAC,EAAC;YACtC,CAAC,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC,EAAE,EAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,GAAG,EAAC;SACvC,CAAC;QAEF,yEAAyE;QACzE,gCAAgC;QAChC,MAAM,KAAK,CAAC,UAAU,CAAC,OAAO,EAAE,EAAC,eAAe,EAAE,MAAM,EAAE,CAAC,EAAE,WAAW,EAAC,CAAC,CAAC;QAC3E,iDAAiD;QAEjD,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,OAAO,GACT,MAAM,KAAK,CAAC,UAAU,CAAC,OAAO,EAAE,EAAC,eAAe,EAAE,MAAM,EAAE,WAAW,EAAC,CAAC,CAAC;QAC5E,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,CAAC,WAAW,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC;QAEzC,mEAAmE;QACnE,MAAM,SAAS,GAAG,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC;QACvC,MAAM,CAAC,SAAS,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QACpC,MAAM,CAAC,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACzC,MAAM,CAAC,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QAEzC,MAAM,KAAK,GAAG,OAAO,CAAC,OAAO,CAAC,GAAG,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC;QAC9D,MAAM,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAChC,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACrC,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QAErC,MAAM,KAAK,GAAG,OAAO,CAAC,OAAO,CAAC,GAAG,KAAK,CAAC,WAAW,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC;QAC9D,MAAM,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAChC,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QACrC,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;IACvC,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/** Unit tests for training with {sample, class} weights. */\n\nimport {memory, ones, Tensor, tensor2d, train, zeros} from '@tensorflow/tfjs-core';\n\nimport * as tfl from '../index';\n\nimport {describeMathCPUAndGPU, expectTensorsClose} from '../utils/test_utils';\nimport {FakeNumericDataset} from './dataset_fakes';\nimport {ClassWeight, ClassWeightMap} from './training_utils';\n\ndescribeMathCPUAndGPU('LayersModel.fit() with classWeight', () => {\n  // Reference Python code:\n  // ```py\n  // import numpy as np\n  // import tensorflow as tf\n  //\n  // model = tf.keras.Sequential()\n  // model.add(tf.keras.layers.Dense(\n  //     units=3,\n  //     input_shape=[2],\n  //     kernel_initializer='zeros',\n  //     activation='softmax'))\n  // model.compile(loss='categorical_crossentropy',\n  //               metrics=['acc'],\n  //               optimizer=tf.keras.optimizers.SGD(1.0))\n  // model.summary()\n  //\n  // xs = np.array([[0, 1], [0, 2], [1, 10], [1, 20], [2, -10], [2, -20]],\n  //               dtype=np.float32)\n  // ys = np.array([[1, 0, 0],\n  //                [1, 0, 0],\n  //                [0, 1, 0],\n  //                [0, 1, 0],\n  //                [0, 0, 1],\n  //                [0, 0, 1]], dtype=np.float32)\n  //\n  // model.fit(xs,\n  //           ys,\n  //           epochs=2,\n  //           class_weight=[{\n  //             0: 1,\n  //             1: 10,\n  //             2: 1\n  //           }])\n  // print(model.get_weights()[0])\n  // ```\n  it('One output, multi-class, one-hot encoding', async () => {\n    const model = tfl.sequential();\n    model.add(tfl.layers.dense({\n      units: 3,\n      inputShape: [2],\n      kernelInitializer: 'zeros',\n      activation: 'softmax'\n    }));\n    model.compile({\n      loss: 'categoricalCrossentropy',\n      metrics: ['acc'],\n      optimizer: train.sgd(1)\n    });\n\n    const xs = tensor2d([[0, 1], [0, 2], [1, 10], [1, 20], [2, -10], [2, -20]]);\n    const ys = tensor2d(\n        [[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1]]);\n    const numTensors0 = memory().numTensors;\n    const history = await model.fit(\n        xs, ys, {epochs: 2, classWeight: [{0: 1, 1: 10, 2: 1}]});\n    expect(memory().numTensors).toEqual(numTensors0);  // Assert no memory leak.\n    expect(history.history.loss.length).toEqual(2);\n    // These loss values are different than what the values would be\n    // if there is no class weighting.\n    expect(history.history.loss[0]).toBeCloseTo(4.3944);\n    expect(history.history.loss[1]).toBeCloseTo(5.3727);\n    expect(history.history.acc.length).toEqual(2);\n    expect(history.history.acc[0]).toBeCloseTo(0.3333);\n    expect(history.history.acc[1]).toBeCloseTo(0.6667);\n  });\n\n  it('One output, multi-class, one-hot encoding, batchSize = 2', async () => {\n    const model = tfl.sequential();\n    model.add(tfl.layers.dense({\n      units: 3,\n      inputShape: [2],\n      kernelInitializer: 'zeros',\n      activation: 'softmax'\n    }));\n    model.compile({\n      loss: 'categoricalCrossentropy',\n      metrics: ['acc'],\n      optimizer: train.sgd(1)\n    });\n\n    const xs = tensor2d([[0, 1], [0, 2], [1, 10], [1, 20], [2, -10], [2, -20]]);\n    const ys = tensor2d(\n        [[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1]]);\n    const numTensors0 = memory().numTensors;\n    const history = await model.fit(xs, ys, {\n      epochs: 2,\n      batchSize: 2,\n      shuffle: false,\n      classWeight: [{0: 1, 1: 10, 2: 1}]\n    });\n    expect(memory().numTensors).toEqual(numTensors0);  // Assert no memory leak.\n    expect(history.history.loss.length).toEqual(2);\n    // These loss values are different than what the values would be\n    // if there is no class weighting.\n    expect(history.history.loss[0]).toBeCloseTo(59.2691);\n    expect(history.history.loss[1]).toBeCloseTo(10.7454);\n    expect(history.history.acc.length).toEqual(2);\n    expect(history.history.acc[0]).toBeCloseTo(0.3333);\n    expect(history.history.acc[1]).toBeCloseTo(0.3333);\n  });\n\n  it('One output, multi-class, one-hot encoding, validationData', async () => {\n    const model = tfl.sequential();\n    model.add(tfl.layers.dense({\n      units: 3,\n      inputShape: [2],\n      kernelInitializer: 'zeros',\n      activation: 'softmax'\n    }));\n    model.compile({\n      loss: 'categoricalCrossentropy',\n      metrics: ['acc'],\n      optimizer: train.sgd(1)\n    });\n\n    const xs = tensor2d([[0, 1], [0, 2], [1, 10], [1, 20], [2, -10], [2, -20]]);\n    const ys = tensor2d(\n        [[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1]]);\n    const numTensors0 = memory().numTensors;\n    const history = await model.fit(xs, ys, {\n      epochs: 2,\n      classWeight: [{0: 1, 1: 10, 2: 1}],\n      validationData: [xs, ys]\n    });\n    expect(memory().numTensors).toEqual(numTensors0);  // Assert no memory leak.\n    expect(history.history.loss.length).toEqual(2);\n    // These loss values are different than what the values would be\n    // if there is no class weighting.\n    expect(history.history.loss[0]).toBeCloseTo(4.3944);\n    expect(history.history.loss[1]).toBeCloseTo(5.3727);\n    expect(history.history.acc.length).toEqual(2);\n    expect(history.history.acc[0]).toBeCloseTo(0.3333);\n    expect(history.history.acc[1]).toBeCloseTo(0.6667);\n\n    expect(history.history.val_loss.length).toEqual(2);\n    expect(history.history.val_loss[0]).toBeCloseTo(5.3727);\n    expect(history.history.val_loss[1]).toBeCloseTo(5.3727);\n    expect(history.history.val_acc.length).toEqual(2);\n    expect(history.history.val_acc[0]).toBeCloseTo(0.6667);\n    expect(history.history.val_acc[1]).toBeCloseTo(0.6667);\n  });\n\n  it('One output, multi-class, one-hot encoding, validationSplit', async () => {\n    const model = tfl.sequential();\n    model.add(tfl.layers.dense({\n      units: 3,\n      inputShape: [2],\n      kernelInitializer: 'zeros',\n      activation: 'softmax'\n    }));\n    model.compile({\n      loss: 'categoricalCrossentropy',\n      metrics: ['acc'],\n      optimizer: train.sgd(1)\n    });\n\n    const xs = tensor2d([[0, 1], [0, 2], [1, 10], [1, 20], [2, -10], [2, -20]]);\n    const ys = tensor2d(\n        [[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1]]);\n    const numTensors0 = memory().numTensors;\n    const history = await model.fit(\n        xs, ys,\n        {epochs: 2, classWeight: [{0: 1, 1: 10, 2: 1}], validationSplit: 0.5});\n    expect(memory().numTensors).toEqual(numTensors0);  // Assert no memory leak.\n    expect(history.history.loss.length).toEqual(2);\n    // These loss values are different than what the values would be\n    // if there is no class weighting.\n    expect(history.history.loss[0]).toBeCloseTo(4.3944);\n    expect(history.history.loss[1]).toBeCloseTo(10.7454);\n    expect(history.history.acc.length).toEqual(2);\n    expect(history.history.acc[0]).toBeCloseTo(0.6667);\n    expect(history.history.acc[1]).toBeCloseTo(0.3333);\n\n    expect(history.history.val_loss.length).toEqual(2);\n    expect(history.history.val_loss[0]).toBeCloseTo(2.9903e-05);\n    expect(history.history.val_loss[1]).toBeCloseTo(2.9903e-05);\n    expect(history.history.val_acc.length).toEqual(2);\n    expect(history.history.val_acc[0]).toBeCloseTo(1);\n    expect(history.history.val_acc[1]).toBeCloseTo(1);\n  });\n\n  it('One output, multi-class, sparse encoding', async () => {\n    const model = tfl.sequential();\n    model.add(tfl.layers.dense({\n      units: 3,\n      inputShape: [2],\n      kernelInitializer: 'zeros',\n      activation: 'softmax'\n    }));\n    model.compile(\n        {loss: 'sparseCategoricalCrossentropy', optimizer: train.sgd(1)});\n\n    const xs = tensor2d([[0, 1], [0, 2], [1, 10], [1, 20], [2, -10], [2, -20]]);\n    const ys = tensor2d([[0], [0], [1], [1], [2], [2]]);\n    const numTensors0 = memory().numTensors;\n    const history =\n        await model.fit(xs, ys, {epochs: 2, classWeight: {0: 1, 1: 10, 2: 1}});\n    expect(memory().numTensors).toEqual(numTensors0);  // Assert no memory leak.\n    expect(history.history.loss.length).toEqual(2);\n    // These loss values are different than what the values would be\n    // if there is no class weighting.\n    expect(history.history.loss[0]).toBeCloseTo(4.3944);\n    expect(history.history.loss[1]).toBeCloseTo(5.3727);\n  });\n\n  // Reference Python code.\n  // ```py\n  // import numpy as np\n  // import tensorflow as tf\n  //\n  // model = tf.keras.Sequential()\n  // model.add(tf.keras.layers.Dense(\n  //     units=1,\n  //     input_shape=[2],\n  //     kernel_initializer='zeros',\n  //     activation='sigmoid'))\n  // model.compile(loss='binary_crossentropy',\n  //               optimizer=tf.keras.optimizers.SGD(1.0))\n  // model.summary()\n  //\n  // xs = np.array([[0, 1], [0, 2], [1, 10], [1, 20]],\n  //               dtype=np.float32)\n  // ys = np.array([[0], [0], [1], [1]], dtype=np.float32)\n  //\n  // # model.fit(xs, ys, epochs=1)\n  // model.fit(xs,\n  //           ys,\n  //           epochs=3,\n  //           class_weight=[{\n  //               0: 0.1,\n  //               1: 0.9\n  //           }])\n  // print(model.get_weights()[0])\n  // ```\n  it('One output, binary classes, sparse encoding', async () => {\n    const model = tfl.sequential();\n    model.add(tfl.layers.dense({\n      units: 1,\n      inputShape: [2],\n      kernelInitializer: 'zeros',\n      activation: 'sigmoid'\n    }));\n    model.compile({loss: 'binaryCrossentropy', optimizer: train.sgd(1)});\n\n    const xs = tensor2d([[0, 1], [0, 2], [1, 10], [1, 20]]);\n    const ys = tensor2d([[0], [0], [1], [1]]);\n    const numTensors0 = memory().numTensors;\n    const history =\n        await model.fit(xs, ys, {epochs: 2, classWeight: [{0: 0.1, 1: 0.9}]});\n    expect(memory().numTensors).toEqual(numTensors0);  // Assert no memory leak.\n    expect(history.history.loss.length).toEqual(2);\n    // These loss values are different than what the values would be\n    // if there is no class weighting.\n    expect(history.history.loss[0]).toBeCloseTo(0.3466);\n    expect(history.history.loss[1]).toBeCloseTo(0.2611);\n  });\n\n  // Python Reference Code:\n  // ```py\n  // import numpy as np\n  // import tensorflow as tf\n  //\n  // inp = tf.keras.Input(shape=[2])\n  // y1 = tf.keras.layers.Dense(\n  //     units=3,\n  //     input_shape=[2],\n  //     kernel_initializer='zeros',\n  //     activation='softmax')(inp)\n  // y2 = tf.keras.layers.Dense(\n  //     units=1,\n  //     input_shape=[2],\n  //     kernel_initializer='zeros',\n  //     activation='sigmoid')(inp)\n  // model = tf.keras.Model(inp, [y1, y2])\n  // model.compile(\n  //     loss=['sparse_categorical_crossentropy', 'binary_crossentropy'],\n  //     optimizer=tf.keras.optimizers.SGD(1.0))\n  // model.summary()\n  //\n  // xs = np.array(\n  //     [[0, 1], [0, 2], [1, 10], [1, 20], [2, 10], [2, 20]],\n  //     dtype=np.float32)\n  // y1s = np.array(\n  //     [[0], [0], [1], [1], [2], [2]], dtype=np.float32)\n  // y2s = np.array(\n  //     [[0], [0], [1], [1], [1], [1]], dtype=np.float32)\n  //\n  // # model.fit(xs, ys, epochs=1)\n  // model.fit(xs,\n  //           [y1s, y2s],\n  //           epochs=3,\n  //           class_weight=[{\n  //               0: 0.1,\n  //               1: 0.2,\n  //               2: 0.7\n  //           }, {\n  //               0: 0.1,\n  //               1: 0.9\n  //           }])\n  // ```\n  it('Two outputs, classWeight as array', async () => {\n    const inp = tfl.input({shape: [2]});\n    const y1 = tfl.layers\n                   .dense({\n                     units: 3,\n                     inputShape: [2],\n                     kernelInitializer: 'zeros',\n                     activation: 'softmax'\n                   })\n                   .apply(inp) as tfl.SymbolicTensor;\n    const y2 = tfl.layers\n                   .dense({\n                     units: 1,\n                     inputShape: [2],\n                     kernelInitializer: 'zeros',\n                     activation: 'sigmoid'\n                   })\n                   .apply(inp) as tfl.SymbolicTensor;\n    const model = tfl.model({inputs: inp, outputs: [y1, y2]});\n    model.compile({\n      loss: ['sparseCategoricalCrossentropy', 'binaryCrossentropy'],\n      optimizer: train.sgd(1)\n    });\n\n    const xs = tensor2d([[0, 1], [0, 2], [1, 10], [1, 20], [2, 10], [2, 20]]);\n    const y1s = tensor2d([[0], [0], [1], [1], [2], [2]]);\n    const y2s = tensor2d([[0], [0], [1], [1], [1], [1]]);\n\n    const numTensors0 = memory().numTensors;\n    const history = await model.fit(\n        xs, [y1s, y2s],\n        {epochs: 3, classWeight: [{0: 0.1, 1: 0.2, 2: 0.7}, {0: 0.1, 1: 0.9}]});\n    expect(memory().numTensors).toEqual(numTensors0);  // Assert no memory leak.\n    expect(history.history.loss.length).toEqual(3);\n    expect(history.history.loss[0]).toBeCloseTo(0.8052);\n    expect(history.history.loss[1]).toBeCloseTo(1.4887);\n    expect(history.history.loss[2]).toBeCloseTo(1.4782);\n    const lossKey0 = `${model.outputNames[0]}_loss`;\n    expect(history.history[lossKey0].length).toEqual(3);\n    expect(history.history[lossKey0][0]).toBeCloseTo(0.3662);\n    expect(history.history[lossKey0][1]).toBeCloseTo(1.2553);\n    expect(history.history[lossKey0][2]).toBeCloseTo(1.2485);\n    const lossKey1 = `${model.outputNames[1]}_loss`;\n    expect(history.history[lossKey1].length).toEqual(3);\n    expect(history.history[lossKey1][0]).toBeCloseTo(0.4390);\n    expect(history.history[lossKey1][1]).toBeCloseTo(0.2333);\n    expect(history.history[lossKey1][2]).toBeCloseTo(0.2297);\n  });\n\n  it('Two outputs, classWeight as array, one being null', async () => {\n    const inp = tfl.input({shape: [2]});\n    const y1 = tfl.layers\n                   .dense({\n                     units: 3,\n                     inputShape: [2],\n                     kernelInitializer: 'zeros',\n                     activation: 'softmax'\n                   })\n                   .apply(inp) as tfl.SymbolicTensor;\n    const y2 = tfl.layers\n                   .dense({\n                     units: 1,\n                     inputShape: [2],\n                     kernelInitializer: 'zeros',\n                     activation: 'sigmoid'\n                   })\n                   .apply(inp) as tfl.SymbolicTensor;\n    const model = tfl.model({inputs: inp, outputs: [y1, y2]});\n    model.compile({\n      loss: ['sparseCategoricalCrossentropy', 'binaryCrossentropy'],\n      optimizer: train.sgd(1)\n    });\n\n    const xs = tensor2d([[0, 1], [0, 2], [1, 10], [1, 20], [2, 10], [2, 20]]);\n    const y1s = tensor2d([[0], [0], [1], [1], [2], [2]]);\n    const y2s = tensor2d([[0], [0], [1], [1], [1], [1]]);\n\n    const numTensors0 = memory().numTensors;\n    const history = await model.fit(\n        xs, [y1s, y2s],\n        {epochs: 3, classWeight: [null, {0: 0.1, 1: 0.9}], shuffle: false});\n    expect(memory().numTensors).toEqual(numTensors0);  // Assert no memory leak.\n    // Note that the following values don't match results from Python,\n    // which is a bug in Python TensorFlow / tf.keras. But the final\n    // kernel value does match Python results. (See b/136123157).\n    expect(history.history.loss.length).toEqual(3);\n    expect(history.history.loss[0]).toBeCloseTo(1.5376);\n    expect(history.history.loss[1]).toBeCloseTo(3.1466);\n    expect(history.history.loss[2]).toBeCloseTo(7.9620);\n    const lossKey0 = `${model.outputNames[0]}_loss`;\n    expect(history.history[lossKey0].length).toEqual(3);\n    expect(history.history[lossKey0][0]).toBeCloseTo(1.0986);\n    expect(history.history[lossKey0][1]).toBeCloseTo(2.9113);\n    expect(history.history[lossKey0][2]).toBeCloseTo(7.7323);\n    const lossKey1 = `${model.outputNames[1]}_loss`;\n    expect(history.history[lossKey1].length).toEqual(3);\n    expect(history.history[lossKey1][0]).toBeCloseTo(0.4390);\n    expect(history.history[lossKey1][1]).toBeCloseTo(0.2333);\n    expect(history.history[lossKey1][2]).toBeCloseTo(0.2298);\n    expectTensorsClose(model.getWeights()[0], tensor2d([\n                         [-0.3333333, -0.03197281, 0.3653062],\n                         [-2.0025878, 1.9823718, 0.02021614]\n                       ]));\n  });\n\n  it('Two outputs, classWeight as map, one output no weighting', async () => {\n    const inp = tfl.input({shape: [2]});\n    const y1 = tfl.layers\n                   .dense({\n                     units: 3,\n                     inputShape: [2],\n                     kernelInitializer: 'zeros',\n                     activation: 'softmax'\n                   })\n                   .apply(inp) as tfl.SymbolicTensor;\n    const y2 = tfl.layers\n                   .dense({\n                     units: 1,\n                     inputShape: [2],\n                     kernelInitializer: 'zeros',\n                     activation: 'sigmoid'\n                   })\n                   .apply(inp) as tfl.SymbolicTensor;\n    const model = tfl.model({inputs: inp, outputs: [y1, y2]});\n    model.compile({\n      loss: ['sparseCategoricalCrossentropy', 'binaryCrossentropy'],\n      optimizer: train.sgd(1)\n    });\n\n    const xs = tensor2d([[0, 1], [0, 2], [1, 10], [1, 20], [2, 10], [2, 20]]);\n    const y1s = tensor2d([[0], [0], [1], [1], [2], [2]]);\n    const y2s = tensor2d([[0], [0], [1], [1], [1], [1]]);\n\n    const numTensors0 = memory().numTensors;\n    const history = await model.fit(xs, [y1s, y2s], {\n      epochs: 3,\n      classWeight: {\n        [model.outputNames[1]]: {0: 0.1, 1: 0.9},\n      },\n      shuffle: false\n    });\n    expect(memory().numTensors).toEqual(numTensors0);  // Assert no memory leak.\n    // Note that the following values don't match results from Python,\n    // which is a bug in Python TensorFlow / tf.keras. But the final\n    // kernel value does match Python results.\n    expect(history.history.loss.length).toEqual(3);\n    expect(history.history.loss[0]).toBeCloseTo(1.5376);\n    expect(history.history.loss[1]).toBeCloseTo(3.1466);\n    expect(history.history.loss[2]).toBeCloseTo(7.9620);\n    const lossKey0 = `${model.outputNames[0]}_loss`;\n    expect(history.history[lossKey0].length).toEqual(3);\n    expect(history.history[lossKey0][0]).toBeCloseTo(1.0986);\n    expect(history.history[lossKey0][1]).toBeCloseTo(2.9113);\n    expect(history.history[lossKey0][2]).toBeCloseTo(7.7323);\n    const lossKey1 = `${model.outputNames[1]}_loss`;\n    expect(history.history[lossKey1].length).toEqual(3);\n    expect(history.history[lossKey1][0]).toBeCloseTo(0.4390);\n    expect(history.history[lossKey1][1]).toBeCloseTo(0.2333);\n    expect(history.history[lossKey1][2]).toBeCloseTo(0.2298);\n    expectTensorsClose(model.getWeights()[0], tensor2d([\n                         [-0.3333333, -0.03197281, 0.3653062],\n                         [-2.0025878, 1.9823718, 0.02021614]\n                       ]));\n  });\n\n  it('Two outputs, classWeight as map', async () => {\n    const inp = tfl.input({shape: [2]});\n    const y1 = tfl.layers\n                   .dense({\n                     units: 3,\n                     inputShape: [2],\n                     kernelInitializer: 'zeros',\n                     activation: 'softmax'\n                   })\n                   .apply(inp) as tfl.SymbolicTensor;\n    const y2 = tfl.layers\n                   .dense({\n                     units: 1,\n                     inputShape: [2],\n                     kernelInitializer: 'zeros',\n                     activation: 'sigmoid'\n                   })\n                   .apply(inp) as tfl.SymbolicTensor;\n    const model = tfl.model({inputs: inp, outputs: [y1, y2]});\n    model.compile({\n      loss: ['sparseCategoricalCrossentropy', 'binaryCrossentropy'],\n      optimizer: train.sgd(1)\n    });\n\n    const xs = tensor2d([[0, 1], [0, 2], [1, 10], [1, 20], [2, 10], [2, 20]]);\n    const y1s = tensor2d([[0], [0], [1], [1], [2], [2]]);\n    const y2s = tensor2d([[0], [0], [1], [1], [1], [1]]);\n\n    const numTensors0 = memory().numTensors;\n    const history = await model.fit(xs, [y1s, y2s], {\n      epochs: 3,\n      classWeight: {\n        [model.outputNames[0]]: {0: 0.1, 1: 0.2, 2: 0.7},\n        [model.outputNames[1]]: {0: 0.1, 1: 0.9}\n      }\n    });\n    expect(memory().numTensors).toEqual(numTensors0);  // Assert no memory leak.\n    expect(history.history.loss.length).toEqual(3);\n    expect(history.history.loss[0]).toBeCloseTo(0.8052);\n    expect(history.history.loss[1]).toBeCloseTo(1.4887);\n    expect(history.history.loss[2]).toBeCloseTo(1.4782);\n    const lossKey0 = `${model.outputNames[0]}_loss`;\n    expect(history.history[lossKey0].length).toEqual(3);\n    expect(history.history[lossKey0][0]).toBeCloseTo(0.3662);\n    expect(history.history[lossKey0][1]).toBeCloseTo(1.2553);\n    expect(history.history[lossKey0][2]).toBeCloseTo(1.2485);\n    const lossKey1 = `${model.outputNames[1]}_loss`;\n    expect(history.history[lossKey1].length).toEqual(3);\n    expect(history.history[lossKey1][0]).toBeCloseTo(0.4390);\n    expect(history.history[lossKey1][1]).toBeCloseTo(0.2333);\n    expect(history.history[lossKey1][2]).toBeCloseTo(0.2297);\n  });\n});\n\ndescribeMathCPUAndGPU('LayersModel.fitDataset() with classWeight', () => {\n  // Reference Python code:\n  // ```py\n  // import numpy as np\n  // import tensorflow as tf\n  //\n  // batch_size = 8\n  // num_batches = 3\n  // epochs = 2\n  //\n  // xs = np.ones([batch_size * num_batches * epochs, 1])\n  // ys = np.concatenate([\n  //     np.zeros([int(batch_size * num_batches * epochs / 2), 1]),\n  //     np.ones([int(batch_size * num_batches * epochs / 2), 1])],\n  //     axis=0)\n  //\n  // dataset = tf.data.Dataset.from_tensor_slices(\n  //     (xs, ys)).batch(batch_size)\n  //\n  // model = tf.keras.Sequential()\n  // model.add(tf.keras.layers.Dense(\n  //     1,\n  //     input_shape=[1],\n  //     activation='sigmoid',\n  //     kernel_initializer='zeros'))\n  // model.compile(loss='binary_crossentropy',\n  //               metrics=['acc'],\n  //               optimizer='sgd')\n  //\n  // history = model.fit(\n  //     dataset,\n  //     steps_per_epoch=num_batches,\n  //     epochs=epochs,\n  //     class_weight={0: 0.1, 1: 2})\n  // print(history.history)\n  // ```\n  it('One output, binary crossentropy, acc metric', async () => {\n    const model = tfl.sequential();\n    model.add(tfl.layers.dense({\n      units: 1,\n      inputShape: [1],\n      kernelInitializer: 'zeros',\n      activation: 'sigmoid'\n    }));\n    model.compile(\n        {loss: 'binaryCrossentropy', metrics: ['acc'], optimizer: 'sgd'});\n\n    const batchSize = 8;\n    const batchesPerEpoch = 3;\n    const epochs = 2;\n    const xTensorsFunc = () =>\n        [ones([batchSize, 1]), ones([batchSize, 1]), ones([batchSize, 1]),\n         ones([batchSize, 1]), ones([batchSize, 1]), ones([batchSize, 1])];\n    const yTensorsFunc = () =>\n        [zeros([batchSize, 1]), zeros([batchSize, 1]), zeros([batchSize, 1]),\n         ones([batchSize, 1]), ones([batchSize, 1]), ones([batchSize, 1])];\n    const dataset = new FakeNumericDataset({\n      xShape: [1],\n      yShape: [1],\n      batchSize,\n      numBatches: batchesPerEpoch * epochs,\n      xTensorsFunc,\n      yTensorsFunc\n    });\n\n    const classWeight: ClassWeight = {0: 0.1, 1: 2};\n\n    // Do a burn-in call to account for initialization of cached tensors (for\n    // the memory-leak check below).\n    await model.fitDataset(dataset, {batchesPerEpoch, epochs: 1, classWeight});\n    model.setWeights([zeros([1, 1]), zeros([1])]);\n\n    const numTensors0 = memory().numTensors;\n    const history =\n        await model.fitDataset(dataset, {batchesPerEpoch, epochs, classWeight});\n    const numTensors1 = memory().numTensors;\n    expect(numTensors1).toEqual(numTensors0);\n\n    // The loss and acc values are different if no classWeight is used.\n    expect(history.history.loss.length).toEqual(2);\n    expect(history.history.loss[0]).toBeCloseTo(0.0693);\n    expect(history.history.loss[1]).toBeCloseTo(1.3695);\n\n    expect(history.history.acc.length).toEqual(2);\n    expect(history.history.acc[0]).toBeCloseTo(1);\n    expect(history.history.acc[1]).toBeCloseTo(0.6667);\n  });\n\n  // Reference Python code:\n  // ```py\n  // import numpy as np\n  // import tensorflow as tf\n  //\n  // batch_size = 8\n  // num_batches = 3\n  // epochs = 2\n  //\n  // xs = np.ones([batch_size * num_batches * epochs, 1])\n  // ys = np.concatenate([\n  //     np.zeros([int(batch_size * num_batches * epochs / 2), 1]),\n  //     np.ones([int(batch_size * num_batches * epochs / 2), 1])],\n  //     axis=0)\n  //\n  // dataset = tf.data.Dataset.from_tensor_slices(\n  //     (xs, ys)).batch(batch_size)\n  //\n  // model = tf.keras.Sequential()\n  // model.add(tf.keras.layers.Dense(\n  //     1,\n  //     input_shape=[1],\n  //     activation='sigmoid',\n  //     kernel_initializer='zeros'))\n  // model.compile(loss='binary_crossentropy',\n  //               metrics=['acc'],\n  //               optimizer='sgd')\n  //\n  // history = model.fit(\n  //     dataset,\n  //     steps_per_epoch=num_batches,\n  //     epochs=epochs,\n  //     class_weight={0: 0.1, 1: 2})\n  // print(history.history)\n  // ```\n  it('One output, binary crossentropyvalidation', async () => {\n    const model = tfl.sequential();\n    model.add(tfl.layers.dense({\n      units: 1,\n      inputShape: [1],\n      kernelInitializer: 'zeros',\n      activation: 'sigmoid'\n    }));\n    model.compile(\n        {loss: 'binaryCrossentropy', metrics: ['acc'], optimizer: 'sgd'});\n\n    const batchSize = 8;\n    const batchesPerEpoch = 3;\n    const epochs = 2;\n    const xTensorsFunc = () =>\n        [ones([batchSize, 1]), ones([batchSize, 1]), ones([batchSize, 1]),\n         ones([batchSize, 1]), ones([batchSize, 1]), ones([batchSize, 1])];\n    const yTensorsFunc = () =>\n        [zeros([batchSize, 1]), zeros([batchSize, 1]), zeros([batchSize, 1]),\n         ones([batchSize, 1]), ones([batchSize, 1]), ones([batchSize, 1])];\n    const dataset = new FakeNumericDataset({\n      xShape: [1],\n      yShape: [1],\n      batchSize,\n      numBatches: batchesPerEpoch * epochs,\n      xTensorsFunc,\n      yTensorsFunc\n    });\n\n    const classWeight: ClassWeight = {0: 0.1, 1: 2};\n\n    // Do a burn-in call to account for initialization of cached tensors (for\n    // the memory-leak check below).\n    await model.fitDataset(\n        dataset,\n        {batchesPerEpoch, epochs: 1, classWeight, validationData: dataset});\n    model.setWeights([zeros([1, 1]), zeros([1])]);\n\n    const numTensors0 = memory().numTensors;\n    const history = await model.fitDataset(\n        dataset,\n        {batchesPerEpoch, epochs, classWeight, validationData: dataset});\n    const numTensors1 = memory().numTensors;\n    expect(numTensors1).toEqual(numTensors0);\n\n    // The loss and acc values are different if no classWeight is used.\n    expect(history.history.loss.length).toEqual(2);\n    expect(history.history.loss[0]).toBeCloseTo(0.0693);\n    expect(history.history.loss[1]).toBeCloseTo(1.3695);\n\n    expect(history.history.acc.length).toEqual(2);\n    expect(history.history.acc[0]).toBeCloseTo(1);\n    expect(history.history.acc[1]).toBeCloseTo(0.6667);\n\n    expect(history.history.val_loss.length).toEqual(2);\n    expect(history.history.val_loss[0]).toBeCloseTo(0.693148);\n    expect(history.history.val_loss[1]).toBeCloseTo(0.693546);\n\n    expect(history.history.val_acc.length).toEqual(2);\n    expect(history.history.val_acc[0]).toBeCloseTo(0.5);\n    expect(history.history.val_acc[1]).toBeCloseTo(0.5);\n  });\n\n  // Reference Python code:\n  // ```py\n  // import numpy as np\n  // import tensorflow as tf\n  //\n  // batch_size = 8\n  // num_batches = 3\n  // epochs = 2\n  //\n  // xs = np.ones([batch_size * num_batches * epochs, 1])\n  // ys1 = np.concatenate([\n  //     np.zeros([int(batch_size * num_batches * epochs / 2), 1]),\n  //     np.ones([int(batch_size * num_batches * epochs / 2), 1])],\n  //     axis=0)\n  // ys2 = np.concatenate([\n  //     np.ones([int(batch_size * num_batches * epochs / 2), 1]),\n  //     np.zeros([int(batch_size * num_batches * epochs / 2), 1])],\n  //     axis=0)\n  //\n  // dataset = tf.data.Dataset.from_tensor_slices(\n  //     (xs, {'out1': ys1, 'out2': ys2})).batch(batch_size)\n  //\n  // inp = tf.keras.Input(shape=[1])\n  // out1 = tf.keras.layers.Dense(\n  //     1,\n  //     input_shape=[1],\n  //     activation='sigmoid',\n  //     kernel_initializer='zeros',\n  //     name='out1')(inp)\n  // out2 = tf.keras.layers.Dense(\n  //     1,\n  //     input_shape=[1],\n  //     activation='sigmoid',\n  //     kernel_initializer='zeros',\n  //     name='out2')(inp)\n  //\n  // model = tf.keras.Model(inp, [out1, out2])\n  // model.compile(\n  //     loss=['binary_crossentropy', 'binary_crossentropy'],\n  //     optimizer='sgd')\n  //\n  // history = model.fit(\n  //     dataset,\n  //     steps_per_epoch=num_batches,\n  //     epochs=epochs,\n  //     class_weight=[{0: 0.1, 1: 2}, {0: 5, 1: 0.5}])\n  // print(history.history)\n  // ```\n  it('Two outputs, binary crossentropy, acc metric', async () => {\n    const inp = tfl.input({shape: [1]});\n    const out1 = tfl.layers\n                     .dense({\n                       units: 1,\n                       inputShape: [1],\n                       kernelInitializer: 'zeros',\n                       activation: 'sigmoid'\n                     })\n                     .apply(inp) as tfl.SymbolicTensor;\n    const out2 = tfl.layers\n                     .dense({\n                       units: 1,\n                       inputShape: [1],\n                       kernelInitializer: 'zeros',\n                       activation: 'sigmoid'\n                     })\n                     .apply(inp) as tfl.SymbolicTensor;\n    const model = tfl.model({inputs: inp, outputs: [out1, out2]});\n    model.compile(\n        {loss: ['binaryCrossentropy', 'binaryCrossentropy'], optimizer: 'sgd'});\n\n    const batchSize = 8;\n    const batchesPerEpoch = 3;\n    const epochs = 2;\n    const xTensorsFunc = () =>\n        [ones([batchSize, 1]), ones([batchSize, 1]), ones([batchSize, 1]),\n         ones([batchSize, 1]), ones([batchSize, 1]), ones([batchSize, 1])];\n    const yTensorsFunc = () => {\n      const output: {[name: string]: Tensor[]} = {};\n      output[model.outputNames[0]] = [\n        zeros([batchSize, 1]), zeros([batchSize, 1]), zeros([batchSize, 1]),\n        ones([batchSize, 1]), ones([batchSize, 1]), ones([batchSize, 1])\n      ];\n      output[model.outputNames[1]] = [\n        ones([batchSize, 1]), ones([batchSize, 1]), ones([batchSize, 1]),\n        zeros([batchSize, 1]), zeros([batchSize, 1]), zeros([batchSize, 1])\n      ];\n      return output;\n    };\n    const dataset = new FakeNumericDataset({\n      xShape: [1],\n      yShape: {[model.outputNames[0]]: [1], [model.outputNames[1]]: [1]},\n      batchSize,\n      numBatches: batchesPerEpoch * epochs,\n      xTensorsFunc,\n      yTensorsFunc\n    });\n\n    const classWeight: ClassWeightMap = {\n      [model.outputNames[0]]: {0: 0.1, 1: 2},\n      [model.outputNames[1]]: {0: 5, 1: 0.5}\n    };\n\n    // Do a burn-in call to account for initialization of cached tensors (for\n    // the memory-leak check below).\n    await model.fitDataset(dataset, {batchesPerEpoch, epochs: 1, classWeight});\n    // model.setWeights([zeros([1, 1]), zeros([1])]);\n\n    const numTensors0 = memory().numTensors;\n    const history =\n        await model.fitDataset(dataset, {batchesPerEpoch, epochs, classWeight});\n    const numTensors1 = memory().numTensors;\n    expect(numTensors1).toEqual(numTensors0);\n\n    // The loss and acc values are different if no classWeight is used.\n    const totalLoss = history.history.loss;\n    expect(totalLoss.length).toEqual(2);\n    expect(totalLoss[0]).toBeCloseTo(0.4146);\n    expect(totalLoss[1]).toBeCloseTo(4.7882);\n\n    const loss0 = history.history[`${model.outputNames[0]}_loss`];\n    expect(loss0.length).toEqual(2);\n    expect(loss0[0]).toBeCloseTo(0.0693);\n    expect(loss0[1]).toBeCloseTo(1.3695);\n\n    const loss1 = history.history[`${model.outputNames[1]}_loss`];\n    expect(loss1.length).toEqual(2);\n    expect(loss1[0]).toBeCloseTo(0.3453);\n    expect(loss1[1]).toBeCloseTo(3.4158);\n  });\n});\n"]}