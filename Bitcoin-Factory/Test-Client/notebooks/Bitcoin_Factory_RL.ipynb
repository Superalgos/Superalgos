{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a951a47f-88f5-40f0-8fe3-5495f76633b5",
   "metadata": {
    "id": "a951a47f-88f5-40f0-8fe3-5495f76633b5"
   },
   "source": [
    "## Import needed deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c3b02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import ray\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "from ray import tune\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.tune import CLIReporter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12a2f3c",
   "metadata": {},
   "source": [
    "## Run tensorboard for data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf54c19d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir \"/tf/notebooks/ray_results/\" --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb19fc",
   "metadata": {},
   "source": [
    "### Set of parameters received from Test Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a97178f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIST_OF_ASSETS</th>\n",
       "      <th>LIST_OF_TIMEFRAMES</th>\n",
       "      <th>NUMBER_OF_INDICATORS_PROPERTIES</th>\n",
       "      <th>TIMESTEPS_TO_TRAIN</th>\n",
       "      <th>OBSERVATION_WINDOW_SIZE</th>\n",
       "      <th>INITIAL_QUOTE_ASSET</th>\n",
       "      <th>INITIAL_BASE_ASSET</th>\n",
       "      <th>TRADING_FEE</th>\n",
       "      <th>ENV_NAME</th>\n",
       "      <th>ENV_VERSION</th>\n",
       "      <th>...</th>\n",
       "      <th>CANDLES_CANDLES-VOLUMES_CANDLES_CANDLE_CLOSE</th>\n",
       "      <th>CANDLES_CANDLES-VOLUMES_CANDLES_CANDLE_OPEN</th>\n",
       "      <th>CANDLES_CANDLES-VOLUMES_VOLUMES_VOLUME_BUY</th>\n",
       "      <th>HOUR_OF_DAY</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>WEEK_OF_YEAR</th>\n",
       "      <th>MONTH_OF_YEAR</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MASTERS_RESISTANCES-AND-SUPPORTS_RESISTANCES_RESISTANCE_RESISTANCE1RATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BTC</td>\n",
       "      <td>01-hs</td>\n",
       "      <td>7</td>\n",
       "      <td>800000</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>BTCAccumulationEnv</td>\n",
       "      <td>v1</td>\n",
       "      <td>...</td>\n",
       "      <td>ON</td>\n",
       "      <td>ON</td>\n",
       "      <td>ON</td>\n",
       "      <td>OFF</td>\n",
       "      <td>OFF</td>\n",
       "      <td>OFF</td>\n",
       "      <td>OFF</td>\n",
       "      <td>OFF</td>\n",
       "      <td>OFF</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  LIST_OF_ASSETS LIST_OF_TIMEFRAMES  NUMBER_OF_INDICATORS_PROPERTIES  \\\n",
       "0            BTC              01-hs                                7   \n",
       "\n",
       "   TIMESTEPS_TO_TRAIN  OBSERVATION_WINDOW_SIZE  INITIAL_QUOTE_ASSET  \\\n",
       "0              800000                       14                    0   \n",
       "\n",
       "   INITIAL_BASE_ASSET  TRADING_FEE            ENV_NAME ENV_VERSION  ...  \\\n",
       "0                   1       0.0075  BTCAccumulationEnv          v1  ...   \n",
       "\n",
       "  CANDLES_CANDLES-VOLUMES_CANDLES_CANDLE_CLOSE  \\\n",
       "0                                           ON   \n",
       "\n",
       "   CANDLES_CANDLES-VOLUMES_CANDLES_CANDLE_OPEN  \\\n",
       "0                                           ON   \n",
       "\n",
       "  CANDLES_CANDLES-VOLUMES_VOLUMES_VOLUME_BUY  HOUR_OF_DAY  DAY_OF_MONTH  \\\n",
       "0                                         ON          OFF           OFF   \n",
       "\n",
       "   DAY_OF_WEEK WEEK_OF_YEAR MONTH_OF_YEAR  YEAR  \\\n",
       "0          OFF          OFF           OFF   OFF   \n",
       "\n",
       "   MASTERS_RESISTANCES-AND-SUPPORTS_RESISTANCES_RESISTANCE_RESISTANCE1RATE  \n",
       "0                                                OFF                        \n",
       "\n",
       "[1 rows x 39 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = pd.read_csv(\n",
    "    '/tf/notebooks/parameters.csv', \n",
    "    sep=' ', \n",
    ")\n",
    "\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb523ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"Trading_Signal_Predictor_RL_V01\"\n",
    "PERCENTAGE_OF_DATASET_FOR_TRAINING = 80\n",
    "TIMESTEPS_TO_TRAIN = parameters['TIMESTEPS_TO_TRAIN'][0]\n",
    "OBSERVATION_WINDOW_SIZE = parameters['OBSERVATION_WINDOW_SIZE'][0]\n",
    "INITIAL_QUOTE_ASSET = parameters['INITIAL_QUOTE_ASSET'][0]\n",
    "INITIAL_BASE_ASSET = parameters['INITIAL_BASE_ASSET'][0]\n",
    "TRADING_FEE = parameters['TRADING_FEE'][0]\n",
    "ENV_VERSION = parameters['ENV_VERSION'][0]\n",
    "ENV_NAME =  parameters['ENV_NAME'][0]\n",
    "EXPLORE_ON_EVAL = parameters['EXPLORE_ON_EVAL'][0]\n",
    "\n",
    "# Hyper-parameters, in case we want to really control them from the test server not from ray\n",
    "ALGORITHM = parameters['ALGORITHM'][0]\n",
    "ROLLOUT_FRAGMENT_LENGTH = parameters['ROLLOUT_FRAGMENT_LENGTH'][0]\n",
    "TRAIN_BATCH_SIZE = parameters['TRAIN_BATCH_SIZE'][0]\n",
    "SGD_MINIBATCH_SIZE = parameters['SGD_MINIBATCH_SIZE'][0]\n",
    "BATCH_MODE = parameters['BATCH_MODE'][0]\n",
    "#VF_CLIP_PARAM = parameters['VF_CLIP_PARAM'][0]\n",
    "FC_SIZE = [parameters['FC_SIZE'][0]]\n",
    "LEARNING_RATE = parameters['LEARNING_RATE'][0]\n",
    "GAMMA = parameters['GAMMA'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "faefa7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    '/tf/notebooks/time-series.csv', \n",
    "    header=0, \n",
    "    index_col=None,\n",
    "    sep=' ', \n",
    "    skipinitialspace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1c061c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>BTC-CANDLE-OPEN-01-HS-1.1</th>\n",
       "      <th>BTC-VOLUME-BUY-01-HS-1.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 12:00 AM</td>\n",
       "      <td>7196.25</td>\n",
       "      <td>7175.46</td>\n",
       "      <td>7177.02</td>\n",
       "      <td>7195.24</td>\n",
       "      <td>255</td>\n",
       "      <td>7195.24</td>\n",
       "      <td>255.907451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 01:00 AM</td>\n",
       "      <td>7230.00</td>\n",
       "      <td>7175.71</td>\n",
       "      <td>7216.27</td>\n",
       "      <td>7176.47</td>\n",
       "      <td>441</td>\n",
       "      <td>7176.47</td>\n",
       "      <td>441.526301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 02:00 AM</td>\n",
       "      <td>7244.87</td>\n",
       "      <td>7211.41</td>\n",
       "      <td>7242.85</td>\n",
       "      <td>7215.52</td>\n",
       "      <td>327</td>\n",
       "      <td>7215.52</td>\n",
       "      <td>327.578405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 03:00 AM</td>\n",
       "      <td>7245.00</td>\n",
       "      <td>7220.00</td>\n",
       "      <td>7225.01</td>\n",
       "      <td>7242.66</td>\n",
       "      <td>391</td>\n",
       "      <td>7242.66</td>\n",
       "      <td>391.862434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 04:00 AM</td>\n",
       "      <td>7230.00</td>\n",
       "      <td>7215.03</td>\n",
       "      <td>7217.27</td>\n",
       "      <td>7225.00</td>\n",
       "      <td>233</td>\n",
       "      <td>7225.00</td>\n",
       "      <td>233.906289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20558</th>\n",
       "      <td>2022-05-06 02:00 PM</td>\n",
       "      <td>36246.49</td>\n",
       "      <td>35637.21</td>\n",
       "      <td>36140.18</td>\n",
       "      <td>35747.51</td>\n",
       "      <td>4540</td>\n",
       "      <td>35747.51</td>\n",
       "      <td>4540.128915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20559</th>\n",
       "      <td>2022-05-06 03:00 PM</td>\n",
       "      <td>36436.20</td>\n",
       "      <td>35950.00</td>\n",
       "      <td>36004.34</td>\n",
       "      <td>36140.17</td>\n",
       "      <td>2150</td>\n",
       "      <td>36140.17</td>\n",
       "      <td>2150.953420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20560</th>\n",
       "      <td>2022-05-06 04:00 PM</td>\n",
       "      <td>36235.29</td>\n",
       "      <td>35928.48</td>\n",
       "      <td>36218.01</td>\n",
       "      <td>36004.33</td>\n",
       "      <td>1754</td>\n",
       "      <td>36004.33</td>\n",
       "      <td>1754.438940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20561</th>\n",
       "      <td>2022-05-06 05:00 PM</td>\n",
       "      <td>36239.40</td>\n",
       "      <td>35848.13</td>\n",
       "      <td>35896.43</td>\n",
       "      <td>36218.02</td>\n",
       "      <td>1337</td>\n",
       "      <td>36218.02</td>\n",
       "      <td>1337.432900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20562</th>\n",
       "      <td>2022-05-06 06:00 PM</td>\n",
       "      <td>36100.00</td>\n",
       "      <td>35834.03</td>\n",
       "      <td>35940.01</td>\n",
       "      <td>35896.42</td>\n",
       "      <td>1007</td>\n",
       "      <td>35896.42</td>\n",
       "      <td>1007.166995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20563 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date      high       low     close      open  volume  \\\n",
       "0      2020-01-01 12:00 AM   7196.25   7175.46   7177.02   7195.24     255   \n",
       "1      2020-01-01 01:00 AM   7230.00   7175.71   7216.27   7176.47     441   \n",
       "2      2020-01-01 02:00 AM   7244.87   7211.41   7242.85   7215.52     327   \n",
       "3      2020-01-01 03:00 AM   7245.00   7220.00   7225.01   7242.66     391   \n",
       "4      2020-01-01 04:00 AM   7230.00   7215.03   7217.27   7225.00     233   \n",
       "...                    ...       ...       ...       ...       ...     ...   \n",
       "20558  2022-05-06 02:00 PM  36246.49  35637.21  36140.18  35747.51    4540   \n",
       "20559  2022-05-06 03:00 PM  36436.20  35950.00  36004.34  36140.17    2150   \n",
       "20560  2022-05-06 04:00 PM  36235.29  35928.48  36218.01  36004.33    1754   \n",
       "20561  2022-05-06 05:00 PM  36239.40  35848.13  35896.43  36218.02    1337   \n",
       "20562  2022-05-06 06:00 PM  36100.00  35834.03  35940.01  35896.42    1007   \n",
       "\n",
       "       BTC-CANDLE-OPEN-01-HS-1.1  BTC-VOLUME-BUY-01-HS-1.1  \n",
       "0                        7195.24                255.907451  \n",
       "1                        7176.47                441.526301  \n",
       "2                        7215.52                327.578405  \n",
       "3                        7242.66                391.862434  \n",
       "4                        7225.00                233.906289  \n",
       "...                          ...                       ...  \n",
       "20558                   35747.51               4540.128915  \n",
       "20559                   36140.17               2150.953420  \n",
       "20560                   36004.33               1754.438940  \n",
       "20561                   36218.02               1337.432900  \n",
       "20562                   35896.42               1007.166995  \n",
       "\n",
       "[20563 rows x 8 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_data(df):\n",
    "    # renaming column labels as we wish, regardless what test server sends, hopefully he will maintain position\n",
    "    df.rename(columns={df.columns[0]: \"date\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[1]: \"high\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[2]: \"low\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[3]: \"close\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[4]: \"open\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[5]: \"volume\"}, inplace=True)\n",
    "    \n",
    "    df['volume'] = np.int64(df['volume'])\n",
    "    df['date'] = pd.to_datetime(df['date'],  unit='ms')\n",
    "    df.sort_values(by='date', ascending=True, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['date'] = df['date'].dt.strftime('%Y-%m-%d %I:%M %p')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "data = prepare_data(df)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef6f85-8116-4be7-821d-6f5b897ade5b",
   "metadata": {
    "id": "f6ef6f85-8116-4be7-821d-6f5b897ade5b"
   },
   "source": [
    "# Setup which data to use for training and which data to use for evaluation of RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22d15ac6-4546-4a5d-bf7d-c117cb231f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "\n",
    "    X_train_test, X_valid = \\\n",
    "        train_test_split(data, train_size=0.67, test_size=0.33, shuffle=False)\n",
    "    \n",
    "    X_train, X_test = \\\n",
    "        train_test_split(X_train_test, train_size=0.50, test_size=0.50, shuffle=False)\n",
    "\n",
    "\n",
    "    return X_train, X_test, X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18402c18-25ef-48ea-85e3-0d7178971e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_valid = \\\n",
    "    split_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bfb53-d874-4dc9-9d29-abf27d42cff8",
   "metadata": {},
   "source": [
    "## Normalize the dataset subsets to make the model converge faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdef363e-b5ae-49f6-9acf-6a553fdbbcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_type = MinMaxScaler\n",
    "\n",
    "def get_feature_scalers(X, scaler_type=scaler_type):\n",
    "    scalers = []\n",
    "    for name in list(X.columns[X.columns != 'date']):\n",
    "        scalers.append(scaler_type().fit(X[name].values.reshape(-1, 1)))\n",
    "    return scalers\n",
    "\n",
    "def get_scaler_transforms(X, scalers):\n",
    "    X_scaled = []\n",
    "    for name, scaler in zip(list(X.columns[X.columns != 'date']), scalers):\n",
    "        X_scaled.append(scaler.transform(X[name].values.reshape(-1, 1)))\n",
    "    X_scaled = pd.concat([pd.DataFrame(column, columns=[name]) for name, column in \\\n",
    "                          zip(list(X.columns[X.columns != 'date']), X_scaled)], axis='columns')\n",
    "    return X_scaled\n",
    "\n",
    "def scale_numpy_array(np_arr, scaler_type = scaler_type):\n",
    "    return scaler_type().fit_transform(np_arr, (-1,1))\n",
    "\n",
    "def normalize_data(X_train, X_test, X_valid):\n",
    "    X_train_test = pd.concat([X_train, X_test], axis='index')\n",
    "    X_train_test_valid = pd.concat([X_train_test, X_valid], axis='index')\n",
    "\n",
    "    X_train_test_dates = X_train_test[['date']]\n",
    "    X_train_test_valid_dates = X_train_test_valid[['date']]\n",
    "\n",
    "    X_train_test = X_train_test.drop(columns=['date'])\n",
    "    X_train_test_valid = X_train_test_valid.drop(columns=['date'])\n",
    "\n",
    "    train_test_scalers = \\\n",
    "        get_feature_scalers(X_train_test, \n",
    "                            scaler_type=scaler_type)\n",
    "    train_test_valid_scalers = \\\n",
    "        get_feature_scalers(X_train_test_valid, \n",
    "                            scaler_type=scaler_type)\n",
    "\n",
    "    X_train_test_scaled = \\\n",
    "        get_scaler_transforms(X_train_test, \n",
    "                              train_test_scalers)\n",
    "    X_train_test_valid_scaled = \\\n",
    "        get_scaler_transforms(X_train_test_valid, \n",
    "                              train_test_scalers)\n",
    "    X_train_test_valid_scaled_leaking = \\\n",
    "        get_scaler_transforms(X_train_test_valid, \n",
    "                              train_test_valid_scalers)\n",
    "\n",
    "    X_train_test_scaled = \\\n",
    "        pd.concat([X_train_test_dates, \n",
    "                   X_train_test_scaled], \n",
    "                  axis='columns')\n",
    "    X_train_test_valid_scaled = \\\n",
    "        pd.concat([X_train_test_valid_dates, \n",
    "                   X_train_test_valid_scaled], \n",
    "                  axis='columns')\n",
    "    X_train_test_valid_scaled_leaking = \\\n",
    "        pd.concat([X_train_test_valid_dates, \n",
    "                   X_train_test_valid_scaled_leaking], \n",
    "                  axis='columns')\n",
    "\n",
    "    X_train_scaled = X_train_test_scaled.iloc[:X_train.shape[0]]\n",
    "    X_test_scaled = X_train_test_scaled.iloc[X_train.shape[0]:]\n",
    "    X_valid_scaled = X_train_test_valid_scaled.iloc[X_train_test.shape[0]:]\n",
    "    X_valid_scaled_leaking = X_train_test_valid_scaled_leaking.iloc[X_train_test.shape[0]:]\n",
    "\n",
    "    return (train_test_scalers, \n",
    "            train_test_valid_scalers, \n",
    "            X_train_scaled, \n",
    "            X_test_scaled, \n",
    "            X_valid_scaled, \n",
    "            X_valid_scaled_leaking)\n",
    "\n",
    "train_test_scalers, train_test_valid_scalers, X_train_scaled, X_test_scaled, X_valid_scaled, X_valid_scaled_leaking = \\\n",
    "    normalize_data(X_train, X_test, X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56e34a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>BTC-CANDLE-OPEN-01-HS-1.1</th>\n",
       "      <th>BTC-VOLUME-BUY-01-HS-1.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6883</th>\n",
       "      <td>2020-10-13 07:00 PM</td>\n",
       "      <td>0.113294</td>\n",
       "      <td>0.125382</td>\n",
       "      <td>0.120059</td>\n",
       "      <td>0.119891</td>\n",
       "      <td>0.033352</td>\n",
       "      <td>0.119891</td>\n",
       "      <td>0.033358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6884</th>\n",
       "      <td>2020-10-13 08:00 PM</td>\n",
       "      <td>0.113194</td>\n",
       "      <td>0.125693</td>\n",
       "      <td>0.120675</td>\n",
       "      <td>0.120051</td>\n",
       "      <td>0.024379</td>\n",
       "      <td>0.120051</td>\n",
       "      <td>0.024420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6885</th>\n",
       "      <td>2020-10-13 09:00 PM</td>\n",
       "      <td>0.113707</td>\n",
       "      <td>0.126209</td>\n",
       "      <td>0.121033</td>\n",
       "      <td>0.120666</td>\n",
       "      <td>0.024464</td>\n",
       "      <td>0.120666</td>\n",
       "      <td>0.024464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6886</th>\n",
       "      <td>2020-10-13 10:00 PM</td>\n",
       "      <td>0.113621</td>\n",
       "      <td>0.126171</td>\n",
       "      <td>0.120857</td>\n",
       "      <td>0.121023</td>\n",
       "      <td>0.017988</td>\n",
       "      <td>0.121023</td>\n",
       "      <td>0.018008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6887</th>\n",
       "      <td>2020-10-13 11:00 PM</td>\n",
       "      <td>0.113538</td>\n",
       "      <td>0.126052</td>\n",
       "      <td>0.120601</td>\n",
       "      <td>0.120848</td>\n",
       "      <td>0.015533</td>\n",
       "      <td>0.120848</td>\n",
       "      <td>0.015559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date      high       low     close      open    volume  \\\n",
       "6883  2020-10-13 07:00 PM  0.113294  0.125382  0.120059  0.119891  0.033352   \n",
       "6884  2020-10-13 08:00 PM  0.113194  0.125693  0.120675  0.120051  0.024379   \n",
       "6885  2020-10-13 09:00 PM  0.113707  0.126209  0.121033  0.120666  0.024464   \n",
       "6886  2020-10-13 10:00 PM  0.113621  0.126171  0.120857  0.121023  0.017988   \n",
       "6887  2020-10-13 11:00 PM  0.113538  0.126052  0.120601  0.120848  0.015533   \n",
       "\n",
       "      BTC-CANDLE-OPEN-01-HS-1.1  BTC-VOLUME-BUY-01-HS-1.1  \n",
       "6883                   0.119891                  0.033358  \n",
       "6884                   0.120051                  0.024420  \n",
       "6885                   0.120666                  0.024464  \n",
       "6886                   0.121023                  0.018008  \n",
       "6887                   0.120848                  0.015559  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77071777-9faf-4f19-9f3e-06d52d4805d5",
   "metadata": {
    "id": "77071777-9faf-4f19-9f3e-06d52d4805d5"
   },
   "source": [
    "# Defining the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5e14536-e09b-4a1e-b478-9d2c32fc67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTradingEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['live', 'human', 'none']}\n",
    "    visualization = None\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        \n",
    "        self.df_scaled = config.get(\"df_scaled\").reset_index(drop=True)\n",
    "        self.df_normal = config.get(\"df_normal\").reset_index(drop=True)\n",
    "        self.window_size = OBSERVATION_WINDOW_SIZE\n",
    "        self.prices, self.features = self._process_data(self.df_scaled)\n",
    "        # The shape of the observation is (window_size * features + environment_features) the environment_features are: quote_asset, base_asset, net_worth. The entire observation is flattened in a 1D np array. \n",
    "        # NOT USED ANYMORE, KEPT FOR REFERENCE\n",
    "        # self.obs_shape = ((OBSERVATION_WINDOW_SIZE * self.features.shape[1] + 3),) \n",
    "\n",
    "        # The shape of the observation is number of candles to look back, and the number of features (candle_features) + 3 (quote_asset, base_asset, net_worth)\n",
    "        self.obs_shape = (OBSERVATION_WINDOW_SIZE, self.features.shape[1] + 3)\n",
    "\n",
    "        # Action space\n",
    "        #self.action_space = spaces.Box(low=np.array([0, 0]), high=np.array([3.0, 1.0]), dtype=np.float32)\n",
    "        self.action_space = spaces.MultiDiscrete([3, 100])\n",
    "        # Observation space\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=self.obs_shape, dtype=np.float32)\n",
    "\n",
    "        # Initialize the episode environment\n",
    "\n",
    "        self._start_candle = OBSERVATION_WINDOW_SIZE # We assume that the first observation is not the first row of the dataframe, in order to avoid the case where there are no calculated indicators.\n",
    "        self._end_candle = len(self.features) - 1\n",
    "        self._trading_fee = config.get(\"trading_fee\")\n",
    "\n",
    "        self._quote_asset = None\n",
    "        self._base_asset = None\n",
    "        self._done = None\n",
    "        self._current_candle = None\n",
    "        self._net_worth = None\n",
    "        self._previous_net_worth = None\n",
    "\n",
    "        # Array that will contain observation history needed for appending it to the observation space\n",
    "        # It will contain observations consisting of the net_worth, base_asset and quote_asset as list of floats\n",
    "        # Other features (OHLC + Indicators) will be appended to the current observation in the _get_observation method that takes the data directly from the available dataframe\n",
    "        self._obs_env_history = None\n",
    "\n",
    "        # Render and analysis data\n",
    "        self._total_reward_accumulated = None\n",
    "        self.trade_history = None\n",
    "        self._first_rendering = None\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        self._done = False\n",
    "        self._current_candle = self._start_candle\n",
    "        self._quote_asset = INITIAL_QUOTE_ASSET\n",
    "        self._base_asset = INITIAL_BASE_ASSET \n",
    "        self._net_worth = INITIAL_QUOTE_ASSET # at the begining our net worth is the initial quote asset\n",
    "        self._previous_net_worth = INITIAL_QUOTE_ASSET # at the begining our previous net worth is the initial quote asset\n",
    "        self._total_reward_accumulated = 0.\n",
    "        self._first_rendering = True\n",
    "        self.trade_history = []\n",
    "        self._obs_env_history = []\n",
    "        \n",
    "        self._initial_obs_data()\n",
    "\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        self._done = False\n",
    "        current_price = random.uniform(\n",
    "            self.df_normal.loc[self._current_candle, \"low\"], self.df_normal.loc[self._current_candle, \"high\"])\n",
    "\n",
    "\n",
    "        action_type = action[0]\n",
    "        amount = action[1] / 100\n",
    "        \n",
    "        if action_type == 0: # Buy\n",
    "            # Buy % assets\n",
    "            # Determine the maximum amount of quote asset that can be bought\n",
    "            available_amount_to_buy_with = self._quote_asset / current_price\n",
    "            # Buy only the amount that agent chose\n",
    "            assets_bought = available_amount_to_buy_with * amount\n",
    "            # Update the quote asset balance\n",
    "            self._quote_asset -= assets_bought * current_price\n",
    "            # Update the base asset\n",
    "            self._base_asset += assets_bought\n",
    "            # substract trading fee from base asset based on the amount bought\n",
    "            self._base_asset -= self._trading_fee * assets_bought\n",
    "\n",
    "            # Add to trade history the amount bought if greater than 0\n",
    "            if assets_bought > 0:\n",
    "                self.trade_history.append({'step': self._current_candle, 'type': 'Buy', 'amount': assets_bought, 'price': current_price, 'total' : assets_bought * current_price, 'percent_amount': action[1]})\n",
    "        \n",
    "\n",
    "        elif action_type == 1: # Sell\n",
    "            # Sell % assets\n",
    "            # Determine the amount of base asset that can be sold\n",
    "            amount_to_sell = self._base_asset * amount\n",
    "            received_quote_asset = amount_to_sell * current_price\n",
    "            # Update the quote asset\n",
    "            self._quote_asset += received_quote_asset\n",
    "            # Update the base asset\n",
    "            self._base_asset -= amount_to_sell\n",
    "            \n",
    "            # substract trading fee from quote asset based on the amount sold\n",
    "            self._quote_asset -= self._trading_fee * received_quote_asset\n",
    "\n",
    "            # Add to trade history the amount sold if greater than 0\n",
    "            if amount_to_sell > 0:\n",
    "                self.trade_history.append({'step': self._current_candle, 'type': 'Sell', 'amount': amount_to_sell, 'price': current_price, 'total' : received_quote_asset, 'percent_amount': action[1]})\n",
    "\n",
    "        else:\n",
    "            # Hold\n",
    "            self.trade_history.append({'step': self._current_candle, 'type': 'Hold', 'amount': '0', 'price': current_price, 'total' : 0, 'percent_amount': action[1]})\n",
    "\n",
    "\n",
    "        # Update the current net worth\n",
    "        self._net_worth = self._base_asset * current_price + self._quote_asset\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Returns the next observation, reward, done and info.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._take_action(action)\n",
    "\n",
    "        # Calculate reward comparing the current net worth with the previous net worth\n",
    "        reward = self._net_worth - self._previous_net_worth\n",
    "\n",
    "        self._total_reward_accumulated += reward\n",
    "\n",
    "        # Update the previous net worth to be the current net worth after the reward has been applied\n",
    "        self._previous_net_worth = self._net_worth\n",
    "\n",
    "        obs = self._get_observation()\n",
    "        # Update the info and add it to history data\n",
    "        info = dict (\n",
    "            total_reward_accumulated = self._total_reward_accumulated,\n",
    "            net_worth = self._net_worth,\n",
    "            last_action_type = self.trade_history[-1]['type'] if len(self.trade_history) > 0 else None,\n",
    "            last_action_amount = self.trade_history[-1]['amount'] if len(self.trade_history) > 0 else None,\n",
    "            current_step = self._current_candle\n",
    "        )\n",
    "\n",
    "        self._current_candle += 1\n",
    "\n",
    "        # Update observation history\n",
    "        self._obs_env_history.append([self._net_worth, self._base_asset, self._quote_asset])\n",
    "\n",
    "        self._done = self._net_worth <= 0 or self._current_candle >= (len(\n",
    "            self.df_normal.loc[:, 'open'].values) - 30)# We assume that the last observation is not the last row of the dataframe, in order to avoid the case where there are no calculated indicators.\n",
    "\n",
    "        if self._done:\n",
    "            print('I have finished the episode')\n",
    "        \n",
    "        return obs, reward, self._done, info\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Returns the current observation.\n",
    "        \"\"\"\n",
    "        data_frame = self.features[(self._current_candle - self.window_size):self._current_candle]\n",
    "\n",
    "        obs_env_history = np.array(self._obs_env_history).astype(np.float32)\n",
    "\n",
    "        #TODO We definetely need to scale the observation history in a better way, this might influence training results\n",
    "        # Doing it ad-hoc might change the scale of the min and max, thus changing the results\n",
    "        obs_env_history = preprocessing.minmax_scale(obs_env_history, (-0.9,0.9)) \n",
    "\n",
    "        obs = np.hstack((data_frame, obs_env_history[(self._current_candle - self.window_size):self._current_candle]))\n",
    "\n",
    "        return obs\n",
    "\n",
    "\n",
    "    def render(self, mode='human', **kwargs):\n",
    "        \"\"\"\n",
    "        Renders a plot with trades made by the agent.\n",
    "        \"\"\"\n",
    "        \n",
    "        if mode == 'human':\n",
    "            print(f'Accumulated Reward: {self._total_reward_accumulated} ---- Current Net Worth: {self._net_worth}')\n",
    "            print(f'Current Quote asset: {self._quote_asset} ---- Current Base asset: {self._base_asset}')\n",
    "            print(f'Number of trades: {len(self.trade_history)}')\n",
    "        \n",
    "            if(len(self.trade_history) > 0):\n",
    "                print(f'Last Action: {self.trade_history[-1][\"type\"]} {self.trade_history[-1][\"amount\"]} assets ({self.trade_history[-1][\"percent_amount\"]} %) at price {self.trade_history[-1][\"price\"]}, total: {self.trade_history[-1][\"total\"]}')\n",
    "            print(f'--------------------------------------------------------------------------------------')\n",
    "        elif mode == 'live':\n",
    "            pass\n",
    "            # if self.visualization == None:\n",
    "            #     self.visualization = LiveTradingGraph(self.df_normal, kwargs.get('title', None))\n",
    "\n",
    "            # if self._current_candle > OBSERVATION_WINDOW_SIZE:\n",
    "            #     self.visualization.render(self._current_candle, self._net_worth, self.trade_history, window_size=OBSERVATION_WINDOW_SIZE)\n",
    "\n",
    "    def close(self):\n",
    "        if self.visualization != None:\n",
    "            self.visualization.close()\n",
    "            self.visualization = None\n",
    "         \n",
    "\n",
    "    def _process_data(self, df_scaled):\n",
    "        \"\"\"\n",
    "        Processes the dataframe into features.\n",
    "        \"\"\"\n",
    "        \n",
    "        prices = self.df_scaled.loc[:, 'close'].to_numpy(dtype=np.float32)\n",
    "\n",
    "        data_frame = df_scaled.iloc[:, 1:] # drop first column which is date TODO: Should be probably fixed outside of this class\n",
    "        # Convert df to numpy array\n",
    "        return prices, data_frame.to_numpy(dtype=np.float32)\n",
    "\n",
    "    def _initial_obs_data(self):\n",
    "        for i in range(self.window_size - len(self._obs_env_history)):\n",
    "            self._obs_env_history.append([self._net_worth, self._base_asset, self._quote_asset])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0443761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# infinite number in python\n",
    "MAX_NET_WORTH = 2147483647\n",
    "MAX_NUM_QUOTE_OR_BASE_ASSET = 2147483647\n",
    "\n",
    "INITIAL_QUOTE_ASSET = 0\n",
    "INITIAL_BASE_ASSET = 1\n",
    "OBSERVATION_WINDOW_SIZE = 24 # Probably we should put it as param ?\n",
    "\n",
    "class BTCAccumulationEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['live', 'human', 'none']}\n",
    "    visualization = None\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        \n",
    "        self.df_scaled = config.get(\"df_scaled\").reset_index(drop=True)\n",
    "        self.df_normal = config.get(\"df_normal\").reset_index(drop=True)\n",
    "        self.window_size = OBSERVATION_WINDOW_SIZE\n",
    "        self.prices, self.features = self._process_data(self.df_scaled)\n",
    "        # The shape of the observation is (window_size * features + environment_features) the environment_features are: quote_asset, base_asset, net_worth. The entire observation is flattened in a 1D np array. \n",
    "        # NOT USED ANYMORE, KEPT FOR REFERENCE\n",
    "        # self.obs_shape = ((OBSERVATION_WINDOW_SIZE * self.features.shape[1] + 3),) \n",
    "\n",
    "        # The shape of the observation is number of candles to look back, and the number of features (candle_features) + 3 (quote_asset, base_asset, net_worth)\n",
    "        self.obs_shape = (OBSERVATION_WINDOW_SIZE, self.features.shape[1] + 3)\n",
    "\n",
    "        # Action space\n",
    "        #self.action_space = spaces.Box(low=np.array([0, 0]), high=np.array([3.0, 1.0]), dtype=np.float32)\n",
    "        self.action_space = spaces.MultiDiscrete([3, 100])\n",
    "        # Observation space\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=self.obs_shape, dtype=np.float32)\n",
    "\n",
    "        # Initialize the episode environment\n",
    "\n",
    "        self._start_candle = OBSERVATION_WINDOW_SIZE # We assume that the first observation is not the first row of the dataframe, in order to avoid the case where there are no calculated indicators.\n",
    "        self._end_candle = len(self.features) - 1\n",
    "        self._trading_fee = config.get(\"trading_fee\")\n",
    "\n",
    "        self._quote_asset = None\n",
    "        self._base_asset = None\n",
    "        self._done = None\n",
    "        self._current_candle = None\n",
    "        self._net_worth = None\n",
    "        self._previous_net_worth = None\n",
    "        self._previous_base_asset = None\n",
    "        self._previous_quote_asset = None\n",
    "\n",
    "        # Array that will contain observation history needed for appending it to the observation space\n",
    "        # It will contain observations consisting of the net_worth, base_asset and quote_asset as list of floats\n",
    "        # Other features (OHLC + Indicators) will be appended to the current observation in the _get_observation method that takes the data directly from the available dataframe\n",
    "        self._obs_env_history = None\n",
    "\n",
    "        # Render and analysis data\n",
    "        self._total_reward_accumulated = None\n",
    "        self.trade_history = None\n",
    "        self._first_rendering = None\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        self._done = False\n",
    "        self._current_candle = self._start_candle\n",
    "        self._quote_asset = INITIAL_QUOTE_ASSET\n",
    "        self._base_asset = INITIAL_BASE_ASSET \n",
    "        self._net_worth = INITIAL_QUOTE_ASSET # at the begining our net worth is the initial quote asset\n",
    "        self._previous_net_worth = INITIAL_QUOTE_ASSET # at the begining our previous net worth is the initial quote asset\n",
    "        self._previous_base_asset = INITIAL_BASE_ASSET\n",
    "        self._previous_quote_asset = INITIAL_QUOTE_ASSET\n",
    "        self._total_reward_accumulated = 0\n",
    "        self._first_rendering = True\n",
    "        self.trade_history = []\n",
    "        self._obs_env_history = []\n",
    "        \n",
    "        self._initial_obs_data()\n",
    "\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        self._done = False\n",
    "        current_price = random.uniform(\n",
    "            self.df_normal.loc[self._current_candle, \"low\"], self.df_normal.loc[self._current_candle, \"high\"])\n",
    "\n",
    "\n",
    "        action_type = action[0]\n",
    "        amount = action[1] / 100\n",
    "        \n",
    "        if action_type == 0: # Buy\n",
    "            # Buy % assets\n",
    "            # Determine the maximum amount of quote asset that can be bought\n",
    "            available_amount_to_buy_with = self._quote_asset / current_price\n",
    "            # Buy only the amount that agent chose\n",
    "            assets_bought = available_amount_to_buy_with * amount\n",
    "            # Update the quote asset balance\n",
    "            self._quote_asset -= assets_bought * current_price\n",
    "            # Update the base asset\n",
    "            self._base_asset += assets_bought\n",
    "            # substract trading fee from base asset based on the amount bought\n",
    "            self._base_asset -= self._trading_fee * assets_bought\n",
    "\n",
    "            # Add to trade history the amount bought if greater than 0\n",
    "            if assets_bought > 0:\n",
    "                self.trade_history.append({'step': self._current_candle, 'type': 'Buy', 'amount': assets_bought, 'price': current_price, 'total' : assets_bought * current_price, 'percent_amount': action[1]})\n",
    "        \n",
    "\n",
    "        elif action_type == 1: # Sell\n",
    "            # Sell % assets\n",
    "            # Determine the amount of base asset that can be sold\n",
    "            amount_to_sell = self._base_asset * amount\n",
    "            received_quote_asset = amount_to_sell * current_price\n",
    "            # Update the quote asset\n",
    "            self._quote_asset += received_quote_asset\n",
    "            # Update the base asset\n",
    "            self._base_asset -= amount_to_sell\n",
    "            \n",
    "            # substract trading fee from quote asset based on the amount sold\n",
    "            self._quote_asset -= self._trading_fee * received_quote_asset\n",
    "\n",
    "            # Add to trade history the amount sold if greater than 0\n",
    "            if amount_to_sell > 0:\n",
    "                self.trade_history.append({'step': self._current_candle, 'type': 'Sell', 'amount': amount_to_sell, 'price': current_price, 'total' : received_quote_asset, 'percent_amount': action[1]})\n",
    "\n",
    "        else:\n",
    "            # Hold\n",
    "            self.trade_history.append({'step': self._current_candle, 'type': 'Hold', 'amount': '0', 'price': current_price, 'total' : 0, 'percent_amount': action[1]})\n",
    "\n",
    "\n",
    "        # Update the current net worth\n",
    "        self._net_worth = self._base_asset * current_price + self._quote_asset\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Returns the next observation, reward, done and info.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._take_action(action)\n",
    "\n",
    "        # Calculate reward comparing the current base asset with the previous base asset\n",
    "        reward = self._base_asset - self._previous_base_asset\n",
    "\n",
    "        self._total_reward_accumulated += reward\n",
    "\n",
    "        # Update the previous net worth to be the current net worth after the reward has been applied\n",
    "        self._previous_net_worth = self._net_worth\n",
    "        self._previous_base_asset = self._base_asset\n",
    "        self._previous_quote_asset = self._quote_asset\n",
    "\n",
    "        obs = self._get_observation()\n",
    "        # Update the info and add it to history data\n",
    "        info = dict (\n",
    "            total_reward_accumulated = self._total_reward_accumulated,\n",
    "            net_worth = self._net_worth,\n",
    "            last_action_type = self.trade_history[-1]['type'] if len(self.trade_history) > 0 else None,\n",
    "            last_action_amount = self.trade_history[-1]['amount'] if len(self.trade_history) > 0 else None,\n",
    "            quote_asset = self._quote_asset,\n",
    "            base_asset = self._base_asset,\n",
    "            current_step = self._current_candle\n",
    "        )\n",
    "\n",
    "        self._current_candle += 1\n",
    "\n",
    "        # Update observation history\n",
    "        self._obs_env_history.append([self._net_worth, self._base_asset, self._quote_asset])\n",
    "\n",
    "        self._done = self._net_worth <= 0 or self._current_candle >= (len(\n",
    "            self.df_normal.loc[:, 'open'].values) - 30)# We assume that the last observation is not the last row of the dataframe, in order to avoid the case where there are no calculated indicators.\n",
    "\n",
    "        if self._done:\n",
    "            print('The episode has finished')\n",
    "        \n",
    "        return obs, reward, self._done, info\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Returns the current observation.\n",
    "        \"\"\"\n",
    "        data_frame = self.features[(self._current_candle - self.window_size):self._current_candle]\n",
    "\n",
    "        obs_env_history = np.array(self._obs_env_history).astype(np.float32)\n",
    "\n",
    "        #TODO We definetely need to scale the observation history in a better way, this might influence training results\n",
    "        # Doing it ad-hoc might change the scale of the min and max, thus changing the results\n",
    "        obs_env_history = preprocessing.minmax_scale(obs_env_history, (-0.9,0.9)) \n",
    "\n",
    "        obs = np.hstack((data_frame, obs_env_history[(self._current_candle - self.window_size):self._current_candle]))\n",
    "\n",
    "        return obs\n",
    "\n",
    "\n",
    "    def render(self, mode='human', **kwargs):\n",
    "        \"\"\"\n",
    "        Renders a plot with trades made by the agent.\n",
    "        \"\"\"\n",
    "        \n",
    "        if mode == 'human':\n",
    "            print(f'Accumulated Reward: {self._total_reward_accumulated} ---- Current Net Worth: {self._net_worth}')\n",
    "            print(f'Current Quote asset: {self._quote_asset} ---- Current Base asset: {self._base_asset}')\n",
    "            print(f'Number of trades: {len(self.trade_history)}')\n",
    "        \n",
    "            if(len(self.trade_history) > 0):\n",
    "                print(f'Last Action: {self.trade_history[-1][\"type\"]} {self.trade_history[-1][\"amount\"]} assets ({self.trade_history[-1][\"percent_amount\"]} %) at price {self.trade_history[-1][\"price\"]}, total: {self.trade_history[-1][\"total\"]}')\n",
    "            print(f'--------------------------------------------------------------------------------------')\n",
    "        elif mode == 'live':\n",
    "            pass\n",
    "            # if self.visualization == None:\n",
    "            #     self.visualization = LiveTradingGraph(self.df_normal, kwargs.get('title', None))\n",
    "\n",
    "            # if self._current_candle > OBSERVATION_WINDOW_SIZE:\n",
    "            #     self.visualization.render(self._current_candle, self._net_worth, self.trade_history, window_size=OBSERVATION_WINDOW_SIZE)\n",
    "\n",
    "    def close(self):\n",
    "        if self.visualization != None:\n",
    "            self.visualization.close()\n",
    "            self.visualization = None\n",
    "         \n",
    "\n",
    "    def _process_data(self, df_scaled):\n",
    "        \"\"\"\n",
    "        Processes the dataframe into features.\n",
    "        \"\"\"\n",
    "        \n",
    "        prices = self.df_scaled.loc[:, 'close'].to_numpy(dtype=np.float32)\n",
    "\n",
    "        data_frame = df_scaled.iloc[:, 1:] # drop first column which is date TODO: Should be probably fixed outside of this class\n",
    "        # Convert df to numpy array\n",
    "        return prices, data_frame.to_numpy(dtype=np.float32)\n",
    "\n",
    "    def _initial_obs_data(self):\n",
    "        for i in range(self.window_size - len(self._obs_env_history)):\n",
    "            self._obs_env_history.append([self._net_worth, self._base_asset, self._quote_asset])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f893cc",
   "metadata": {},
   "source": [
    "### Allocate optimal resources method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8835a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_resource_allocation(available_cpu, available_gpu):\n",
    "    \"\"\"\n",
    "    Finds the optimal resource allocation for the agent based on the available resources in the cluster\n",
    "    \"\"\"\n",
    "    # If we have GPU available, we allocate it all for the training, while creating as much workers as CPU cores we have minus one for the driver which holds the trainer\n",
    "    if available_gpu > 0:\n",
    "        return {\n",
    "            'num_workers': available_cpu - 1,\n",
    "            'num_cpus_per_worker': 1,\n",
    "            'num_envs_per_worker': 1,\n",
    "            'num_gpus_per_worker': 0,\n",
    "            'num_cpus_for_driver': 1,\n",
    "            'num_gpus' : available_gpu\n",
    "        }\n",
    "    # If we don't have GPU available, we allocate enough CPU cores for stepping the env (workers) while having enough for training maintaing a ratio of around 3 workers with 1 CPU to 1 driver CPU\n",
    "    else:\n",
    "        # according to the benchmark, we should allocate more workers, each with 1 cpu, letting the rest for the driver\n",
    "        num_workers = int(math.floor((available_cpu  * 75) / 100))\n",
    "        num_cpu_for_driver = available_cpu - num_workers\n",
    "        return {\n",
    "            'num_workers': num_workers,\n",
    "            'num_cpus_per_worker': 1, # this should be enough for stepping an env at once\n",
    "            'num_envs_per_worker': 1, # it doesn't seem to add any benefits to have more than one env per worker\n",
    "            'num_gpus_per_worker': 0, # the inference is done pretty fast, so there is no need to use GPU, at least not when we run one trial at once\n",
    "            'num_cpus_for_driver': num_cpu_for_driver,\n",
    "            'num_gpus' : 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b670e8a2",
   "metadata": {},
   "source": [
    "### Init ray and trainer config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca8fc3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 09:32:33,068\tINFO services.py:1456 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import ray\n",
    "import os\n",
    "from ray import tune\n",
    "from ray.rllib.env.vector_env import VectorEnv\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Ray\n",
    "ray.shutdown() # let's shutdown first any running instances of ray (don't confuse it with the cluster)\n",
    "os.environ['RAY_record_ref_creation_sites'] = '1' # Needed for debugging when things go wrong\n",
    "ray.init() \n",
    "\n",
    "try:\n",
    "    available_gpu_in_cluster = ray.available_resources()['GPU']\n",
    "except KeyError as e:\n",
    "    available_gpu_in_cluster = 0\n",
    "\n",
    "available_cpu_in_cluster = ray.available_resources()['CPU'] if ray.available_resources()['CPU']  else 0\n",
    "\n",
    "# In the first version we assume that we have only one node cluster, so the allocation logic is based on that\n",
    "# So the resources are maximized for one ray tune trial at a time\n",
    "parallel_config = find_optimal_resource_allocation(available_cpu_in_cluster, 0) # Currently we are going to disable GPU ussage due to it's poor performance on a single instance cluster\n",
    "\n",
    "trading_fee = 0.0075\n",
    "training_config = {\n",
    "            \"trading_fee\": trading_fee,\n",
    "            \"df_normal\": X_train,\n",
    "            \"df_scaled\": X_train_scaled,\n",
    "}\n",
    "\n",
    "eval_config = {\n",
    "            \"trading_fee\": trading_fee,\n",
    "            \"df_normal\": X_test,\n",
    "            \"df_scaled\": X_test_scaled,\n",
    "}\n",
    "\n",
    "if ENV_NAME == 'SimpleTrading':\n",
    "    training_env = SimpleTradingEnv(training_config)\n",
    "    eval_env = SimpleTradingEnv(eval_config)\n",
    "\n",
    "    training_env_key = \"SimpleTradingEnv-training-V01\"\n",
    "    eval_env_key = \"SimpleTradingEnv-evaluating-V01\"\n",
    "    \n",
    "elif ENV_NAME == 'BTCAccumulationEnv':\n",
    "    training_env = BTCAccumulationEnv(training_config)\n",
    "    eval_env = BTCAccumulationEnv(eval_config)\n",
    "    \n",
    "    training_env_key = \"BTCAccumulationEnv-training-V01\"\n",
    "    eval_env_key = \"BTCAccumulationEnv-evaluating-V01\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tune.register_env(training_env_key, lambda _: training_env)\n",
    "tune.register_env(eval_env_key, lambda _: eval_env)\n",
    "\n",
    "\n",
    "# Create the ppo trainer configuration\n",
    "ppo_trainer_config = {\n",
    "        \"env\": training_env_key, # Ray will automatically create multiple environments and vectorize them if needed\n",
    "        \"horizon\": len(X_train_scaled) - 30,\n",
    "        \"log_level\": \"INFO\",\n",
    "        \"framework\": \"tf\",\n",
    "        #\"eager_tracing\": True,\n",
    "        \"ignore_worker_failures\": True, \n",
    "        \"num_workers\": parallel_config.get(\"num_workers\"), # Number of workers is per trial run, so the more we put the less parallelism we have\n",
    "        \"num_envs_per_worker\": parallel_config.get(\"num_envs_per_worker\"), # This influences also the length of the episode. the environment length will be split by the number of environments per worker\n",
    "        \"num_gpus\": parallel_config.get(\"num_gpus\"), # Number of GPUs to use in training (0 means CPU only). After a few experiments, it seems that using GPU is not helping\n",
    "        \"num_cpus_per_worker\": parallel_config.get(\"num_cpus_per_worker\"), # After some testing, seems the fastest way for this kind of enviroment. It's better to run more trials in parallel than to finish a trial with a couple of minutes faster. Because we can end trial earlier if we see that our model eventuall converge\n",
    "        \"num_cpus_for_driver\": parallel_config.get(\"num_cpus_for_driver\"), # Number of CPUs to use for the driver. This is the number of CPUs used for the training process.\n",
    "        \"num_gpus_per_worker\": parallel_config.get(\"num_gpus_per_worker\"), \n",
    "        \"rollout_fragment_length\": 200, # Size of batches collected from each worker. If num_envs_per_worker is > 1 the rollout value will be multiplied by num_envs_per_worker\n",
    "        \"train_batch_size\": 2048, # Number of timesteps collected for each SGD round. This defines the size of each SGD epoch. the batch size is composed of fragments defined above\n",
    "        \"sgd_minibatch_size\": 64,\n",
    "        \"batch_mode\": \"complete_episodes\",\n",
    "        \"vf_clip_param\": 100, # Default is 10, but we increase it to 100 to adapt it to our rewards scale. It helps our value function to converge faster\n",
    "        \"lr\": 0.00001,  # Hyperparameter grid search defined above\n",
    "        \"gamma\": 0.95,  # This can have a big impact on the result and needs to be properly tuned\n",
    "        #\"observation_filter\": \"MeanStdFilter\",\n",
    "        \"model\": {\n",
    "        #    \"fcnet_hiddens\": FC_SIZE,  # Hyperparameter grid search defined above\n",
    "            # \"use_lstm\": True,\n",
    "            # \"lstm_cell_size\": 256,\n",
    "            # \"lstm_use_prev_action_reward\": True,\n",
    "            # \"lstm_use_prev_action\": True,\n",
    "            \n",
    "        },\n",
    "        #\"sgd_minibatch_size\": MINIBATCH_SIZE,  # Hyperparameter grid search defined above\n",
    "        \"evaluation_interval\": 5,  # Run one evaluation step on every x `Trainer.train()` call.\n",
    "        \"evaluation_duration\": 1,  # How many episodes to run evaluations for each time we evaluate.\n",
    "        \"evaluation_config\": {\n",
    "            \"explore\": True,  # We usually don't want to explore during evaluation. All actions have to be repeatable. Similar to deterministic = True, but on-policy algorithms can get better results with exploration.\n",
    "            \"env\": eval_env_key, # We need to define a new environment for evaluation with different parameters\n",
    "        },\n",
    "        \"logger_config\": {\n",
    "            \"logdir\": \"/tmp/ray_logging/\",\n",
    "            \"type\": \"ray.tune.logger.UnifiedLogger\",\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d93ae3",
   "metadata": {},
   "source": [
    "### Custom reporter to get progress in Superalgos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41eab5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import ProgressReporter\n",
    "from typing import Dict, List, Optional, Union\n",
    "import json\n",
    "\n",
    "class CustomReporter(ProgressReporter):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_report_frequency: int = 10, # in seconds\n",
    "        location: str = \"/tf/notebooks/\",\n",
    "    ):\n",
    "        self._max_report_freqency = max_report_frequency\n",
    "        self._last_report_time = 0\n",
    "        self._location = location\n",
    "\n",
    "    def should_report(self, trials, done=False):\n",
    "        if time.time() - self._last_report_time > self._max_report_freqency:\n",
    "            self._last_report_time = time.time()\n",
    "            return True\n",
    "        return done\n",
    "\n",
    "    def report(self, trials, *sys_info):\n",
    "\n",
    "        trial_status_dict = {}\n",
    "        for trial in trials:\n",
    "            trial_status_dict['status'] = trial.status\n",
    "            trial_status_dict['name'] = trial.trial_id\n",
    "            trial_status_dict['episodeRewardMax'] = int(trial.last_result['episode_reward_max']) if trial.last_result.get(\"episode_reward_max\") else 0\n",
    "            trial_status_dict['episodeRewardMean'] = int(trial.last_result['episode_reward_mean']) if trial.last_result.get(\"episode_reward_mean\") else 0\n",
    "            trial_status_dict['episodeRewardMin'] = int(trial.last_result['episode_reward_min']) if trial.last_result.get(\"episode_reward_min\") else 0\n",
    "            trial_status_dict['timestepsExecuted'] = int(trial.last_result['timesteps_total']) if trial.last_result.get(\"timesteps_total\") else 0\n",
    "            trial_status_dict['timestepsTotal'] = int(TIMESTEPS_TO_TRAIN)\n",
    "\n",
    "        \n",
    "        sys.stdout.write(json.dumps(trial_status_dict))\n",
    "        sys.stdout.write('\\n')\n",
    "\n",
    "        # Write the results to JSON file\n",
    "        with open(self._location + \"training_results.json\", \"w+\") as f:\n",
    "            json.dump(trial_status_dict, f)\n",
    "            f.close()\n",
    "    \n",
    "    def set_start_time(self, timestamp: Optional[float] = None):\n",
    "        if timestamp is not None:\n",
    "            self._start_time = time.time()\n",
    "        else:\n",
    "            self._start_time = timestamp\n",
    "\n",
    "custom_reporter = CustomReporter(max_report_frequency=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3d132dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL_SCENARIO\n"
     ]
    }
   ],
   "source": [
    "# Before starting printing a custom text to let Superalgos know that we are in a RL scenario\n",
    "sys.stdout.write('RL_SCENARIO')\n",
    "sys.stdout.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a77757",
   "metadata": {},
   "source": [
    "### Run ray tune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f508d31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:06:30,971\tINFO trial_runner.py:803 -- starting PPO_BTCAccumulationEnv-training-V01_e1f4e_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:34,400\tINFO trainer.py:2295 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:34,424\tWARNING ppo.py:240 -- `train_batch_size` (2048) cannot be achieved with your other settings (num_workers=6 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 341.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:34,424\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5310)\u001b[0m 2022-05-19 14:06:38,799\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5310)\u001b[0m 2022-05-19 14:06:38,799\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:06:38,891\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:06:38,891\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5310)\u001b[0m 2022-05-19 14:06:38,996\tINFO tf_policy.py:166 -- TFPolicy (worker=2) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5316)\u001b[0m 2022-05-19 14:06:38,921\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5316)\u001b[0m 2022-05-19 14:06:38,921\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5314)\u001b[0m 2022-05-19 14:06:38,938\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5314)\u001b[0m 2022-05-19 14:06:38,938\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5318)\u001b[0m 2022-05-19 14:06:38,952\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5318)\u001b[0m 2022-05-19 14:06:38,952\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5312)\u001b[0m 2022-05-19 14:06:38,958\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5312)\u001b[0m 2022-05-19 14:06:38,958\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5310)\u001b[0m 2022-05-19 14:06:39,060\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5310)\u001b[0m 2022-05-19 14:06:39,061\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5310)\u001b[0m 2022-05-19 14:06:39,061\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5310)\u001b[0m 2022-05-19 14:06:39,062\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5310)\u001b[0m 2022-05-19 14:06:39,062\tINFO dynamic_tf_policy.py:718 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:06:39,072\tINFO tf_policy.py:166 -- TFPolicy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5316)\u001b[0m 2022-05-19 14:06:39,115\tINFO tf_policy.py:166 -- TFPolicy (worker=5) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:06:39,151\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:06:39,152\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:06:39,153\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:06:39,153\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:06:39,154\tINFO dynamic_tf_policy.py:718 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5316)\u001b[0m 2022-05-19 14:06:39,184\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5316)\u001b[0m 2022-05-19 14:06:39,185\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5316)\u001b[0m 2022-05-19 14:06:39,185\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5316)\u001b[0m 2022-05-19 14:06:39,186\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5316)\u001b[0m 2022-05-19 14:06:39,186\tINFO dynamic_tf_policy.py:718 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5314)\u001b[0m 2022-05-19 14:06:39,138\tINFO tf_policy.py:166 -- TFPolicy (worker=4) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5314)\u001b[0m 2022-05-19 14:06:39,221\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5314)\u001b[0m 2022-05-19 14:06:39,221\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5314)\u001b[0m 2022-05-19 14:06:39,223\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5314)\u001b[0m 2022-05-19 14:06:39,224\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5314)\u001b[0m 2022-05-19 14:06:39,225\tINFO dynamic_tf_policy.py:718 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5318)\u001b[0m 2022-05-19 14:06:39,148\tINFO tf_policy.py:166 -- TFPolicy (worker=6) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5318)\u001b[0m 2022-05-19 14:06:39,221\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5318)\u001b[0m 2022-05-19 14:06:39,225\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5312)\u001b[0m 2022-05-19 14:06:39,169\tINFO tf_policy.py:166 -- TFPolicy (worker=3) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5318)\u001b[0m 2022-05-19 14:06:39,229\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5318)\u001b[0m 2022-05-19 14:06:39,231\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5318)\u001b[0m 2022-05-19 14:06:39,231\tINFO dynamic_tf_policy.py:718 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5312)\u001b[0m 2022-05-19 14:06:39,256\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5312)\u001b[0m 2022-05-19 14:06:39,257\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5312)\u001b[0m 2022-05-19 14:06:39,257\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5312)\u001b[0m 2022-05-19 14:06:39,258\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5312)\u001b[0m 2022-05-19 14:06:39,258\tINFO dynamic_tf_policy.py:718 -- Testing `postprocess_trajectory` w/ dummy batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:40,108\tINFO worker_set.py:154 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Box([[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]], [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], (24, 10), float32), MultiDiscrete([  3 100])), '__env__': (Box([[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]], [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], (24, 10), float32), MultiDiscrete([  3 100]))}\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:40,291\tINFO tf_policy.py:166 -- TFPolicy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:40,356\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:40,356\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:40,357\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:40,357\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:40,358\tINFO dynamic_tf_policy.py:718 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:40,959\tINFO rollout_worker.py:1727 -- Built policy map: {}\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:40,959\tINFO rollout_worker.py:1728 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fc61b388070>}\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:40,964\tINFO rollout_worker.py:666 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fc60857ecd0>}\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:40,980\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:40,980\tWARNING ppo.py:240 -- `train_batch_size` (2048) cannot be achieved with your other settings (num_workers=6 num_envs_per_worker=1 rollout_fragment_length=341)! Auto-adjusting `rollout_fragment_length` to 341.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:40,980\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:40,997\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:40,997\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:41,150\tINFO tf_policy.py:166 -- TFPolicy (worker=local) running on CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:41,200\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:41,200\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:41,201\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:41,201\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:41,201\tINFO dynamic_tf_policy.py:718 -- Testing `postprocess_trajectory` w/ dummy batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'RUNNING', 'name': 'e1f4e_00000', 'episodeRewardMax': 0, 'episodeRewardMean': 0, 'episodeRewardMin': 0, 'timestepsExecuted': 0, 'timestepsTotal': 1000}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:41,800\tINFO rollout_worker.py:1727 -- Built policy map: {}\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:41,800\tINFO rollout_worker.py:1728 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fc608497d60>}\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:41,800\tINFO rollout_worker.py:666 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fc6084ad130>}\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:06:41,801\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:06:41,809\tINFO rollout_worker.py:809 -- Generating sample batch of size 341\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:06:41,810\tINFO sampler.py:672 -- Raw obs from env: { 0: { 'agent0': np.ndarray((24, 10), dtype=float32, min=-0.9, max=0.057, mean=-0.242)}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:06:41,810\tINFO sampler.py:673 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:06:41,810\tINFO sampler.py:908 -- Preprocessed obs: np.ndarray((24, 10), dtype=float32, min=-0.9, max=0.057, mean=-0.242)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:06:41,810\tINFO sampler.py:913 -- Filtered obs: np.ndarray((24, 10), dtype=float32, min=-0.9, max=0.057, mean=-0.242)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:06:41,811\tINFO sampler.py:1143 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m                                   'obs': np.ndarray((24, 10), dtype=float32, min=-0.9, max=0.057, mean=-0.242),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m                                   'prev_action': None,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m                                   'prev_reward': None,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m                                   'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:06:41,811\tINFO tf_run_builder.py:98 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:06:41,857\tINFO sampler.py:1169 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=int64, min=2.0, max=89.0, mean=45.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 103), dtype=float32, min=-0.01, max=0.007, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-5.705, max=-5.705, mean=-5.705),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.003, max=0.003, mean=0.003),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.002, max=-0.002, mean=-0.002)})}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'RUNNING', 'name': 'e1f4e_00000', 'episodeRewardMax': 0, 'episodeRewardMean': 0, 'episodeRewardMin': 0, 'timestepsExecuted': 0, 'timestepsTotal': 1000}\n",
      "\n",
      "\n",
      "{'status': 'RUNNING', 'name': 'e1f4e_00000', 'episodeRewardMax': 0, 'episodeRewardMean': 0, 'episodeRewardMin': 0, 'timestepsExecuted': 0, 'timestepsTotal': 1000}\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5316)\u001b[0m The episode has finished\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5314)\u001b[0m The episode has finished\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5318)\u001b[0m The episode has finished\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5310)\u001b[0m The episode has finished\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m The episode has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:07:06,343\tINFO simple_list_collector.py:904 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((6834, 103), dtype=float32, min=-0.011, max=0.011, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m               'action_logp': np.ndarray((6834,), dtype=float32, min=-5.716, max=-5.695, mean=-5.704),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m               'actions': np.ndarray((6834, 2), dtype=int32, min=0.0, max=99.0, mean=25.095),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m               'advantages': np.ndarray((6834,), dtype=float32, min=-0.661, max=0.576, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m               'agent_index': np.ndarray((6834,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m               'dones': np.ndarray((6834,), dtype=bool, min=0.0, max=1.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m               'eps_id': np.ndarray((6834,), dtype=int64, min=371883862.0, max=371883862.0, mean=371883862.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m               'infos': np.ndarray((6834,), dtype=object, head={'total_reward_accumulated': 0, 'net_worth': 7203.5401884096955, 'last_action_type': 'Hold', 'last_action_amount': '0', 'quote_asset': 0, 'base_asset': 1, 'current_step': 24}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m               'new_obs': np.ndarray((6834, 24, 10), dtype=float32, min=-0.9, max=1.0, mean=-0.172),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m               'obs': np.ndarray((6834, 24, 10), dtype=float32, min=-0.9, max=1.0, mean=-0.172),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m               'rewards': np.ndarray((6834,), dtype=float32, min=-0.761, max=0.856, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m               'unroll_id': np.ndarray((6834,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m               'value_targets': np.ndarray((6834,), dtype=float32, min=-0.659, max=0.576, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m               'vf_preds': np.ndarray((6834,), dtype=float32, min=-0.004, max=0.007, mean=-0.001)}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m 2022-05-19 14:07:06,360\tINFO rollout_worker.py:854 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m { 'action_dist_inputs': np.ndarray((6834, 103), dtype=float32, min=-0.011, max=0.011, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m   'action_logp': np.ndarray((6834,), dtype=float32, min=-5.716, max=-5.695, mean=-5.704),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m   'actions': np.ndarray((6834, 2), dtype=int32, min=0.0, max=99.0, mean=25.095),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m   'advantages': np.ndarray((6834,), dtype=float32, min=-0.661, max=0.576, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m   'agent_index': np.ndarray((6834,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m   'dones': np.ndarray((6834,), dtype=bool, min=0.0, max=1.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m   'eps_id': np.ndarray((6834,), dtype=int64, min=371883862.0, max=371883862.0, mean=371883862.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m   'obs': np.ndarray((6834, 24, 10), dtype=float32, min=-0.9, max=1.0, mean=-0.172),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m   'rewards': np.ndarray((6834,), dtype=float32, min=-0.761, max=0.856, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m   'unroll_id': np.ndarray((6834,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m   'value_targets': np.ndarray((6834,), dtype=float32, min=-0.659, max=0.576, mean=-0.002)}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5309)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:07:06,552\tINFO rollout_ops.py:251 -- Collected more training samples than expected (actual=41004, expected=2048). This may be because you have many workers or long episodes in 'complete_episodes' batch mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:07:06,553\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:07:06,553\tINFO tf_policy.py:930 -- Optimizing variable <tf.Variable 'default_policy/fc_1/kernel:0' shape=(240, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:07:06,553\tINFO tf_policy.py:930 -- Optimizing variable <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:07:06,553\tINFO tf_policy.py:930 -- Optimizing variable <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:07:06,553\tINFO tf_policy.py:930 -- Optimizing variable <tf.Variable 'default_policy/fc_2/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:07:06,553\tINFO tf_policy.py:930 -- Optimizing variable <tf.Variable 'default_policy/logits/kernel:0' shape=(256, 103) dtype=float32>\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:07:06,553\tINFO tf_policy.py:930 -- Optimizing variable <tf.Variable 'default_policy/logits/bias:0' shape=(103,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:07:06,553\tINFO tf_policy.py:930 -- Optimizing variable <tf.Variable 'default_policy/value_out_2/kernel:0' shape=(256, 1) dtype=float32>\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:07:06,553\tINFO tf_policy.py:930 -- Optimizing variable <tf.Variable 'default_policy/value_out_2/bias:0' shape=(1,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=5272)\u001b[0m 2022-05-19 14:07:06,553\tINFO tf_run_builder.py:98 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=5312)\u001b[0m The episode has finished\n",
      "{'status': 'RUNNING', 'name': 'e1f4e_00000', 'episodeRewardMax': 0, 'episodeRewardMean': 0, 'episodeRewardMin': 0, 'timestepsExecuted': 0, 'timestepsTotal': 1000}\n",
      "\n",
      "\n",
      "{'status': 'RUNNING', 'name': 'e1f4e_00000', 'episodeRewardMax': 0, 'episodeRewardMean': 0, 'episodeRewardMin': 0, 'timestepsExecuted': 0, 'timestepsTotal': 1000}\n",
      "\n",
      "\n",
      "{'status': 'RUNNING', 'name': 'e1f4e_00000', 'episodeRewardMax': 0, 'episodeRewardMean': 0, 'episodeRewardMin': 0, 'timestepsExecuted': 0, 'timestepsTotal': 1000}\n",
      "\n",
      "\n",
      "{'status': 'RUNNING', 'name': 'e1f4e_00000', 'episodeRewardMax': 0, 'episodeRewardMean': 0, 'episodeRewardMin': 0, 'timestepsExecuted': 0, 'timestepsTotal': 1000}\n",
      "\n",
      "\n",
      "{'status': 'RUNNING', 'name': 'e1f4e_00000', 'episodeRewardMax': 0, 'episodeRewardMean': 0, 'episodeRewardMin': 0, 'timestepsExecuted': 0, 'timestepsTotal': 1000}\n",
      "\n",
      "\n",
      "{'status': 'RUNNING', 'name': 'e1f4e_00000', 'episodeRewardMax': 0, 'episodeRewardMean': 0, 'episodeRewardMin': 0, 'timestepsExecuted': 0, 'timestepsTotal': 1000}\n",
      "\n",
      "\n",
      "Result for PPO_BTCAccumulationEnv-training-V01_e1f4e_00000:\n",
      "  agent_timesteps_total: 41004\n",
      "  custom_metrics: {}\n",
      "  date: 2022-05-19_14-07-58\n",
      "  done: true\n",
      "  episode_len_mean: 6834.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.9998109839141472\n",
      "  episode_reward_mean: -0.9999197355900619\n",
      "  episode_reward_min: -0.9999985181311754\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 6\n",
      "  experiment_id: 080968aef98d431ca99e78786cc17d8f\n",
      "  hostname: 6dbede3eb3f4\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 5.696840763092041\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006905407179147005\n",
      "          model: {}\n",
      "          policy_loss: -0.016825631260871887\n",
      "          total_loss: -0.01136694010347128\n",
      "          vf_explained_var: -0.21717078983783722\n",
      "          vf_loss: 0.004077608231455088\n",
      "        num_agent_steps_trained: 64.0\n",
      "    num_agent_steps_sampled: 41004\n",
      "    num_agent_steps_trained: 41004\n",
      "    num_steps_sampled: 41004\n",
      "    num_steps_trained: 41004\n",
      "    num_steps_trained_this_iter: 41004\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 6\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.32072072072072\n",
      "    ram_util_percent: 60.37297297297299\n",
      "  pid: 5272\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07888596396828766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.404192045705256\n",
      "    mean_inference_ms: 0.9799115688618842\n",
      "    mean_raw_obs_processing_ms: 0.1138552198523982\n",
      "  time_since_restore: 77.11189532279968\n",
      "  time_this_iter_s: 77.11189532279968\n",
      "  time_total_s: 77.11189532279968\n",
      "  timers:\n",
      "    learn_throughput: 783.274\n",
      "    learn_time_ms: 52349.52\n",
      "    load_throughput: 296522829.683\n",
      "    load_time_ms: 0.138\n",
      "    sample_throughput: 1603.405\n",
      "    sample_time_ms: 25573.071\n",
      "    update_time_ms: 3.581\n",
      "  timestamp: 1652969278\n",
      "  timesteps_since_restore: 41004\n",
      "  timesteps_this_iter: 41004\n",
      "  timesteps_total: 41004\n",
      "  training_iteration: 1\n",
      "  trial_id: e1f4e_00000\n",
      "  warmup_time: 7.407155752182007\n",
      "  \n",
      "{'status': 'TERMINATED', 'name': 'e1f4e_00000', 'episodeRewardMax': 0, 'episodeRewardMean': 0, 'episodeRewardMin': 0, 'timestepsExecuted': 41004, 'timestepsTotal': 1000}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:07:59,741\tINFO tune.py:701 -- Total run time: 88.89 seconds (88.10 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(\n",
    "    run_or_experiment=ALGORITHM,\n",
    "    name=EXPERIMENT_NAME,\n",
    "    metric='episode_reward_mean',\n",
    "    mode='max',\n",
    "    stop={\n",
    "        # An iteration is equal with one SGD round which in our case is equal to train_batch_size. If after X iterations we still don't have a good result, we stop the trial\n",
    "        \"timesteps_total\": TIMESTEPS_TO_TRAIN      \n",
    "    },\n",
    "    config=ppo_trainer_config,\n",
    "    num_samples=1,  # Have one sample for each hyperparameter combination. You can have more to average out randomness.\n",
    "    keep_checkpoints_num=30,  # Keep the last X checkpoints\n",
    "    checkpoint_freq=5,  # Checkpoint every X iterations (save the model)\n",
    "    local_dir=\"/tf/notebooks/ray_results/\",  # Local directory to store checkpoints and results, we are using tmp folder until we move the notebook to a docker instance and we can use the same directory across all instances, no matter the underlying OS\n",
    "    progress_reporter=custom_reporter,\n",
    "    fail_fast=\"raise\",\n",
    "    resume=False # Resume training from the last checkpoint if any exists\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38e489d",
   "metadata": {},
   "source": [
    "### Evaluate trained model restoring it from checkpoint\n",
    "\n",
    "#### Store the results in a file to be picked up by Superalgos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27386b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 09:32:40,381\tINFO trainer.py:2295 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-05-20 09:32:40,393\tWARNING ppo.py:240 -- `train_batch_size` (2048) cannot be achieved with your other settings (num_workers=6 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 341.\n",
      "2022-05-20 09:32:40,394\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2642)\u001b[0m 2022-05-20 09:32:43,721\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2642)\u001b[0m 2022-05-20 09:32:43,721\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2641)\u001b[0m 2022-05-20 09:32:43,720\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2641)\u001b[0m 2022-05-20 09:32:43,720\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2646)\u001b[0m 2022-05-20 09:32:43,733\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2646)\u001b[0m 2022-05-20 09:32:43,733\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2644)\u001b[0m 2022-05-20 09:32:43,718\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2644)\u001b[0m 2022-05-20 09:32:43,718\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m 2022-05-20 09:32:43,726\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m 2022-05-20 09:32:43,726\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2650)\u001b[0m 2022-05-20 09:32:43,798\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2650)\u001b[0m 2022-05-20 09:32:43,798\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2642)\u001b[0m 2022-05-20 09:32:43,908\tINFO tf_policy.py:166 -- TFPolicy (worker=2) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2642)\u001b[0m 2022-05-20 09:32:43,975\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2642)\u001b[0m 2022-05-20 09:32:43,975\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2642)\u001b[0m 2022-05-20 09:32:43,976\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2642)\u001b[0m 2022-05-20 09:32:43,977\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2642)\u001b[0m 2022-05-20 09:32:43,977\tINFO dynamic_tf_policy.py:718 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2641)\u001b[0m 2022-05-20 09:32:43,909\tINFO tf_policy.py:166 -- TFPolicy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2641)\u001b[0m 2022-05-20 09:32:43,975\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2641)\u001b[0m 2022-05-20 09:32:43,975\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2641)\u001b[0m 2022-05-20 09:32:43,977\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2641)\u001b[0m 2022-05-20 09:32:43,977\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2641)\u001b[0m 2022-05-20 09:32:43,978\tINFO dynamic_tf_policy.py:718 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2646)\u001b[0m 2022-05-20 09:32:43,921\tINFO tf_policy.py:166 -- TFPolicy (worker=4) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2646)\u001b[0m 2022-05-20 09:32:43,990\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2646)\u001b[0m 2022-05-20 09:32:43,991\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2646)\u001b[0m 2022-05-20 09:32:43,992\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2646)\u001b[0m 2022-05-20 09:32:43,992\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2646)\u001b[0m 2022-05-20 09:32:43,993\tINFO dynamic_tf_policy.py:718 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2644)\u001b[0m 2022-05-20 09:32:43,907\tINFO tf_policy.py:166 -- TFPolicy (worker=3) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2644)\u001b[0m 2022-05-20 09:32:43,974\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2644)\u001b[0m 2022-05-20 09:32:43,975\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2644)\u001b[0m 2022-05-20 09:32:43,975\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2644)\u001b[0m 2022-05-20 09:32:43,976\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2644)\u001b[0m 2022-05-20 09:32:43,976\tINFO dynamic_tf_policy.py:718 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m 2022-05-20 09:32:43,917\tINFO tf_policy.py:166 -- TFPolicy (worker=5) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m 2022-05-20 09:32:43,990\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m 2022-05-20 09:32:43,990\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m 2022-05-20 09:32:43,991\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m 2022-05-20 09:32:43,992\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2648)\u001b[0m 2022-05-20 09:32:43,992\tINFO dynamic_tf_policy.py:718 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2650)\u001b[0m 2022-05-20 09:32:44,001\tINFO tf_policy.py:166 -- TFPolicy (worker=6) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2650)\u001b[0m 2022-05-20 09:32:44,081\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2650)\u001b[0m 2022-05-20 09:32:44,081\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2650)\u001b[0m 2022-05-20 09:32:44,082\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2650)\u001b[0m 2022-05-20 09:32:44,083\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2650)\u001b[0m 2022-05-20 09:32:44,083\tINFO dynamic_tf_policy.py:718 -- Testing `postprocess_trajectory` w/ dummy batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 09:32:44,922\tINFO worker_set.py:154 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Box([[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]], [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], (24, 10), float32), MultiDiscrete([  3 100])), '__env__': (Box([[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]], [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], (24, 10), float32), MultiDiscrete([  3 100]))}\n",
      "2022-05-20 09:32:45,077\tINFO tf_policy.py:166 -- TFPolicy (worker=local) running on CPU.\n",
      "2022-05-20 09:32:45,123\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "2022-05-20 09:32:45,124\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "2022-05-20 09:32:45,126\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "2022-05-20 09:32:45,128\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "2022-05-20 09:32:45,128\tINFO dynamic_tf_policy.py:718 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "2022-05-20 09:32:45,694\tINFO rollout_worker.py:1727 -- Built policy map: {}\n",
      "2022-05-20 09:32:45,695\tINFO rollout_worker.py:1728 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f27183d3220>}\n",
      "2022-05-20 09:32:45,695\tINFO rollout_worker.py:666 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f288e7e6640>}\n",
      "2022-05-20 09:32:45,714\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-05-20 09:32:45,714\tWARNING ppo.py:240 -- `train_batch_size` (2048) cannot be achieved with your other settings (num_workers=6 num_envs_per_worker=1 rollout_fragment_length=341)! Auto-adjusting `rollout_fragment_length` to 341.\n",
      "2022-05-20 09:32:45,715\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-05-20 09:32:45,735\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-05-20 09:32:45,735\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-05-20 09:32:45,891\tINFO tf_policy.py:166 -- TFPolicy (worker=local) running on CPU.\n",
      "2022-05-20 09:32:45,936\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "2022-05-20 09:32:45,937\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "2022-05-20 09:32:45,938\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "2022-05-20 09:32:45,940\tINFO dynamic_tf_policy.py:709 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "2022-05-20 09:32:45,941\tINFO dynamic_tf_policy.py:718 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "2022-05-20 09:32:46,489\tINFO rollout_worker.py:1727 -- Built policy map: {}\n",
      "2022-05-20 09:32:46,489\tINFO rollout_worker.py:1728 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f26d4c5f640>}\n",
      "2022-05-20 09:32:46,490\tINFO rollout_worker.py:666 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f26d4c5f5e0>}\n",
      "2022-05-20 09:32:46,491\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n",
      "2022-05-20 09:32:46,552\tINFO trainable.py:534 -- Restored on 172.17.0.2 from checkpoint: /tf/notebooks/ray_results/Trading_Signal_Predictor_RL_V01/PPO_BTCAccumulationEnv-training-V01_c7147_00000_0_2022-05-19_15-10-11/checkpoint_000020/checkpoint-20\n",
      "2022-05-20 09:32:46,553\tINFO trainable.py:543 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': 820080, '_time_total': 1804.1247940063477, '_episodes_total': 120}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode has finished\n"
     ]
    }
   ],
   "source": [
    "# best_trial = analysis.get_best_trial(metric=\"episode_reward_mean\", mode=\"max\", scope=\"all\") \n",
    "# best_checkpoint = analysis.get_best_checkpoint(best_trial, metric=\"episode_reward_mean\")\n",
    "\n",
    "\n",
    "agent = ppo.PPOTrainer(config=ppo_trainer_config)\n",
    "agent.restore(\"/tf/notebooks/ray_results/Trading_Signal_Predictor_RL_V01/PPO_BTCAccumulationEnv-training-V01_c7147_00000_0_2022-05-19_15-10-11/checkpoint_000020/checkpoint-20\")\n",
    "\n",
    "json_dict = {}\n",
    "net_worths = []\n",
    "q_assets = []\n",
    "b_assets = []\n",
    "net_worths_at_end = []\n",
    "q_assets_at_end = []\n",
    "b_assets_at_end = []\n",
    "episodes_to_run = 1\n",
    "\n",
    "for i in range(episodes_to_run):\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = eval_env.reset() # we are using the evaluation environment for evaluation\n",
    "    last_info = None\n",
    "    while not done:\n",
    "        action = agent.compute_single_action(obs, explore=True) # stochastic evaluation\n",
    "        obs, reward, done, info = eval_env.step(action)\n",
    "        net_worths.append(info['net_worth']) # Add all historical net worths to a list to print statistics at the end of the episode\n",
    "        q_assets.append(info['quote_asset']) # Add all historical quote assets to a list to print statistics at the end of the episode\n",
    "        b_assets.append(info['base_asset']) # Add all historical base assets to a list to print statistics at the end of the episode\n",
    "        episode_reward += reward\n",
    "        last_info = info\n",
    "\n",
    "    net_worths_at_end.append(last_info['net_worth']) # Add all historical net worths to a list to print statistics at the end of the episode\n",
    "    q_assets_at_end.append(last_info['quote_asset']) # Add all historical quote assets to a list to print statistics at the end of the episode\n",
    "    b_assets_at_end.append(last_info['base_asset']) # Add all historical base assets to a list to print statistics at the end of the episode\n",
    "\n",
    "json_dict['meanNetWorth'] = np.mean(net_worths)\n",
    "json_dict['stdNetWorth'] = np.std(net_worths)\n",
    "json_dict['minNetWorth'] = np.min(net_worths)\n",
    "json_dict['maxNetWorth'] = np.max(net_worths)\n",
    "json_dict['stdQuoteAsset'] = np.std(q_assets)\n",
    "json_dict['minQuoteAsset'] = np.min(q_assets)\n",
    "json_dict['maxQuoteAsset'] = np.max(q_assets)\n",
    "json_dict['stdBaseAsset'] = np.std(b_assets)\n",
    "json_dict['minBaseAsset'] = np.min(b_assets)\n",
    "json_dict['maxBaseAsset'] = np.max(b_assets)\n",
    "json_dict['meanNetWorthAtEnd'] = np.mean(net_worths_at_end)\n",
    "json_dict['stdNetWorthAtEnd'] = np.std(net_worths_at_end)\n",
    "json_dict['minNetWorthAtEnd'] = np.min(net_worths_at_end)\n",
    "json_dict['maxNetWorthAtEnd'] = np.max(net_worths_at_end)\n",
    "\n",
    "\n",
    "# Write the results to JSON file\n",
    "with open(\"evaluation_results.json\", \"w+\") as f:\n",
    "    json.dump(json_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38cd3293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "os.remove('/tf/notebooks/training_results.json')\n",
    "os.remove('/tf/notebooks/evaluation_results.json')\n",
    "ray.shutdown()\n",
    "sys.stdout.write('RL_SCENARIO_END')\n",
    "sys.stdout.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TensorTrade.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
